{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T13:08:45.437857Z",
     "iopub.status.busy": "2024-11-14T13:08:45.437325Z",
     "iopub.status.idle": "2024-11-14T13:08:49.314877Z",
     "shell.execute_reply": "2024-11-14T13:08:49.313237Z",
     "shell.execute_reply.started": "2024-11-14T13:08:45.437797Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\\downsampled\\downsampled_df_shuffled.csv\n",
      "['dataset\\\\downsampled\\\\downsampled_df_shuffled.csv']\n",
      "\n",
      "Imports done\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
    "# from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.tree import plot_tree\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "input_files = []\n",
    "\n",
    "for dirname, _, filenames in os.walk('dataset\\downsampled'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        input_files.append(os.path.join(dirname, filename))\n",
    "        \n",
    "print(input_files)\n",
    "print(\"\\nImports done\")\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup for CSECICIDS2018_improved and CICIDS2017_improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T13:08:49.318059Z",
     "iopub.status.busy": "2024-11-14T13:08:49.317437Z",
     "iopub.status.idle": "2024-11-14T13:08:49.332555Z",
     "shell.execute_reply": "2024-11-14T13:08:49.331053Z",
     "shell.execute_reply.started": "2024-11-14T13:08:49.318011Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions defined\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "def dataset_cleaner(df, label_column):\n",
    "    \"\"\"\n",
    "    Clean the dataset by handling missing values, scaling numerical features, and \n",
    "    applying one-hot encoding to categorical features, while leaving the label class intact.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame, the input DataFrame.\n",
    "    - label_column: str, the name of the target (label) column to be preserved.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with cleaned features and the label column intact.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Separate label column from features\n",
    "    label = df[label_column]\n",
    "    df = df.drop(columns=[label_column])\n",
    "    \n",
    "    # Drop all inf and other NaN values\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "    # Define numerical and categorical columns\n",
    "    numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # 1. Handle numerical columns (impute missing values and scale)\n",
    "    if numerical_cols:\n",
    "        # Imputation for numerical columns\n",
    "        numerical_imputer = SimpleImputer(strategy='mean')\n",
    "        df[numerical_cols] = numerical_imputer.fit_transform(df[numerical_cols])\n",
    "\n",
    "        # Scaling numerical columns\n",
    "        scaler = StandardScaler()\n",
    "        df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "    if categorical_cols:\n",
    "        # 2. Handle categorical columns (impute missing values and apply one-hot encoding)\n",
    "        # Imputation for categorical columns\n",
    "        categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        df[categorical_cols] = categorical_imputer.fit_transform(df[categorical_cols])\n",
    "\n",
    "        # One-hot encoding for categorical columns\n",
    "        encoder = OneHotEncoder(drop='first', sparse=False)  # Use sparse=False to get a dense matrix\n",
    "        encoded_categorical = encoder.fit_transform(df[categorical_cols])\n",
    "\n",
    "        # Convert the one-hot encoded data to a DataFrame and concatenate with original DataFrame\n",
    "        encoded_df = pd.DataFrame(encoded_categorical, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "        # Drop original categorical columns and concatenate encoded columns\n",
    "        df = df.drop(columns=categorical_cols)\n",
    "        df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "    # Re-add the label column\n",
    "    df[label_column] = label\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_target_features(df, test_size = 0.30):\n",
    "    X = df.drop(['Label'], axis=1)\n",
    "    y = df['Label']\n",
    "    \n",
    "    return train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "print(\"Functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "def select_features_correlation(X, y, k=10):\n",
    "    \"\"\"\n",
    "    Select top k features based on correlation with the target.\n",
    "\n",
    "    Parameters:\n",
    "    - X: DataFrame, the feature matrix.\n",
    "    - y: Series, the target variable.\n",
    "    - k: int, number of top features to select.\n",
    "\n",
    "    Returns:\n",
    "    - List of top k features based on correlation.\n",
    "    \"\"\"\n",
    "    # Factorize the target and convert to pandas Series\n",
    "    y_factorized = pd.Series(y.factorize()[0], index=y.index)\n",
    "    \n",
    "    # Convert categorical features to numeric\n",
    "    X_encoded = X.apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    # Compute correlation\n",
    "    correlation = X_encoded.corrwith(y_factorized)\n",
    "    \n",
    "    # Create a DataFrame for correlation values\n",
    "    correlation_df = pd.DataFrame({\n",
    "        'Feature': correlation.index,\n",
    "        'Correlation': correlation.values\n",
    "    }).sort_values(by='Correlation', key=abs, ascending=False)\n",
    "    \n",
    "    # Select top k features\n",
    "    return correlation_df.head(k)['Feature'].tolist()\n",
    "\n",
    "def select_features_information_gain(X, y, k=10):\n",
    "    \"\"\"\n",
    "    Select top k features based on information gain with the target.\n",
    "\n",
    "    Parameters:\n",
    "    - X: DataFrame, the feature matrix.\n",
    "    - y: Series, the target variable.\n",
    "    - k: int, number of top features to select.\n",
    "\n",
    "    Returns:\n",
    "    - List of top k features based on information gain.\n",
    "    \"\"\"\n",
    "    # Convert categorical features to numeric\n",
    "    X_encoded = X.apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    # Calculate information gain\n",
    "    info_gain = mutual_info_classif(X_encoded, y, discrete_features='auto', random_state=42)\n",
    "    \n",
    "    # Create a DataFrame for information gain values\n",
    "    info_gain_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Information Gain': info_gain\n",
    "    }).sort_values(by='Information Gain', ascending=False)\n",
    "    \n",
    "    # Select top k features\n",
    "    return info_gain_df.head(k)['Feature'].tolist()\n",
    "\n",
    "def get_top_features_across_files(file_paths, label_column, k=10):\n",
    "    \"\"\"\n",
    "    Get top k features across multiple files, based on both correlation and information gain.\n",
    "\n",
    "    Parameters:\n",
    "    - file_paths: list of file paths.\n",
    "    - label_column: name of the label column.\n",
    "    - k: number of top features to select.\n",
    "\n",
    "    Returns:\n",
    "    - List of top k features consistent across all files.\n",
    "    \"\"\"\n",
    "    feature_counter = Counter()\n",
    "    idx = 1\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path)\n",
    "        X = df.drop(columns=[label_column])\n",
    "        y = df[label_column]\n",
    "        \n",
    "        # Select top features using both methods\n",
    "        top_corr_features = select_features_correlation(X, y, k)\n",
    "        top_ig_features = select_features_information_gain(X, y, k)\n",
    "        \n",
    "        # Combine and count features\n",
    "        feature_counter.update(top_corr_features + top_ig_features)\n",
    "        print(\"File \"+str(idx)+\" done.\")\n",
    "        idx = idx+1\n",
    "    \n",
    "    # Get most common features across files\n",
    "    most_common_features = feature_counter.most_common(k)\n",
    "    return [feature for feature, _ in most_common_features]\n",
    "\n",
    "def keep_selected_features(df, selected_features, label_column):\n",
    "    \"\"\"\n",
    "    Keep only the selected features in the given DataFrame, while leaving the label column intact.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame, the input DataFrame.\n",
    "    - selected_features: list of feature names to keep.\n",
    "    - label_column: str, the name of the target (label) column to be preserved.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with only the selected features and the label column.\n",
    "    \"\"\"\n",
    "    # Extract the label column\n",
    "    label = df[label_column]\n",
    "    \n",
    "    # Drop the label column from the selected features if it's included\n",
    "    if label_column in selected_features:\n",
    "        selected_features.remove(label_column)\n",
    "    \n",
    "    # Keep only the selected features\n",
    "    updated_df = df[selected_features]\n",
    "    \n",
    "    # Re-add the label column\n",
    "    updated_df[label_column] = label\n",
    "    \n",
    "    return updated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_features = get_top_features_across_files(input_files,'Label',20)\n",
    "# print(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = ['Attack', 'TCP_WIN_MAX_IN', 'TCP_FLAGS', 'L7_PROTO', 'MAX_IP_PKT_LEN', 'LONGEST_FLOW_PKT', 'MAX_TTL', 'MIN_TTL', 'SERVER_TCP_FLAGS', 'FLOW_DURATION_MILLISECONDS', 'CLIENT_TCP_FLAGS', 'PROTOCOL', 'DNS_TTL_ANSWER', 'DNS_QUERY_ID', 'MIN_IP_PKT_LEN', 'DNS_QUERY_TYPE', 'SHORTEST_FLOW_PKT', 'DURATION_OUT', 'DURATION_IN']\n",
    "top_features.remove('Attack')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load each file, do label clean up, run KNN on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T13:08:49.335342Z",
     "iopub.status.busy": "2024-11-14T13:08:49.334885Z",
     "iopub.status.idle": "2024-11-14T14:02:29.148390Z",
     "shell.execute_reply": "2024-11-14T14:02:29.146241Z",
     "shell.execute_reply.started": "2024-11-14T13:08:49.335296Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing KNN model\n",
      "1/1 CURRENT FILE: dataset\\downsampled\\downsampled_df_shuffled.csv\n",
      "\n",
      "LOADED FILE dataset\\downsampled\\downsampled_df_shuffled.csv, STARTING CLEANUP\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\AppData\\Local\\Temp\\ipykernel_11924\\902232939.py:118: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  updated_df[label_column] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEANUP DONE, STARTING SPLIT\n",
      "\n",
      "DONE WRITING TEST SETS TO FILES, STARTING TRAIN\n",
      "\n",
      "RUNNING KNN MODEL TRAIN...\n",
      "\n",
      "KNN MODEL TRAINED ON FILE: dataset\\downsampled\\downsampled_df_shuffled.csv \n",
      "\n",
      "\n",
      "1/1 files worked on\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will add to these 3 lists test set and predictions for all csv\n",
    "# then we will merge them in one pandas object\n",
    "# then we can do metrics on the dataset as a whole\n",
    "# pros: will get whole dataset metrics\n",
    "# cons: the 3 list and merged object may overflow 30 gb ram (praying it wont)\n",
    "X_test_sets = []\n",
    "y_test_sets = []\n",
    "\n",
    "# MODELS\n",
    "print(\"Initializing KNN model\")\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "i = 0\n",
    "\n",
    "for f in input_files:\n",
    "    \n",
    "    i += 1\n",
    "        \n",
    "    print(f\"{i}/{len(input_files)} CURRENT FILE: {f}\\n\")\n",
    "    \n",
    "    df = pd.read_csv(f)\n",
    "    \n",
    "    print(f\"LOADED FILE {f}, STARTING CLEANUP\\n\")\n",
    "    \n",
    "    df = keep_selected_features(df,top_features,'Label') \n",
    "    \n",
    "    df = dataset_cleaner(df,'Label')\n",
    "    \n",
    "    print(f\"CLEANUP DONE, STARTING SPLIT\\n\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = get_target_features(df, 0.30)\n",
    "    \n",
    "    # no need for df now\n",
    "    df = None\n",
    "    gc.collect()\n",
    "    \n",
    "    X_test.to_csv(f\"test_chunks/{i}X_test.csv\", index=False)\n",
    "    y_test.to_csv(f\"test_chunks/{i}y_test.csv\", index=False)\n",
    "    \n",
    "    # no need for the test sets either\n",
    "    del X_test\n",
    "    del y_test\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"DONE WRITING TEST SETS TO FILES, STARTING TRAIN\\n\")\n",
    "    \n",
    "    # KNN\n",
    "    print(\"RUNNING KNN MODEL TRAIN...\\n\")\n",
    "    \n",
    "    knn_model.fit(X_train, y_train)\n",
    "    \n",
    "    del X_train\n",
    "    del y_train\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"KNN MODEL TRAINED ON FILE: {f} \\n\\n\")\n",
    "    \n",
    "\n",
    "print(f\"{i}/{len(input_files)} files worked on\\n\\n\")\n",
    "\n",
    "# clear memory\n",
    "df = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict and prepare outputs for metric calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T15:13:31.460024Z",
     "iopub.status.busy": "2024-11-14T15:13:31.459486Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READING X TEST SETS AND PREDICTING\n",
      "\n",
      "Reading X_test file 1/1\n",
      "\n",
      "Read 1-th X_test file, shape: (1367443, 18)\n",
      "Predicting and appending...\n"
     ]
    }
   ],
   "source": [
    "print(\"READING X TEST SETS AND PREDICTING\\n\")\n",
    "gc.collect()\n",
    "\n",
    "knn_predictions = []\n",
    "y_tests = []\n",
    "\n",
    "for curr in range(1,i+1):\n",
    "    print(f\"Reading X_test file {curr}/{i}\\n\")\n",
    "\n",
    "    df_X = pd.read_csv(f\"test_chunks/{curr}X_test.csv\")\n",
    "    \n",
    "    print(f\"Read {curr}-th X_test file, shape: {df_X.shape}\\nPredicting and appending...\")\n",
    "    \n",
    "    pred = knn_model.predict(df_X)\n",
    "    knn_predictions.append(pred)\n",
    "    \n",
    "    del df_X \n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"Done reading and predictiong on X_test file {curr}/{i}\\n\")\n",
    "    \n",
    "\n",
    "knn_predictions_joined = np.concatenate(knn_predictions)\n",
    "\n",
    "# remove prediction list\n",
    "del knn_predictions\n",
    "gc.collect()\n",
    "\n",
    "print(\"PREDICTION DONE USING KNN MODELS: \")\n",
    "print( knn_predictions_joined.shape)\n",
    "\n",
    "print(\"\\nREADING y_test FILES\\n\")\n",
    "\n",
    "for curr in range(1,i+1):\n",
    "    print(f\"Reading y_test file {curr}/{i}\\n\")\n",
    "\n",
    "    df_y = pd.read_csv(f\"test_chunks/{curr}y_test.csv\")\n",
    "    \n",
    "    print(f\"Read {curr}-th y_test file, shape: {df_y.shape}\\nAppending...\")\n",
    "    \n",
    "    y_tests.append(df_y)\n",
    "    \n",
    "    del df_y\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"Done reading and adding y_test file {curr}/{i}\\n\")\n",
    "    \n",
    "y_test_sets_joined = pd.concat(y_tests)\n",
    "\n",
    "# remove y_tests\n",
    "del y_tests\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "print(\"DONE READING y_test FILES: \")\n",
    "print(y_test_sets_joined.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-14T14:02:29.172962Z",
     "iopub.status.idle": "2024-11-14T14:02:29.173578Z",
     "shell.execute_reply": "2024-11-14T14:02:29.173290Z",
     "shell.execute_reply.started": "2024-11-14T14:02:29.173248Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "knn_accuracy = accuracy_score(y_test_sets_joined, knn_predictions_joined)\n",
    "knn_f1 = f1_score(y_test_sets_joined, knn_predictions_joined, average='macro')\n",
    "knn_precision = precision_score(y_test_sets_joined, knn_predictions_joined, average='macro')\n",
    "knn_recall = recall_score(y_test_sets_joined, knn_predictions_joined, average='macro')\n",
    "\n",
    "print(\"KNN METRICS: \\n\")\n",
    "print(f\"ACCURACY: \\t {knn_accuracy:.4f}\")\n",
    "print(f\"F1 SCORE: \\t {knn_f1:.4f}\")\n",
    "print(f\"PRECISION: \\t {knn_precision:.4f}\")\n",
    "print(f\"RECALL: \\t {knn_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-14T14:02:29.176215Z",
     "iopub.status.idle": "2024-11-14T14:02:29.177023Z",
     "shell.execute_reply": "2024-11-14T14:02:29.176649Z",
     "shell.execute_reply.started": "2024-11-14T14:02:29.176612Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "classes = [\"BENIGN\", \"Attack\"]\n",
    "title = \"Confusion Matrix for KNN Classifier\"\n",
    "\n",
    "cm = confusion_matrix(y_test_sets_joined, knn_predictions_joined)\n",
    "plt.figure(figsize=(14, 14))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.title(title)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filename=\"model.pkl\"):\n",
    "    with open(filename, \"wb\") as file:\n",
    "        pickle.dump(model, file)\n",
    "    print(f\"Model saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename=\"model.pkl\"):\n",
    "    with open(filename, \"rb\") as file:\n",
    "        model = pickle.load(file)\n",
    "    print(f\"Model loaded from {filename}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a pickle file            \n",
    "save_model(knn_model,\"models/knn.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3629354,
     "sourceId": 6308217,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
