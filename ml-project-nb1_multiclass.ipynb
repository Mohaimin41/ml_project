{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports done\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import time\n",
    "from enum import Enum\n",
    "from typing import Optional, Tuple, List, Union\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "try:\n",
    "    from tensorflow._api.v2.v2 import keras\n",
    "except ImportError:\n",
    "    from tensorflow import keras\n",
    "\n",
    "from keras import Input, Model\n",
    "import keras.layers as layers\n",
    "from keras.layers import Dense, Conv1D, Layer, MultiHeadAttention, Dropout, LayerNormalization, Embedding, Concatenate, Reshape, Lambda, Flatten, GlobalAveragePooling1D\n",
    "\n",
    "print(\"Imports done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import hashlib\n",
    "import json\n",
    "import pickle\n",
    "from typing import Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_identifier(d:dict):\n",
    "    raw_json = json.dumps(d, sort_keys=True, indent=False)\n",
    "    hash = hashlib.sha1(raw_json.encode(\"utf8\")).digest()\n",
    "    x = base64.b64encode(hash)\n",
    "    x = x.decode(\"ASCII\")\n",
    "    x = x.replace(\"+\", \"0\").replace(\"=\", \"0\").replace(\"/\", \"0\")\n",
    "    return x\n",
    "\n",
    "def save_feather_plus_metadata(save_path:str, df:pd.DataFrame, metadata:object):\n",
    "    metadata_path = save_path + \".metadata.pickle\"\n",
    "    df.to_feather(save_path)\n",
    "    with open(metadata_path, \"wb\") as w:\n",
    "        pickle.dump(metadata, w)\n",
    "\n",
    "def save_pickle(save_path:str, obj:dict):\n",
    "    with open(save_path, \"wb\") as w:\n",
    "        pickle.dump(obj, w)\n",
    "\n",
    "def load_pickle(save_path:str):\n",
    "    with open(save_path, \"rb\") as r:\n",
    "        return pickle.load(r, fix_imports=True)\n",
    "\n",
    "def load_feather_plus_metadata(load_path:str) -> Tuple[pd.DataFrame, object]:\n",
    "    metadata_path = load_path + \".metadata.pickle\"\n",
    "    with open(metadata_path, \"rb\") as r:\n",
    "        metadata = pickle.load(r, fix_imports=True)\n",
    "    data = pd.read_feather(load_path)\n",
    "    return data, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dataset Specifications: List some metadata, including column names for ease of use**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DatasetSpecification:\n",
    "    def  __init__(self, include_fields:List[str], categorical_fields:List[str], class_column:str, benign_label:str, test_column:Optional[str]=None):\n",
    "        \"\"\"\n",
    "        Defines the format of specific NIDS dataset\n",
    "        :param include_fields: The fields to include as part of classification\n",
    "        :param categorical_fields: Fields that should be treated as categorical\n",
    "        :param class_column: The column name that includes the class of the flow, eg. DDoS or Benign\n",
    "        :param benign_label: The label of benign traffic, eg. Benign or 0\n",
    "        :param test_column: The column indicating if this row is a member of the test or training dataset\n",
    "        \"\"\"\n",
    "        self.include_fields:List[str] = include_fields\n",
    "        self.categorical_fields:List[str] = categorical_fields\n",
    "        self.class_column = class_column\n",
    "        self.benign_label = benign_label\n",
    "        self.test_column:Optional[str] = test_column\n",
    "\n",
    "class NamedDatasetSpecifications:\n",
    "    \"\"\"\n",
    "    Example specifications of some common datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    cse_cic_ids_2018 = DatasetSpecification(\n",
    "            include_fields=['NUM_PKTS_UP_TO_128_BYTES', 'SRC_TO_DST_SECOND_BYTES', 'OUT_PKTS', 'OUT_BYTES', 'NUM_PKTS_128_TO_256_BYTES', 'DST_TO_SRC_AVG_THROUGHPUT', 'DURATION_IN', 'L4_SRC_PORT', 'ICMP_TYPE', 'PROTOCOL', 'SERVER_TCP_FLAGS', 'IN_PKTS', 'NUM_PKTS_512_TO_1024_BYTES', 'CLIENT_TCP_FLAGS', 'TCP_WIN_MAX_IN', 'NUM_PKTS_256_TO_512_BYTES', 'SHORTEST_FLOW_PKT', 'MIN_IP_PKT_LEN', 'LONGEST_FLOW_PKT', 'L4_DST_PORT', 'MIN_TTL', 'DST_TO_SRC_SECOND_BYTES', 'NUM_PKTS_1024_TO_1514_BYTES', 'DURATION_OUT', 'FLOW_DURATION_MILLISECONDS', 'TCP_FLAGS', 'MAX_TTL', 'SRC_TO_DST_AVG_THROUGHPUT', 'ICMP_IPV4_TYPE', 'MAX_IP_PKT_LEN', 'RETRANSMITTED_OUT_BYTES', 'IN_BYTES', 'RETRANSMITTED_IN_BYTES', 'TCP_WIN_MAX_OUT', 'L7_PROTO', 'RETRANSMITTED_OUT_PKTS', 'RETRANSMITTED_IN_PKTS'],\n",
    "            categorical_fields=['CLIENT_TCP_FLAGS', 'L4_SRC_PORT', 'TCP_FLAGS', 'ICMP_IPV4_TYPE', 'ICMP_TYPE', 'PROTOCOL', 'SERVER_TCP_FLAGS', 'L4_DST_PORT', 'L7_PROTO', 'Attack'],\n",
    "            class_column=\"Attack\",\n",
    "            benign_label=\"Benign\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Some helper classes**\n",
    "\n",
    "these are useful in, including but not limited to: dataset column datatypes, main class parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CategoricalFormat(Enum):\n",
    "    \"\"\"\n",
    "    The format of variables expected by the model as input\n",
    "    \"\"\"\n",
    "    Integers = 0,\n",
    "    \"\"\"\n",
    "    If categorical values should be dictionary encoded as integers\n",
    "    \"\"\"\n",
    "    OneHot = 1\n",
    "    \"\"\"\n",
    "    If categorical values should be one-hot encoded\n",
    "    \"\"\"\n",
    "\n",
    "class EvaluationDatasetSampling(Enum):\n",
    "    \"\"\"\n",
    "    How to choose evaluation samples from the raw dataset\n",
    "    \"\"\"\n",
    "    LastRows = 0\n",
    "    \"\"\"\n",
    "    Take the last rows in the dataset to form the evaluation dataset\n",
    "    \"\"\"\n",
    "    RandomRows  = 1\n",
    "    \"\"\"\n",
    "    Randomly sample rows to make up the evaluation dataset\n",
    "    \"\"\"\n",
    "    FilterColumn = 2\n",
    "    \"\"\"\n",
    "    Define a column that contains a flag indicating if this row is part of the evaluation set\n",
    "    \"\"\"\n",
    "\n",
    "class FlowTransformerParameters:\n",
    "    \"\"\"\n",
    "    Allows the configuration of overall parameters of the FlowTransformer\n",
    "    :param window_size: The number of flows to use in each window\n",
    "    :param mlp_layer_sizes: The number of nodes in each layer of the outer classification MLP of FlowTransformer\n",
    "    :param mlp_dropout: The amount of dropout to be applied between the layers of the outer classification MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, window_size:int, mlp_layer_sizes:List[int], mlp_dropout:float=0.1):\n",
    "        self.window_size:int = window_size\n",
    "        self.mlp_layer_sizes = mlp_layer_sizes\n",
    "        self.mlp_dropout = mlp_dropout\n",
    "\n",
    "        # Is the order of flows important within any individual window\n",
    "        self._train_ensure_flows_are_ordered_within_windows = True\n",
    "\n",
    "        # Should windows be sampled sequentially during training\n",
    "        self._train_draw_sequential_windows = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Input Enum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ModelInputSpecification:\n",
    "    def __init__(self, feature_names:List[str], n_numeric_features:int, levels_per_categorical_feature:List[int], categorical_format:CategoricalFormat):\n",
    "        self.feature_names = feature_names\n",
    "\n",
    "        self.numeric_feature_names = feature_names[:n_numeric_features]\n",
    "        self.categorical_feature_names = feature_names[n_numeric_features:]\n",
    "        self.categorical_format:CategoricalFormat = categorical_format\n",
    "\n",
    "        self.n_numeric_features = n_numeric_features\n",
    "        self.levels_per_categorical_feature = levels_per_categorical_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Framework Component class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Component():\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def parameters(self) -> dict:\n",
    "        warnings.warn(\"Parameters have not been implemented for this class!\")\n",
    "        return {}\n",
    "class FunctionalComponent(Component):\n",
    "    def __init__(self):\n",
    "        self.sequence_length: Optional[int] = None\n",
    "        self.model_input_specification: Optional[ModelInputSpecification] = None\n",
    "        self.input_shape: Optional[Tuple[int]] = None\n",
    "\n",
    "    def apply(self, X, prefix: str = None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def build(self, sequence_length:int, model_input_specification:ModelInputSpecification):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.model_input_specification = model_input_specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Base Classification Head**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BaseClassificationHead(FunctionalComponent):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def apply_before_transformer(self, X, prefix:str=None):\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Classification Head: Last token, FeatureWise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LastTokenClassificationHead(BaseClassificationHead):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def apply(self, X, prefix: str = None):\n",
    "        if prefix is None:\n",
    "            prefix = \"\"\n",
    "\n",
    "        x = Lambda(lambda x: x[..., -1, :], name=f\"{prefix}slice_last\")(X)\n",
    "        #x = Flatten(name=f\"{prefix}flatten_last\")(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"Last Token\"\n",
    "\n",
    "    @property\n",
    "    def parameters(self) -> dict:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturewiseEmbedding(BaseClassificationHead):\n",
    "    def __init__(self, project:bool=False):\n",
    "        super().__init__()\n",
    "        self.project: bool = project\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        if self.project:\n",
    "            return f\"Featurewise Embed - Projection\"\n",
    "        else:\n",
    "            return f\"Featurewise Embed - Dense\"\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return {}\n",
    "\n",
    "\n",
    "    def apply(self, X, prefix:str=None):\n",
    "        if prefix is None:\n",
    "            prefix = \"\"\n",
    "\n",
    "        if self.model_input_specification is None:\n",
    "            raise Exception(\"Please call build() before calling apply!\")\n",
    "\n",
    "        x = Dense(1,\n",
    "                  activation=\"linear\",\n",
    "                  use_bias=(not self.project),\n",
    "                  name=f\"{prefix}featurewise_embed\")(X)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Input Encoding:Record Level Projection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BaseInputEncoding(FunctionalComponent):\n",
    "    def apply(self, X:List[\"keras.Input\"], prefix: str = None):\n",
    "        raise NotImplementedError(\"Please override this with a custom implementation\")\n",
    "\n",
    "    @property\n",
    "    def required_input_format(self) -> CategoricalFormat:\n",
    "        raise NotImplementedError(\"Please override this with a custom implementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EmbedLayerType(Enum):\n",
    "    Dense = 0,\n",
    "    Lookup = 1,\n",
    "    Projection = 2\n",
    "\n",
    "class RecordLevelEmbed(BaseInputEncoding):\n",
    "    def __init__(self, embed_dimension: int, project:bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dimension: int = embed_dimension\n",
    "        self.project: bool = project\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        if self.project:\n",
    "            return \"Record Level Projection\"\n",
    "        return \"Record Level Embedding\"\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return {\n",
    "            \"dimensions_per_feature\": self.embed_dimension\n",
    "        }\n",
    "\n",
    "    def apply(self, X:List[keras.Input], prefix: str = None):\n",
    "        if prefix is None:\n",
    "            prefix = \"\"\n",
    "\n",
    "        assert self.model_input_specification.categorical_format == CategoricalFormat.OneHot\n",
    "\n",
    "        x = Concatenate(name=f\"{prefix}feature_concat\", axis=-1)(X)\n",
    "        x = Dense(self.embed_dimension, activation=\"linear\", use_bias=not self.project, name=f\"{prefix}embed\")(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def required_input_format(self) -> CategoricalFormat:\n",
    "        return CategoricalFormat.OneHot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Input Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BasePreProcessing(Component):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit_numerical(self, column_name:str, values:np.array):\n",
    "        raise NotImplementedError(\"Please override this base class with a custom implementation\")\n",
    "\n",
    "    def transform_numerical(self, column_name:str, values: np.array):\n",
    "        raise NotImplementedError(\"Please override this base class with a custom implementation\")\n",
    "\n",
    "    def fit_categorical(self, column_name:str, values:np.array):\n",
    "        raise NotImplementedError(\"Please override this base class with a custom implementation\")\n",
    "\n",
    "    def transform_categorical(self, column_name:str, values:np.array, expected_categorical_format:CategoricalFormat):\n",
    "        raise NotImplementedError(\"Please override this base class with a custom implementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class StandardPreProcessing(BasePreProcessing):\n",
    "    def __init__(self, n_categorical_levels: int, clip_numerical_values:bool=False):\n",
    "        super().__init__()\n",
    "        self.n_categorical_levels:int = n_categorical_levels\n",
    "        self.clip_numerical_values:bool = clip_numerical_values\n",
    "        self.min_range = {}\n",
    "        self.encoded_levels = {}\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"Standard Preprocessing\"\n",
    "\n",
    "    @property\n",
    "    def parameters(self) -> dict:\n",
    "        return {\n",
    "            \"n_categorical_levels\": self.n_categorical_levels,\n",
    "            \"clip_numerical_values\": self.clip_numerical_values\n",
    "        }\n",
    "\n",
    "    def fit_numerical(self, column_name: str, values: np.array):\n",
    "\n",
    "        v0 = np.min(values)\n",
    "        v1 = np.max(values)\n",
    "        r = v1 - v0\n",
    "\n",
    "        self.min_range[column_name] = (v0, r)\n",
    "\n",
    "    def transform_numerical(self, column_name: str, values: np.array):\n",
    "        col_min, col_range = self.min_range[column_name]\n",
    "\n",
    "        if col_range == 0:\n",
    "            return np.zeros_like(values, dtype=\"float32\")\n",
    "\n",
    "        # center on zero\n",
    "        values -= col_min\n",
    "\n",
    "        # apply a logarithm\n",
    "        col_values = np.log(values + 1)\n",
    "\n",
    "        # scale max to 1\n",
    "        col_values *= 1. / np.log(col_range + 1)\n",
    "\n",
    "        if self.clip_numerical_values:\n",
    "            col_values = np.clip(col_values, 0., 1.)\n",
    "\n",
    "        return col_values\n",
    "\n",
    "    def fit_categorical(self, column_name: str, values: np.array):\n",
    "        levels, level_counts = np.unique(values, return_counts=True)\n",
    "        sorted_levels = list(sorted(zip(levels, level_counts), key=lambda x: x[1], reverse=True))\n",
    "        self.encoded_levels[column_name] = [s[0] for s in sorted_levels[:self.n_categorical_levels]]\n",
    "\n",
    "\n",
    "    def transform_categorical(self, column_name:str, values: np.array, expected_categorical_format: CategoricalFormat):\n",
    "        encoded_levels = self.encoded_levels[column_name]\n",
    "        print(f\"Encoding the {len(encoded_levels)} levels for {column_name}\")\n",
    "\n",
    "        result_values = np.ones(len(values), dtype=\"uint32\")\n",
    "        for level_i, level in enumerate(encoded_levels):\n",
    "            level_mask = values == level\n",
    "\n",
    "            # we use +1 here, as 0 = previously unseen, and 1 to (n + 1) are the encoded levels\n",
    "            result_values[level_mask] = level_i + 1\n",
    "\n",
    "        if expected_categorical_format == CategoricalFormat.Integers:\n",
    "            return result_values\n",
    "\n",
    "        v = pd.get_dummies(result_values, prefix=column_name)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Transformer classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BaseSequential(FunctionalComponent):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoders Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(Layer):\n",
    "    def __init__(self, input_dimension:int, inner_dimension:int, num_heads:int, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.input_dimension = input_dimension\n",
    "        self.inner_dimension = inner_dimension\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=input_dimension)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(inner_dimension, activation='relu'),\n",
    "            Dense(input_dimension)\n",
    "        ])\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    # noinspection PyMethodOverriding\n",
    "    def call(self, inputs, training, mask=None):\n",
    "        # inputs = (target_seq, enc_output)\n",
    "        target_seq = inputs\n",
    "        enc_output = inputs\n",
    "\n",
    "        # self attention of target_seq\n",
    "        attn_output = self.mha(target_seq, target_seq)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = target_seq + attn_output\n",
    "        out1 = self.layernorm1(out1)\n",
    "\n",
    "        # multi-head attention with encoder output as the key and value, and target_seq as the query\n",
    "        attn_output = self.mha(out1, enc_output)\n",
    "        attn_output = self.dropout2(attn_output, training=training)\n",
    "        out2 = out1 + attn_output\n",
    "        out2 = self.layernorm2(out2)\n",
    "\n",
    "        # feed forward network\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out3 = out2 + ffn_output\n",
    "        out3 = self.layernorm2(out3)\n",
    "\n",
    "        return out3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GPT3Attention(layers.Layer):\n",
    "    def __init__(self, n_heads, d_model, dropout_rate=0.1):\n",
    "        super(GPT3Attention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // n_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.n_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # noinspection PyMethodOverriding\n",
    "    def call(self, q, k, v, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        scaled_attention_logits = tf.matmul(q, k, transpose_b=True)\n",
    "        scaled_attention_logits = scaled_attention_logits / tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
    "        output = tf.reshape(output, (batch_size, -1, self.d_model))\n",
    "\n",
    "        output = self.dense(output)\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class MultiHeadAttentionImplementation:\n",
    "    Keras = 0,\n",
    "    GPT3 = 1\n",
    "\n",
    "class TransformerEncoderBlock(layers.Layer):\n",
    "    def __init__(self, input_dimension:int, inner_dimension:int, num_heads:int, dropout_rate=0.1, use_conv:bool=False, prefix:str=None, attn_implementation:MultiHeadAttentionImplementation = MultiHeadAttentionImplementation.Keras):\n",
    "\n",
    "        if prefix is None:\n",
    "            prefix = \"\"\n",
    "\n",
    "        super().__init__(name=f\"{prefix}transformer_encoder\")\n",
    "\n",
    "        if inner_dimension < input_dimension:\n",
    "            warnings.warn(f\"Typically inner_dimension should be greater than or equal to the input_dimension!\")\n",
    "\n",
    "        self.attn_implementation = attn_implementation\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.attention = \\\n",
    "            layers.MultiHeadAttention(num_heads=num_heads, key_dim=inner_dimension, name=f\"{prefix}multi_head_attn\") \\\n",
    "                if attn_implementation == MultiHeadAttentionImplementation.Keras else\\\n",
    "                GPT3Attention(num_heads, inner_dimension, dropout_rate=0.0)\n",
    "\n",
    "        layer_norm = 1e-6\n",
    "\n",
    "        self.attention_dropout = layers.Dropout(dropout_rate, name=f\"{prefix}attention_dropout\")\n",
    "        self.attention_layer_norm = layers.LayerNormalization(epsilon=layer_norm, name=f\"{prefix}attention_layer_norm\")\n",
    "\n",
    "        self.feed_forward_0 = Conv1D(filters=inner_dimension, kernel_size=1, activation=\"relu\", name=f\"{prefix}feed_forward_0\") \\\n",
    "            if use_conv else Dense(inner_dimension, activation=\"relu\", name=f\"{prefix}feed_forward_0\")\n",
    "        self.feed_forward_1 = Conv1D(filters=input_dimension, kernel_size=1, activation=\"relu\", name=f\"{prefix}feed_forward_1\") \\\n",
    "            if use_conv else Dense(input_dimension, activation=\"relu\", name=f\"{prefix}feed_forward_1\")\n",
    "\n",
    "        self.feed_forward_dropout = layers.Dropout(dropout_rate, name=f\"{prefix}feed_forward_dropout\")\n",
    "        self.feed_forward_layer_norm = layers.LayerNormalization(epsilon=layer_norm, name=f\"{prefix}feed_forward_layer_norm\")\n",
    "\n",
    "    # noinspection PyMethodOverriding\n",
    "    def call(self, inputs, training, mask=None):\n",
    "        x = inputs\n",
    "        x = self.attention(x, x) if self.attn_implementation == MultiHeadAttentionImplementation.Keras else self.attention(x, x, x, mask)\n",
    "\n",
    "        attention_output = self.attention_dropout(x, training=training) if self.dropout_rate > 0 else x\n",
    "\n",
    "        x = inputs + attention_output\n",
    "        x = self.attention_layer_norm(x)\n",
    "        x = self.feed_forward_0(x)\n",
    "        x = self.feed_forward_1(x)\n",
    "        x = self.feed_forward_dropout(x, training=training) if self.dropout_rate > 0 else x\n",
    "        feed_forward_output = x\n",
    "\n",
    "        return self.feed_forward_layer_norm(attention_output + feed_forward_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BasicTransformer(BaseSequential):\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        if self.use_conv:\n",
    "            return f\"Basic Conv Transformer\" + (\" Decoder\" if self.is_decoder else \"\")\n",
    "        else:\n",
    "            return f\"Basic Dense Transformer\" + (\" Decoder\" if self.is_decoder else \"\")\n",
    "\n",
    "    @property\n",
    "    def parameters(self) -> dict:\n",
    "        return {\n",
    "            \"n_layers\": self.n_layers,\n",
    "            \"internal_size\": self.internal_size,\n",
    "            \"use_conv\": self.use_conv,\n",
    "            \"n_heads\": self.n_heads,\n",
    "            \"dropout_rate\": self.dropout_rate,\n",
    "            \"head_size\": self.internal_size\n",
    "        }\n",
    "\n",
    "    def __init__(self, n_layers:int, internal_size:int, n_heads:int, use_conv:bool=False, dropout_rate:float=0.1, is_decoder=False):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.internal_size = internal_size\n",
    "        self.use_conv = use_conv\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "    def apply(self, X, prefix: str = None):\n",
    "        #window_size = self.sequence_length\n",
    "        real_size = X.shape[-1]\n",
    "\n",
    "        m_x = X\n",
    "\n",
    "        for layer_i in range(self.n_layers):\n",
    "            if self.is_decoder:\n",
    "                if self.use_conv:\n",
    "                    raise NotImplementedError()\n",
    "                m_x = TransformerDecoderBlock(real_size, self.internal_size, self.n_heads, dropout_rate=self.dropout_rate)(m_x)\n",
    "            else:\n",
    "                m_x = TransformerEncoderBlock(real_size, self.internal_size, self.n_heads, dropout_rate=self.dropout_rate, use_conv=self.use_conv, prefix=f\"{prefix}block_{layer_i}_\")(m_x)\n",
    "\n",
    "        return m_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GPTSmallTransformer(BaseSequential):\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"GPT Model\"\n",
    "\n",
    "    @property\n",
    "    def parameters(self) -> dict:\n",
    "        return {\n",
    "            \"n_layers\": self.n_layers,\n",
    "            \"internal_size\": self.internal_size,\n",
    "            \"n_heads\": self.n_heads,\n",
    "            \"dropout_rate\": self.dropout_rate,\n",
    "            \"head_size\": self.head_size\n",
    "        }\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_layers = 12\n",
    "        self.internal_size = 768\n",
    "        self.n_heads = 12\n",
    "        self.head_size = self.internal_size / self.n_heads\n",
    "        self.dropout_rate = 0.02\n",
    "        self.is_decoder = True\n",
    "\n",
    "    def apply(self, X, prefix: str = None):\n",
    "        #window_size = self.sequence_length\n",
    "        real_size = X.shape[-1]\n",
    "\n",
    "        m_x = X\n",
    "\n",
    "        for layer_i in range(self.n_layers):\n",
    "            m_x = TransformerDecoderBlock(real_size, self.internal_size, self.n_heads, dropout_rate=self.dropout_rate)(m_x)\n",
    "\n",
    "        return m_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Main FlowTransformer T_T**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FlowTransformer:\n",
    "    retain_inmem_cache = False\n",
    "    inmem_cache = None\n",
    "\n",
    "    def  __init__(self, pre_processing:BasePreProcessing,\n",
    "                  input_encoding:BaseInputEncoding,\n",
    "                  sequential_model:FunctionalComponent,\n",
    "                  classification_head:BaseClassificationHead,\n",
    "                  params:FlowTransformerParameters,\n",
    "                  rs:np.random.RandomState=None):\n",
    "\n",
    "        self.rs = np.random.RandomState() if rs is None else rs\n",
    "        self.classification_head = classification_head\n",
    "        self.sequential_model = sequential_model\n",
    "        self.input_encoding = input_encoding\n",
    "        self.pre_processing = pre_processing\n",
    "        self.parameters = params\n",
    "\n",
    "        self.dataset_specification: Optional[DatasetSpecification] = None\n",
    "\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "        self.training_mask = None\n",
    "        self.model_input_spec: Optional[ModelInputSpecification] = None\n",
    "\n",
    "        self.experiment_key = {}\n",
    "\n",
    "        self.y_backup: pd.Series = pd.Series()\n",
    "\n",
    "    def build_model(self, prefix:str=None):\n",
    "        if prefix is None:\n",
    "            prefix = \"\"\n",
    "\n",
    "        if self.X is None:\n",
    "            raise Exception(\"Please call load_dataset before calling build_model()\")\n",
    "\n",
    "        m_inputs = []\n",
    "        for numeric_feature in self.model_input_spec.numeric_feature_names:\n",
    "            m_input = Input((self.parameters.window_size, 1), name=f\"{prefix}input_{numeric_feature}\", dtype=\"float32\")\n",
    "            m_inputs.append(m_input)\n",
    "\n",
    "        for categorical_feature_name, categorical_feature_levels in \\\n",
    "            zip(self.model_input_spec.categorical_feature_names, self.model_input_spec.levels_per_categorical_feature):\n",
    "            m_input = Input(\n",
    "                (self.parameters.window_size, 1 if self.model_input_spec.categorical_format == CategoricalFormat.Integers else categorical_feature_levels),\n",
    "                name=f\"{prefix}input_{categorical_feature_name}\",\n",
    "                dtype=\"int32\" if self.model_input_spec.categorical_format == CategoricalFormat.Integers else \"float32\"\n",
    "            )\n",
    "            m_inputs.append(m_input)\n",
    "\n",
    "        self.input_encoding.build(self.parameters.window_size, self.model_input_spec)\n",
    "        self.sequential_model.build(self.parameters.window_size, self.model_input_spec)\n",
    "        self.classification_head.build(self.parameters.window_size, self.model_input_spec)\n",
    "\n",
    "        m_x = self.input_encoding.apply(m_inputs, prefix)\n",
    "\n",
    "        # in case the classification head needs to add tokens at this stage\n",
    "        m_x = self.classification_head.apply_before_transformer(m_x, prefix)\n",
    "\n",
    "        m_x = self.sequential_model.apply(m_x, prefix)\n",
    "        m_x = self.classification_head.apply(m_x, prefix)\n",
    "\n",
    "        for layer_i, layer_size in enumerate(self.parameters.mlp_layer_sizes):\n",
    "            m_x = Dense(layer_size, activation=\"relu\", name=f\"{prefix}classification_mlp_{layer_i}_{layer_size}\")(m_x)\n",
    "            m_x = Dropout(self.parameters.mlp_dropout)(m_x) if self.parameters.mlp_dropout > 0 else m_x\n",
    "\n",
    "        #### CNG\n",
    "        m_x = Dense(15, activation=\"softmax\", name=f\"{prefix}multiclass_classification_out\")(m_x)\n",
    "        m = Model(m_inputs, m_x)\n",
    "        #m.summary()\n",
    "        return m\n",
    "\n",
    "    def _load_preprocessed_dataset(self, dataset_name:str,\n",
    "                     dataset:Union[pd.DataFrame, str],\n",
    "                     specification:DatasetSpecification,\n",
    "                     cache_folder:Optional[str]=None,\n",
    "                     n_rows:int=0,\n",
    "                     evaluation_dataset_sampling:EvaluationDatasetSampling=EvaluationDatasetSampling.LastRows,\n",
    "                     evaluation_percent:float=0.2,\n",
    "                     numerical_filter=1_000_000_000) -> Tuple[pd.DataFrame, ModelInputSpecification]:\n",
    "\n",
    "        cache_file_path = None\n",
    "\n",
    "        if dataset_name is None:\n",
    "            raise Exception(f\"Dataset name must be specified so FlowTransformer can optimise operations between subsequent calls!\")\n",
    "\n",
    "        pp_key = get_identifier(\n",
    "            {\n",
    "                \"__preprocessing_name\": self.pre_processing.name,\n",
    "                **self.pre_processing.parameters\n",
    "            }\n",
    "        )\n",
    "\n",
    "        local_key = get_identifier({\n",
    "            \"evaluation_percent\": evaluation_percent,\n",
    "            \"numerical_filter\": numerical_filter,\n",
    "            \"categorical_method\": str(self.input_encoding.required_input_format),\n",
    "            \"n_rows\": n_rows,\n",
    "        })\n",
    "\n",
    "        cache_key = f\"{dataset_name}_{n_rows}_{pp_key}_{local_key}\"\n",
    "\n",
    "        if FlowTransformer.retain_inmem_cache:\n",
    "            if FlowTransformer.inmem_cache is not None and cache_key in FlowTransformer.inmem_cache:\n",
    "                print(f\"Using in-memory cached version of this pre-processed dataset. To turn off this functionality set FlowTransformer.retain_inmem_cache = False\")\n",
    "                return FlowTransformer.inmem_cache[cache_key]\n",
    "\n",
    "        if cache_folder is not None:\n",
    "            cache_file_name = f\"{cache_key}.feather\"\n",
    "            cache_file_path = os.path.join(cache_folder, cache_file_name)\n",
    "\n",
    "            print(f\"Using cache file path: {cache_file_path}\")\n",
    "\n",
    "            if os.path.exists(cache_file_path):\n",
    "                print(f\"Reading directly from cache {cache_file_path}...\")\n",
    "                model_input_spec: ModelInputSpecification\n",
    "                dataset, model_input_spec = load_feather_plus_metadata(cache_file_path)\n",
    "                return dataset, model_input_spec\n",
    "\n",
    "        if isinstance(dataset, str):\n",
    "            print(f\"Attempting to read dataset from path {dataset}...\")\n",
    "            if dataset.lower().endswith(\".feather\"):\n",
    "                # read as a feather file\n",
    "                dataset = pd.read_feather(dataset, columns=specification.include_fields+[specification.class_column])\n",
    "            elif dataset.lower().endswith(\".csv\"):\n",
    "                dataset = pd.read_csv(dataset, nrows=n_rows if n_rows > 0 else None)\n",
    "            else:\n",
    "                raise Exception(\"Unrecognised dataset filetype!\")\n",
    "        elif not isinstance(dataset, pd.DataFrame):\n",
    "            raise Exception(\"Unrecognised dataset input type, should be a path to a CSV or feather file, or a pandas dataframe!\")\n",
    "\n",
    "        assert isinstance(dataset, pd.DataFrame)\n",
    "\n",
    "        if 0 < n_rows < len(dataset):\n",
    "            dataset = dataset.iloc[:n_rows]\n",
    "\n",
    "        training_mask = np.ones(len(dataset),  dtype=bool)\n",
    "        eval_n = int(len(dataset) * evaluation_percent)\n",
    "\n",
    "        if evaluation_dataset_sampling == EvaluationDatasetSampling.FilterColumn:\n",
    "            if dataset.columns[-1] != specification.test_column:\n",
    "                raise Exception(f\"Ensure that the 'test' ({specification.test_column}) column is the last column of the dataset being loaded, and that the name of this column is provided as part of the dataset specification\")\n",
    "\n",
    "        if evaluation_dataset_sampling != EvaluationDatasetSampling.LastRows:\n",
    "            warnings.warn(\"Using EvaluationDatasetSampling options other than LastRows might leak some information during training, if for example the context window leading up to a particular flow contains an evaluation flow, and this flow has out of range values (out of range to when pre-processing was applied on the training flows), then the model might potentially learn to handle these. In any case, no class leakage is present.\")\n",
    "\n",
    "        if evaluation_dataset_sampling == EvaluationDatasetSampling.LastRows:\n",
    "            training_mask[-eval_n:] = False\n",
    "        elif evaluation_dataset_sampling == EvaluationDatasetSampling.RandomRows:\n",
    "            index = np.arange(self.parameters.window_size, len(dataset))\n",
    "            sample = self.rs.choice(index, eval_n, replace=False)\n",
    "            training_mask[sample] = False\n",
    "        elif evaluation_dataset_sampling == EvaluationDatasetSampling.FilterColumn:\n",
    "            # must be the last column of the dataset\n",
    "            training_column = dataset.columns[-1]\n",
    "            print(f\"Using the last column {training_column} as the training mask column\")\n",
    "\n",
    "            v, c = np.unique(dataset[training_column].values,  return_counts=True)\n",
    "            min_index = np.argmin(c)\n",
    "            min_v = v[min_index]\n",
    "\n",
    "            warnings.warn(f\"Autodetected class {min_v} of {training_column} to represent the evaluation class!\")\n",
    "\n",
    "            eval_indices = np.argwhere(dataset[training_column].values == min_v).reshape(-1)\n",
    "            eval_indices = eval_indices[(eval_indices > self.parameters.window_size)]\n",
    "\n",
    "            training_mask[eval_indices] = False\n",
    "            del dataset[training_column]\n",
    "\n",
    "        numerical_columns = set(specification.include_fields).difference(specification.categorical_fields)\n",
    "        categorical_columns = specification.categorical_fields\n",
    "\n",
    "        print(f\"Set y to = {specification.class_column}\")\n",
    "        # print(f\"One hot encoding y...\")\n",
    "\n",
    "        new_df = {\"__training\": training_mask, \"__y\": dataset[specification.class_column].values}\n",
    "        new_features = []\n",
    "\n",
    "        print(\"Converting numerical columns to floats, and removing out of range values...\")\n",
    "        for col_name in numerical_columns:\n",
    "            assert col_name in dataset.columns\n",
    "            new_features.append(col_name)\n",
    "\n",
    "            col_values = dataset[col_name].values\n",
    "            col_values[~np.isfinite(col_values)] = 0\n",
    "            col_values[col_values < -numerical_filter] = 0\n",
    "            col_values[col_values > numerical_filter] = 0\n",
    "            col_values = col_values.astype(\"float32\")\n",
    "\n",
    "            if not np.all(np.isfinite(col_values)):\n",
    "                raise Exception(\"Flow format data had non finite values after float transformation!\")\n",
    "\n",
    "            new_df[col_name] = col_values\n",
    "\n",
    "        print(f\"Applying pre-processing to numerical values\")\n",
    "        for i, col_name in enumerate(numerical_columns):\n",
    "            print(f\"[Numerical {i+1:,} / {len(numerical_columns)}] Processing numerical column {col_name}...\")\n",
    "            all_data = new_df[col_name]\n",
    "            training_data = all_data[training_mask]\n",
    "\n",
    "            self.pre_processing.fit_numerical(col_name, training_data)\n",
    "            new_df[col_name] = self.pre_processing.transform_numerical(col_name, all_data)\n",
    "\n",
    "        print(f\"Applying pre-processing to categorical values\")\n",
    "        print(\"Keeping a copy of label column\")\n",
    "        y_backup = new_df['__y']\n",
    "        y_backup = pd.Series(y_backup)\n",
    "        print(f\"DEBOG: at ft main class {y_backup.shape}\")\n",
    "\n",
    "        levels_per_categorical_feature = []\n",
    "        for i, col_name in enumerate(categorical_columns):\n",
    "            new_features.append(col_name)\n",
    "            # if col_name == specification.class_column:\n",
    "            #     continue\n",
    "            print(f\"[Categorical {i+1:,} / {len(categorical_columns)}] Processing categorical column {col_name}...\")\n",
    "\n",
    "            all_data = dataset[col_name].values\n",
    "            training_data = all_data[training_mask]\n",
    "\n",
    "            self.pre_processing.fit_categorical(col_name, training_data)\n",
    "            new_values = self.pre_processing.transform_categorical(col_name, all_data, self.input_encoding.required_input_format)\n",
    "\n",
    "            if self.input_encoding.required_input_format == CategoricalFormat.OneHot:\n",
    "                # multiple columns of one hot values\n",
    "                if isinstance(new_values, pd.DataFrame):\n",
    "                    levels_per_categorical_feature.append(len(new_values.columns))\n",
    "                    for c in new_values.columns:\n",
    "                        new_df[c] = new_values[c]\n",
    "                else:\n",
    "                    n_one_hot_levels = new_values.shape[1]\n",
    "                    levels_per_categorical_feature.append(n_one_hot_levels)\n",
    "                    for z in range(n_one_hot_levels):\n",
    "                        new_df[f\"{col_name}_{z}\"] = new_values[:, z]\n",
    "            else:\n",
    "                # single column of integers\n",
    "                levels_per_categorical_feature.append(len(np.unique(new_values)))\n",
    "                new_df[col_name] = new_values\n",
    "\n",
    "        print(f\"Generating pre-processed dataframe...\")\n",
    "        new_df = pd.DataFrame(new_df)\n",
    "        #### CNG\n",
    "        self.y_backup = y_backup\n",
    "        print(f\"DEBOG: also at ft main class{self.y_backup.shape}\")\n",
    "\n",
    "        model_input_spec = ModelInputSpecification(new_features, len(numerical_columns), levels_per_categorical_feature, self.input_encoding.required_input_format)\n",
    "\n",
    "        print(f\"Input data frame had shape ({len(dataset)},{len(dataset.columns)}), output data frame has shape ({len(new_df)},{len(new_df.columns)}) after pre-processing...\")\n",
    "\n",
    "        if cache_file_path is not None:\n",
    "            print(f\"Writing to cache file path: {cache_file_path}...\")\n",
    "            save_feather_plus_metadata(cache_file_path, new_df, model_input_spec)\n",
    "\n",
    "        if FlowTransformer.retain_inmem_cache:\n",
    "            if FlowTransformer.inmem_cache is None:\n",
    "                FlowTransformer.inmem_cache = {}\n",
    "\n",
    "            FlowTransformer.inmem_cache.clear()\n",
    "            FlowTransformer.inmem_cache[cache_key] = (new_df, model_input_spec)\n",
    "\n",
    "        return new_df, model_input_spec\n",
    "\n",
    "    def load_dataset(self, dataset_name:str,\n",
    "                     dataset:Union[pd.DataFrame, str],\n",
    "                     specification:DatasetSpecification,\n",
    "                     cache_path:Optional[str]=None,\n",
    "                     n_rows:int=0,\n",
    "                     evaluation_dataset_sampling:EvaluationDatasetSampling=EvaluationDatasetSampling.LastRows,\n",
    "                     evaluation_percent:float=0.2,\n",
    "                     numerical_filter=1_000_000_000) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load a dataset and prepare it for training\n",
    "\n",
    "        :param dataset: The path to a CSV dataset to load from, or a dataframe\n",
    "        :param cache_path: Where to store a cached version of this file\n",
    "        :param n_rows: The number of rows to ingest from the dataset, or 0 to ingest all\n",
    "        \"\"\"\n",
    "\n",
    "        if cache_path is None:\n",
    "            cache_path = \"cache\"\n",
    "\n",
    "        if not os.path.exists(cache_path):\n",
    "            warnings.warn(f\"Could not find cache folder: {cache_path}, attempting to create\")\n",
    "            os.mkdir(cache_path)\n",
    "\n",
    "        self.dataset_specification = specification\n",
    "        df, model_input_spec = self._load_preprocessed_dataset(dataset_name, dataset, specification, cache_path, n_rows, evaluation_dataset_sampling, evaluation_percent, numerical_filter)\n",
    "        \n",
    "        # print(\"DEBOG: \")\n",
    "        # with pd.option_context('display.max_columns', None, 'display.max_colwidth', None):\n",
    "        #     print(df.columns)\n",
    "            \n",
    "        training_mask = df[\"__training\"].values\n",
    "        del df[\"__training\"]\n",
    "\n",
    "\n",
    "        #### CNG\n",
    "        attack_columns = [col for col in df.columns if col.startswith(str(self.dataset_specification.class_column))]\n",
    "\n",
    "        y = df[attack_columns].values\n",
    "        del df['__y']\n",
    "\n",
    "        self.X = df\n",
    "        self.y = y\n",
    "        self.training_mask = training_mask\n",
    "        self.model_input_spec = model_input_spec\n",
    "\n",
    "        return df\n",
    "\n",
    "    def evaluate(self, m:keras.Model, batch_size, early_stopping_patience:int=5, epochs:int=100, steps_per_epoch:int=128):\n",
    "        n_malicious_per_batch = int(0.5 * batch_size)\n",
    "        n_legit_per_batch = batch_size - n_malicious_per_batch\n",
    "\n",
    "        overall_y_preserve = np.zeros(dtype=\"float32\", shape=(n_malicious_per_batch + n_legit_per_batch,15))\n",
    "        overall_y_preserve[:n_malicious_per_batch] = 1.\n",
    "\n",
    "        selectable_mask = np.zeros(len(self.X), dtype=bool)\n",
    "        selectable_mask[self.parameters.window_size:-self.parameters.window_size] = True\n",
    "        train_mask = self.training_mask\n",
    "\n",
    "        #### CNG\n",
    "        y_mask = ~(self.y_backup.astype('str') == str(self.dataset_specification.benign_label))\n",
    "        # y_mask = self.y\n",
    "        print(f\"DEBOG: still at ft main {self.y_backup.shape}\")\n",
    "        indices_train = np.argwhere(train_mask).reshape(-1)\n",
    "        malicious_indices_train = np.argwhere(train_mask & y_mask & selectable_mask).reshape(-1)\n",
    "        legit_indices_train = np.argwhere(train_mask & ~y_mask & selectable_mask).reshape(-1)\n",
    "\n",
    "        indices_test:np.ndarray = np.argwhere(~train_mask).reshape(-1)\n",
    "\n",
    "        def get_windows_for_indices(indices:np.ndarray, ordered) -> List[pd.DataFrame]:\n",
    "            X: List[pd.DataFrame] = []\n",
    "\n",
    "            if ordered:\n",
    "                # we don't really want to include eval samples as part of context, because out of range values might be learned\n",
    "                # by the model, _but_ we are forced to in the windowed approach, if users haven't just selected the\n",
    "                # \"take last 10%\" as eval option. We warn them prior to this though.\n",
    "                for i1 in indices:\n",
    "                    X.append(self.X.iloc[(i1 - self.parameters.window_size) + 1:i1 + 1])\n",
    "            else:\n",
    "                context_indices_batch = np.random.choice(indices_train, size=(batch_size, self.parameters.window_size),\n",
    "                                                         replace=False).reshape(-1)\n",
    "                context_indices_batch[:, -1] = indices\n",
    "\n",
    "                for index in context_indices_batch:\n",
    "                    X.append(self.X.iloc[index])\n",
    "\n",
    "            return X\n",
    "\n",
    "        feature_columns_map = {}\n",
    "\n",
    "        def samplewise_to_featurewise(X):\n",
    "            sequence_length = len(X[0])\n",
    "\n",
    "            combined_df = pd.concat(X)\n",
    "\n",
    "            featurewise_X = []\n",
    "\n",
    "            if len(feature_columns_map) == 0:\n",
    "                for feature in self.model_input_spec.feature_names:\n",
    "                    if feature in self.model_input_spec.numeric_feature_names or self.model_input_spec.categorical_format == CategoricalFormat.Integers:\n",
    "                        feature_columns_map[feature] = feature\n",
    "                    else:\n",
    "                        # this is a one-hot encoded categorical feature\n",
    "                        feature_columns_map[feature] = [c for c in X[0].columns if str(c).startswith(feature)]\n",
    "\n",
    "            for feature in self.model_input_spec.feature_names:\n",
    "                feature_columns = feature_columns_map[feature]\n",
    "                combined_values = combined_df[feature_columns].values\n",
    "\n",
    "                # maybe this can be faster with a reshape but I couldn't get it to work\n",
    "                combined_values = np.array([combined_values[i:i+sequence_length] for i in range(0, len(combined_values), sequence_length)])\n",
    "                featurewise_X.append(combined_values)\n",
    "\n",
    "            return featurewise_X\n",
    "\n",
    "        print(f\"Building eval dataset...\")\n",
    "        eval_X = get_windows_for_indices(indices_test, True)\n",
    "        print(f\"Splitting dataset to featurewise...\")\n",
    "        eval_featurewise_X = samplewise_to_featurewise(eval_X)\n",
    "        eval_y = self.y[indices_test]\n",
    "        #### CNG\n",
    "        # eval_P = eval_y\n",
    "        # n_eval_P = np.count_nonzero(eval_P)\n",
    "        # eval_N = ~eval_y\n",
    "        # n_eval_N = np.count_nonzero(eval_N)\n",
    "        print(f\"Evaluation dataset is built!\")\n",
    "\n",
    "        # print(f\"Positive samples in eval set: {n_eval_P}\")\n",
    "        # print(f\"Negative samples in eval set: {n_eval_N}\")\n",
    "\n",
    "        epoch_results = []\n",
    "\n",
    "        # def run_evaluation(epoch):\n",
    "        #     pred_y = m.predict(eval_featurewise_X, verbose=True)\n",
    "        #     pred_y = pred_y.reshape(-1) > 0.5\n",
    "\n",
    "        #     #### CNG\n",
    "\n",
    "        #     pred_P = pred_y\n",
    "        #     n_pred_P = np.count_nonzero(pred_P)\n",
    "\n",
    "        #     pred_N = ~pred_y\n",
    "        #     n_pred_N = np.count_nonzero(pred_N)\n",
    "\n",
    "        #     TP = np.count_nonzero(pred_P & eval_P)\n",
    "        #     FP = np.count_nonzero(pred_P & ~eval_P)\n",
    "        #     TN = np.count_nonzero(pred_N & eval_N)\n",
    "        #     FN = np.count_nonzero(pred_N & ~eval_N)\n",
    "\n",
    "        #     sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        #     specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "        #     balanced_accuracy = (sensitivity + specificity) / 2\n",
    "\n",
    "        #     precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        #     recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "        #     f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "        #     print(f\"Epoch {epoch} yielded predictions: {pred_y.shape}, overall balanced accuracy: {balanced_accuracy * 100:.2f}%, TP = {TP:,} / {n_eval_P:,}, TN = {TN:,} / {n_eval_N:,}\")\n",
    "\n",
    "        #     epoch_results.append({\n",
    "        #         \"epoch\": epoch,\n",
    "        #         \"P\": n_eval_P,\n",
    "        #         \"N\": n_eval_N,\n",
    "        #         \"pred_P\": n_pred_P,\n",
    "        #         \"pred_N\": n_pred_N,\n",
    "        #         \"TP\": TP,\n",
    "        #         \"FP\": FP,\n",
    "        #         \"TN\": TN,\n",
    "        #         \"FN\": FN,\n",
    "        #         \"bal_acc\": balanced_accuracy,\n",
    "        #         \"f1\": f1_score\n",
    "        #     })\n",
    "        \n",
    "        def run_evaluation(epoch):\n",
    "            \"\"\"\n",
    "            Evaluate a multiclass classification model on eval_featurewise_X and eval_y.\n",
    "            \n",
    "            Args:\n",
    "                epoch (int): The epoch number of the current evaluation.\n",
    "\n",
    "            Returns:\n",
    "                None. Appends evaluation metrics to `epoch_results` list.\n",
    "            \"\"\"\n",
    "            # Predict probabilities (assume softmax output) and get predicted class indices\n",
    "            pred_probs = m.predict(eval_featurewise_X, verbose=True)  # Softmax output, shape (num_samples, num_classes)\n",
    "            pred_y = np.argmax(pred_probs, axis=1)  # Predicted class indices\n",
    "            true_y = eval_y   # Convert one-hot to indices if necessary\n",
    "            \n",
    "            # Calculate confusion matrix and classification report\n",
    "            confusion = confusion_matrix(true_y, pred_y)\n",
    "            precision = precision_score(true_y, pred_y, average=None)  # Per-class precision\n",
    "            recall = recall_score(true_y, pred_y, average=None)        # Per-class recall\n",
    "            f1 = f1_score(true_y, pred_y, average=None)                # Per-class F1-score\n",
    "\n",
    "            # Calculate overall metrics (macro and weighted averages)\n",
    "            macro_precision = precision_score(true_y, pred_y, average='macro')\n",
    "            macro_recall = recall_score(true_y, pred_y, average='macro')\n",
    "            macro_f1 = f1_score(true_y, pred_y, average='macro')\n",
    "            \n",
    "            weighted_precision = precision_score(true_y, pred_y, average='weighted')\n",
    "            weighted_recall = recall_score(true_y, pred_y, average='weighted')\n",
    "            weighted_f1 = f1_score(true_y, pred_y, average='weighted')\n",
    "            \n",
    "            balanced_accuracy = balanced_accuracy_score(true_y, pred_y)\n",
    "\n",
    "            # Print summary\n",
    "            print(f\"Epoch {epoch} yielded predictions: {pred_y.shape}, balanced accuracy: {balanced_accuracy * 100:.2f}%\")\n",
    "            print(f\"Confusion Matrix:\\n{confusion}\")\n",
    "            print(f\"Per-Class Metrics:\")\n",
    "            for i, (p, r, f) in enumerate(zip(precision, recall, f1)):\n",
    "                print(f\"  Class {i}: Precision = {p:.4f}, Recall = {r:.4f}, F1-score = {f:.4f}\")\n",
    "            print(f\"Macro Avg - Precision: {macro_precision:.4f}, Recall: {macro_recall:.4f}, F1: {macro_f1:.4f}\")\n",
    "            print(f\"Weighted Avg - Precision: {weighted_precision:.4f}, Recall: {weighted_recall:.4f}, F1: {weighted_f1:.4f}\")\n",
    "            \n",
    "            # Append metrics for this epoch\n",
    "            epoch_results.append({\n",
    "                \"epoch\": epoch,\n",
    "                \"confusion_matrix\": confusion.tolist(),  # Store it as a list for JSON compatibility\n",
    "                \"balanced_accuracy\": balanced_accuracy,\n",
    "                \"macro_precision\": macro_precision,\n",
    "                \"macro_recall\": macro_recall,\n",
    "                \"macro_f1\": macro_f1,\n",
    "                \"weighted_precision\": weighted_precision,\n",
    "                \"weighted_recall\": weighted_recall,\n",
    "                \"weighted_f1\": weighted_f1,\n",
    "                \"class_precision\": precision.tolist(),\n",
    "                \"class_recall\": recall.tolist(),\n",
    "                \"class_f1\": f1.tolist(),\n",
    "            })\n",
    "\n",
    "\n",
    "        class BatchYielder():\n",
    "            def __init__(self, ordered, random, rs):\n",
    "                self.ordered = ordered\n",
    "                self.random = random\n",
    "                self.cursor_malicious = 0\n",
    "                self.cursor_legit = 0\n",
    "                self.rs = rs\n",
    "\n",
    "            def get_batch(self):\n",
    "                malicious_indices_batch = self.rs.choice(malicious_indices_train, size=n_malicious_per_batch,\n",
    "                                                         replace=False) \\\n",
    "                    if self.random else \\\n",
    "                    malicious_indices_train[self.cursor_malicious:self.cursor_malicious + n_malicious_per_batch]\n",
    "\n",
    "                legitimate_indices_batch = self.rs.choice(legit_indices_train, size=n_legit_per_batch, replace=False) \\\n",
    "                    if self.random else \\\n",
    "                    legit_indices_train[self.cursor_legit:self.cursor_legit + n_legit_per_batch]\n",
    "\n",
    "                indices = np.concatenate([malicious_indices_batch, legitimate_indices_batch])\n",
    "\n",
    "                self.cursor_malicious = self.cursor_malicious + n_malicious_per_batch\n",
    "                self.cursor_malicious = self.cursor_malicious % (len(malicious_indices_train) - n_malicious_per_batch)\n",
    "\n",
    "                self.cursor_legit = self.cursor_legit + n_legit_per_batch\n",
    "                self.cursor_legit = self.cursor_legit % (len(legit_indices_train) - n_legit_per_batch)\n",
    "\n",
    "                X = get_windows_for_indices(indices, self.ordered)\n",
    "                # each x in X contains a dataframe, with window_size rows and all the features of the flows. There are batch_size of these.\n",
    "\n",
    "                # we have a dataframe containing batch_size x (window_size, features)\n",
    "                # we actually want a result of features x (batch_size, sequence_length, feature_dimension)\n",
    "                featurewise_X = samplewise_to_featurewise(X)\n",
    "\n",
    "                return featurewise_X, overall_y_preserve\n",
    "\n",
    "        batch_yielder = BatchYielder(self.parameters._train_ensure_flows_are_ordered_within_windows, not self.parameters._train_draw_sequential_windows, self.rs)\n",
    "\n",
    "        min_loss = 100\n",
    "        iters_since_loss_decrease = 0\n",
    "\n",
    "        train_results = []\n",
    "        final_epoch = 0\n",
    "\n",
    "        last_print = time.time()\n",
    "        elapsed_time = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            final_epoch = epoch\n",
    "\n",
    "            has_reduced_loss = False\n",
    "            for step in range(steps_per_epoch):\n",
    "                batch_X, batch_y = batch_yielder.get_batch()\n",
    "\n",
    "                t0 = time.time()\n",
    "                batch_results = m.train_on_batch(batch_X, batch_y)\n",
    "                t1 = time.time()\n",
    "\n",
    "                if epoch > 0 or step > 0:\n",
    "                    elapsed_time += (t1 - t0)\n",
    "                    if epoch == 0 and step == 1:\n",
    "                        # include time for last \"step\" that we skipped with step > 0 for epoch == 0\n",
    "                        elapsed_time *= 2\n",
    "\n",
    "                train_results.append(batch_results + [elapsed_time, epoch])\n",
    "\n",
    "                batch_loss = batch_results[0] if isinstance(batch_results, list) else batch_results\n",
    "\n",
    "                if time.time() - last_print > 3:\n",
    "                    last_print = time.time()\n",
    "                    early_stop_phrase = \"\" if early_stopping_patience <= 0 else f\" (early stop in {early_stopping_patience - iters_since_loss_decrease:,})\"\n",
    "                    print(f\"Epoch = {epoch:,} / {epochs:,}{early_stop_phrase}, step = {step}, loss = {batch_loss:.5f}, results = {batch_results} -- elapsed (train): {elapsed_time:.2f}s\")\n",
    "\n",
    "                if batch_loss < min_loss:\n",
    "                    has_reduced_loss = True\n",
    "                    min_loss = batch_loss\n",
    "\n",
    "            if has_reduced_loss:\n",
    "                iters_since_loss_decrease = 0\n",
    "            else:\n",
    "                iters_since_loss_decrease += 1\n",
    "\n",
    "            do_early_stop = early_stopping_patience > 0 and iters_since_loss_decrease > early_stopping_patience\n",
    "            is_last_epoch = epoch == epochs - 1\n",
    "            run_eval = epoch in [6] or is_last_epoch or do_early_stop\n",
    "\n",
    "            if run_eval:\n",
    "                run_evaluation(epoch)\n",
    "\n",
    "            if do_early_stop:\n",
    "                print(f\"Early stopping at epoch: {epoch}\")\n",
    "                break\n",
    "\n",
    "        eval_results = pd.DataFrame(epoch_results)\n",
    "\n",
    "        return (train_results, eval_results, final_epoch)\n",
    "\n",
    "\n",
    "    def time(self, m:keras.Model, batch_size, n_steps=128, n_repeats=4):\n",
    "        n_malicious_per_batch = int(0.5 * batch_size)\n",
    "        n_legit_per_batch = batch_size - n_malicious_per_batch\n",
    "\n",
    "        overall_y_preserve = np.zeros(dtype=\"float32\", shape=(n_malicious_per_batch + n_legit_per_batch,))\n",
    "        overall_y_preserve[:n_malicious_per_batch] = 1.\n",
    "\n",
    "        selectable_mask = np.zeros(len(self.X), dtype=bool)\n",
    "        selectable_mask[self.parameters.window_size:-self.parameters.window_size] = True\n",
    "        train_mask = self.training_mask\n",
    "\n",
    "        y_mask = ~(self.y_backup.astype('str') == str(self.dataset_specification.benign_label))\n",
    "\n",
    "        indices_train = np.argwhere(train_mask).reshape(-1)\n",
    "        malicious_indices_train = np.argwhere(train_mask & y_mask & selectable_mask).reshape(-1)\n",
    "        legit_indices_train = np.argwhere(train_mask & ~y_mask & selectable_mask).reshape(-1)\n",
    "\n",
    "        indices_test:np.ndarray = np.argwhere(~train_mask).reshape(-1)\n",
    "\n",
    "        def get_windows_for_indices(indices:np.ndarray, ordered) -> List[pd.DataFrame]:\n",
    "            X: List[pd.DataFrame] = []\n",
    "\n",
    "            if ordered:\n",
    "                # we don't really want to include eval samples as part of context, because out of range values might be learned\n",
    "                # by the model, _but_ we are forced to in the windowed approach, if users haven't just selected the\n",
    "                # \"take last 10%\" as eval option. We warn them prior to this though.\n",
    "                for i1 in indices:\n",
    "                    X.append(self.X.iloc[(i1 - self.parameters.window_size) + 1:i1 + 1])\n",
    "            else:\n",
    "                context_indices_batch = np.random.choice(indices_train, size=(batch_size, self.parameters.window_size),\n",
    "                                                         replace=False).reshape(-1)\n",
    "                context_indices_batch[:, -1] = indices\n",
    "\n",
    "                for index in context_indices_batch:\n",
    "                    X.append(self.X.iloc[index])\n",
    "\n",
    "            return X\n",
    "\n",
    "        feature_columns_map = {}\n",
    "\n",
    "        def samplewise_to_featurewise(X):\n",
    "            sequence_length = len(X[0])\n",
    "\n",
    "            combined_df = pd.concat(X)\n",
    "\n",
    "            featurewise_X = []\n",
    "\n",
    "            if len(feature_columns_map) == 0:\n",
    "                for feature in self.model_input_spec.feature_names:\n",
    "                    if feature in self.model_input_spec.numeric_feature_names or self.model_input_spec.categorical_format == CategoricalFormat.Integers:\n",
    "                        feature_columns_map[feature] = feature\n",
    "                    else:\n",
    "                        # this is a one-hot encoded categorical feature\n",
    "                        feature_columns_map[feature] = [c for c in X[0].columns if str(c).startswith(feature)]\n",
    "\n",
    "            for feature in self.model_input_spec.feature_names:\n",
    "                feature_columns = feature_columns_map[feature]\n",
    "                combined_values = combined_df[feature_columns].values\n",
    "\n",
    "                # maybe this can be faster with a reshape but I couldn't get it to work\n",
    "                combined_values = np.array([combined_values[i:i+sequence_length] for i in range(0, len(combined_values), sequence_length)])\n",
    "                featurewise_X.append(combined_values)\n",
    "\n",
    "            return featurewise_X\n",
    "\n",
    "\n",
    "        epoch_results = []\n",
    "\n",
    "\n",
    "        class BatchYielder():\n",
    "            def __init__(self, ordered, random, rs):\n",
    "                self.ordered = ordered\n",
    "                self.random = random\n",
    "                self.cursor_malicious = 0\n",
    "                self.cursor_legit = 0\n",
    "                self.rs = rs\n",
    "\n",
    "            def get_batch(self):\n",
    "                malicious_indices_batch = self.rs.choice(malicious_indices_train, size=n_malicious_per_batch,\n",
    "                                                         replace=False) \\\n",
    "                    if self.random else \\\n",
    "                    malicious_indices_train[self.cursor_malicious:self.cursor_malicious + n_malicious_per_batch]\n",
    "\n",
    "                legitimate_indices_batch = self.rs.choice(legit_indices_train, size=n_legit_per_batch, replace=False) \\\n",
    "                    if self.random else \\\n",
    "                    legit_indices_train[self.cursor_legit:self.cursor_legit + n_legit_per_batch]\n",
    "\n",
    "                indices = np.concatenate([malicious_indices_batch, legitimate_indices_batch])\n",
    "\n",
    "                self.cursor_malicious = self.cursor_malicious + n_malicious_per_batch\n",
    "                self.cursor_malicious = self.cursor_malicious % (len(malicious_indices_train) - n_malicious_per_batch)\n",
    "\n",
    "                self.cursor_legit = self.cursor_legit + n_legit_per_batch\n",
    "                self.cursor_legit = self.cursor_legit % (len(legit_indices_train) - n_legit_per_batch)\n",
    "\n",
    "                X = get_windows_for_indices(indices, self.ordered)\n",
    "                # each x in X contains a dataframe, with window_size rows and all the features of the flows. There are batch_size of these.\n",
    "\n",
    "                # we have a dataframe containing batch_size x (window_size, features)\n",
    "                # we actually want a result of features x (batch_size, sequence_length, feature_dimension)\n",
    "                featurewise_X = samplewise_to_featurewise(X)\n",
    "\n",
    "                return featurewise_X, overall_y_preserve\n",
    "\n",
    "        batch_yielder = BatchYielder(self.parameters._train_ensure_flows_are_ordered_within_windows, not self.parameters._train_draw_sequential_windows, self.rs)\n",
    "\n",
    "        min_loss = 100\n",
    "        iters_since_loss_decrease = 0\n",
    "\n",
    "        final_epoch = 0\n",
    "\n",
    "        last_print = time.time()\n",
    "        elapsed_time = 0\n",
    "\n",
    "        batch_times = []\n",
    "\n",
    "\n",
    "        for step in range(n_steps):\n",
    "            batch_X, batch_y = batch_yielder.get_batch()\n",
    "\n",
    "            local_batch_times = []\n",
    "            for i in range(n_repeats):\n",
    "                t0 = time.time()\n",
    "                batch_results = m.predict_on_batch(batch_X)\n",
    "                t1 = time.time()\n",
    "                local_batch_times.append(t1 - t0)\n",
    "\n",
    "            batch_times.append(local_batch_times)\n",
    "\n",
    "            if time.time() - last_print > 3:\n",
    "                last_print = time.time()\n",
    "                print(f\"Step = {step}, running model evaluation... Average times = {np.mean(np.array(batch_times).reshape(-1))}\")\n",
    "\n",
    "        return batch_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported and defined\n",
      "Defined flowtransformer, starting load\n",
      "Using cache file path: cache\\CSE_CIC_IDS_0_QdLmZHuh8yOmlGcKBEkf7hepImY0_VHNk9ujbqtTXGSrgVayeqG486IQ0.feather\n",
      "Attempting to read dataset from path dataset/downsampled/downsampled_df_shuffled.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\AppData\\Local\\Temp\\ipykernel_19036\\2385110080.py:283: UserWarning: Could not find cache folder: cache, attempting to create\n",
      "  warnings.warn(f\"Could not find cache folder: {cache_path}, attempting to create\")\n",
      "C:\\Users\\ahmed\\AppData\\Local\\Temp\\ipykernel_19036\\2385110080.py:146: UserWarning: Using EvaluationDatasetSampling options other than LastRows might leak some information during training, if for example the context window leading up to a particular flow contains an evaluation flow, and this flow has out of range values (out of range to when pre-processing was applied on the training flows), then the model might potentially learn to handle these. In any case, no class leakage is present.\n",
      "  warnings.warn(\"Using EvaluationDatasetSampling options other than LastRows might leak some information during training, if for example the context window leading up to a particular flow contains an evaluation flow, and this flow has out of range values (out of range to when pre-processing was applied on the training flows), then the model might potentially learn to handle these. In any case, no class leakage is present.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set y to = Attack\n",
      "Converting numerical columns to floats, and removing out of range values...\n",
      "Applying pre-processing to numerical values\n",
      "[Numerical 1 / 28] Processing numerical column IN_BYTES...\n",
      "[Numerical 2 / 28] Processing numerical column MAX_IP_PKT_LEN...\n",
      "[Numerical 3 / 28] Processing numerical column LONGEST_FLOW_PKT...\n",
      "[Numerical 4 / 28] Processing numerical column MIN_TTL...\n",
      "[Numerical 5 / 28] Processing numerical column DURATION_IN...\n",
      "[Numerical 6 / 28] Processing numerical column NUM_PKTS_512_TO_1024_BYTES...\n",
      "[Numerical 7 / 28] Processing numerical column MIN_IP_PKT_LEN...\n",
      "[Numerical 8 / 28] Processing numerical column NUM_PKTS_128_TO_256_BYTES...\n",
      "[Numerical 9 / 28] Processing numerical column NUM_PKTS_UP_TO_128_BYTES...\n",
      "[Numerical 10 / 28] Processing numerical column RETRANSMITTED_IN_PKTS...\n",
      "[Numerical 11 / 28] Processing numerical column RETRANSMITTED_IN_BYTES...\n",
      "[Numerical 12 / 28] Processing numerical column DST_TO_SRC_AVG_THROUGHPUT...\n",
      "[Numerical 13 / 28] Processing numerical column IN_PKTS...\n",
      "[Numerical 14 / 28] Processing numerical column FLOW_DURATION_MILLISECONDS...\n",
      "[Numerical 15 / 28] Processing numerical column NUM_PKTS_1024_TO_1514_BYTES...\n",
      "[Numerical 16 / 28] Processing numerical column RETRANSMITTED_OUT_PKTS...\n",
      "[Numerical 17 / 28] Processing numerical column DURATION_OUT...\n",
      "[Numerical 18 / 28] Processing numerical column TCP_WIN_MAX_IN...\n",
      "[Numerical 19 / 28] Processing numerical column DST_TO_SRC_SECOND_BYTES...\n",
      "[Numerical 20 / 28] Processing numerical column MAX_TTL...\n",
      "[Numerical 21 / 28] Processing numerical column NUM_PKTS_256_TO_512_BYTES...\n",
      "[Numerical 22 / 28] Processing numerical column OUT_PKTS...\n",
      "[Numerical 23 / 28] Processing numerical column TCP_WIN_MAX_OUT...\n",
      "[Numerical 24 / 28] Processing numerical column SRC_TO_DST_AVG_THROUGHPUT...\n",
      "[Numerical 25 / 28] Processing numerical column SRC_TO_DST_SECOND_BYTES...\n",
      "[Numerical 26 / 28] Processing numerical column SHORTEST_FLOW_PKT...\n",
      "[Numerical 27 / 28] Processing numerical column OUT_BYTES...\n",
      "[Numerical 28 / 28] Processing numerical column RETRANSMITTED_OUT_BYTES...\n",
      "Applying pre-processing to categorical values\n",
      "Keeping a copy of label column\n",
      "DEBOG: at ft main class (4558141,)\n",
      "[Categorical 1 / 10] Processing categorical column CLIENT_TCP_FLAGS...\n",
      "Encoding the 32 levels for CLIENT_TCP_FLAGS\n",
      "[Categorical 2 / 10] Processing categorical column L4_SRC_PORT...\n",
      "Encoding the 32 levels for L4_SRC_PORT\n",
      "[Categorical 3 / 10] Processing categorical column TCP_FLAGS...\n",
      "Encoding the 32 levels for TCP_FLAGS\n",
      "[Categorical 4 / 10] Processing categorical column ICMP_IPV4_TYPE...\n",
      "Encoding the 32 levels for ICMP_IPV4_TYPE\n",
      "[Categorical 5 / 10] Processing categorical column ICMP_TYPE...\n",
      "Encoding the 32 levels for ICMP_TYPE\n",
      "[Categorical 6 / 10] Processing categorical column PROTOCOL...\n",
      "Encoding the 6 levels for PROTOCOL\n",
      "[Categorical 7 / 10] Processing categorical column SERVER_TCP_FLAGS...\n",
      "Encoding the 32 levels for SERVER_TCP_FLAGS\n",
      "[Categorical 8 / 10] Processing categorical column L4_DST_PORT...\n",
      "Encoding the 32 levels for L4_DST_PORT\n",
      "[Categorical 9 / 10] Processing categorical column L7_PROTO...\n",
      "Encoding the 32 levels for L7_PROTO\n",
      "[Categorical 10 / 10] Processing categorical column Attack...\n",
      "Encoding the 15 levels for Attack\n",
      "Generating pre-processed dataframe...\n",
      "DEBOG: also at ft main class(4558141,)\n",
      "Input data frame had shape (4558141,46), output data frame has shape (4558141,307) after pre-processing...\n",
      "Writing to cache file path: cache\\CSE_CIC_IDS_0_QdLmZHuh8yOmlGcKBEkf7hepImY0_VHNk9ujbqtTXGSrgVayeqG486IQ0.feather...\n",
      "Loaded dataset, building model\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_IN_BYTES (InputLayer  [(None, 8, 1)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_MAX_IP_PKT_LEN (Inpu  [(None, 8, 1)]               0         []                            \n",
      " tLayer)                                                                                          \n",
      "                                                                                                  \n",
      " input_LONGEST_FLOW_PKT (In  [(None, 8, 1)]               0         []                            \n",
      " putLayer)                                                                                        \n",
      "                                                                                                  \n",
      " input_MIN_TTL (InputLayer)  [(None, 8, 1)]               0         []                            \n",
      "                                                                                                  \n",
      " input_DURATION_IN (InputLa  [(None, 8, 1)]               0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " input_NUM_PKTS_512_TO_1024  [(None, 8, 1)]               0         []                            \n",
      " _BYTES (InputLayer)                                                                              \n",
      "                                                                                                  \n",
      " input_MIN_IP_PKT_LEN (Inpu  [(None, 8, 1)]               0         []                            \n",
      " tLayer)                                                                                          \n",
      "                                                                                                  \n",
      " input_NUM_PKTS_128_TO_256_  [(None, 8, 1)]               0         []                            \n",
      " BYTES (InputLayer)                                                                               \n",
      "                                                                                                  \n",
      " input_NUM_PKTS_UP_TO_128_B  [(None, 8, 1)]               0         []                            \n",
      " YTES (InputLayer)                                                                                \n",
      "                                                                                                  \n",
      " input_RETRANSMITTED_IN_PKT  [(None, 8, 1)]               0         []                            \n",
      " S (InputLayer)                                                                                   \n",
      "                                                                                                  \n",
      " input_RETRANSMITTED_IN_BYT  [(None, 8, 1)]               0         []                            \n",
      " ES (InputLayer)                                                                                  \n",
      "                                                                                                  \n",
      " input_DST_TO_SRC_AVG_THROU  [(None, 8, 1)]               0         []                            \n",
      " GHPUT (InputLayer)                                                                               \n",
      "                                                                                                  \n",
      " input_IN_PKTS (InputLayer)  [(None, 8, 1)]               0         []                            \n",
      "                                                                                                  \n",
      " input_FLOW_DURATION_MILLIS  [(None, 8, 1)]               0         []                            \n",
      " ECONDS (InputLayer)                                                                              \n",
      "                                                                                                  \n",
      " input_NUM_PKTS_1024_TO_151  [(None, 8, 1)]               0         []                            \n",
      " 4_BYTES (InputLayer)                                                                             \n",
      "                                                                                                  \n",
      " input_RETRANSMITTED_OUT_PK  [(None, 8, 1)]               0         []                            \n",
      " TS (InputLayer)                                                                                  \n",
      "                                                                                                  \n",
      " input_DURATION_OUT (InputL  [(None, 8, 1)]               0         []                            \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " input_TCP_WIN_MAX_IN (Inpu  [(None, 8, 1)]               0         []                            \n",
      " tLayer)                                                                                          \n",
      "                                                                                                  \n",
      " input_DST_TO_SRC_SECOND_BY  [(None, 8, 1)]               0         []                            \n",
      " TES (InputLayer)                                                                                 \n",
      "                                                                                                  \n",
      " input_MAX_TTL (InputLayer)  [(None, 8, 1)]               0         []                            \n",
      "                                                                                                  \n",
      " input_NUM_PKTS_256_TO_512_  [(None, 8, 1)]               0         []                            \n",
      " BYTES (InputLayer)                                                                               \n",
      "                                                                                                  \n",
      " input_OUT_PKTS (InputLayer  [(None, 8, 1)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_TCP_WIN_MAX_OUT (Inp  [(None, 8, 1)]               0         []                            \n",
      " utLayer)                                                                                         \n",
      "                                                                                                  \n",
      " input_SRC_TO_DST_AVG_THROU  [(None, 8, 1)]               0         []                            \n",
      " GHPUT (InputLayer)                                                                               \n",
      "                                                                                                  \n",
      " input_SRC_TO_DST_SECOND_BY  [(None, 8, 1)]               0         []                            \n",
      " TES (InputLayer)                                                                                 \n",
      "                                                                                                  \n",
      " input_SHORTEST_FLOW_PKT (I  [(None, 8, 1)]               0         []                            \n",
      " nputLayer)                                                                                       \n",
      "                                                                                                  \n",
      " input_OUT_BYTES (InputLaye  [(None, 8, 1)]               0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " input_RETRANSMITTED_OUT_BY  [(None, 8, 1)]               0         []                            \n",
      " TES (InputLayer)                                                                                 \n",
      "                                                                                                  \n",
      " input_CLIENT_TCP_FLAGS (In  [(None, 8, 32)]              0         []                            \n",
      " putLayer)                                                                                        \n",
      "                                                                                                  \n",
      " input_L4_SRC_PORT (InputLa  [(None, 8, 32)]              0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " input_TCP_FLAGS (InputLaye  [(None, 8, 32)]              0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " input_ICMP_IPV4_TYPE (Inpu  [(None, 8, 32)]              0         []                            \n",
      " tLayer)                                                                                          \n",
      "                                                                                                  \n",
      " input_ICMP_TYPE (InputLaye  [(None, 8, 32)]              0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " input_PROTOCOL (InputLayer  [(None, 8, 6)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_SERVER_TCP_FLAGS (In  [(None, 8, 32)]              0         []                            \n",
      " putLayer)                                                                                        \n",
      "                                                                                                  \n",
      " input_L4_DST_PORT (InputLa  [(None, 8, 32)]              0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " input_L7_PROTO (InputLayer  [(None, 8, 32)]              0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_Attack (InputLayer)   [(None, 8, 15)]              0         []                            \n",
      "                                                                                                  \n",
      " feature_concat (Concatenat  (None, 8, 305)               0         ['input_IN_BYTES[0][0]',      \n",
      " e)                                                                  'input_MAX_IP_PKT_LEN[0][0]',\n",
      "                                                                     'input_LONGEST_FLOW_PKT[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'input_MIN_TTL[0][0]',       \n",
      "                                                                     'input_DURATION_IN[0][0]',   \n",
      "                                                                     'input_NUM_PKTS_512_TO_1024_B\n",
      "                                                                    YTES[0][0]',                  \n",
      "                                                                     'input_MIN_IP_PKT_LEN[0][0]',\n",
      "                                                                     'input_NUM_PKTS_128_TO_256_BY\n",
      "                                                                    TES[0][0]',                   \n",
      "                                                                     'input_NUM_PKTS_UP_TO_128_BYT\n",
      "                                                                    ES[0][0]',                    \n",
      "                                                                     'input_RETRANSMITTED_IN_PKTS[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'input_RETRANSMITTED_IN_BYTES\n",
      "                                                                    [0][0]',                      \n",
      "                                                                     'input_DST_TO_SRC_AVG_THROUGH\n",
      "                                                                    PUT[0][0]',                   \n",
      "                                                                     'input_IN_PKTS[0][0]',       \n",
      "                                                                     'input_FLOW_DURATION_MILLISEC\n",
      "                                                                    ONDS[0][0]',                  \n",
      "                                                                     'input_NUM_PKTS_1024_TO_1514_\n",
      "                                                                    BYTES[0][0]',                 \n",
      "                                                                     'input_RETRANSMITTED_OUT_PKTS\n",
      "                                                                    [0][0]',                      \n",
      "                                                                     'input_DURATION_OUT[0][0]',  \n",
      "                                                                     'input_TCP_WIN_MAX_IN[0][0]',\n",
      "                                                                     'input_DST_TO_SRC_SECOND_BYTE\n",
      "                                                                    S[0][0]',                     \n",
      "                                                                     'input_MAX_TTL[0][0]',       \n",
      "                                                                     'input_NUM_PKTS_256_TO_512_BY\n",
      "                                                                    TES[0][0]',                   \n",
      "                                                                     'input_OUT_PKTS[0][0]',      \n",
      "                                                                     'input_TCP_WIN_MAX_OUT[0][0]'\n",
      "                                                                    , 'input_SRC_TO_DST_AVG_THROUG\n",
      "                                                                    HPUT[0][0]',                  \n",
      "                                                                     'input_SRC_TO_DST_SECOND_BYTE\n",
      "                                                                    S[0][0]',                     \n",
      "                                                                     'input_SHORTEST_FLOW_PKT[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'input_OUT_BYTES[0][0]',     \n",
      "                                                                     'input_RETRANSMITTED_OUT_BYTE\n",
      "                                                                    S[0][0]',                     \n",
      "                                                                     'input_CLIENT_TCP_FLAGS[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'input_L4_SRC_PORT[0][0]',   \n",
      "                                                                     'input_TCP_FLAGS[0][0]',     \n",
      "                                                                     'input_ICMP_IPV4_TYPE[0][0]',\n",
      "                                                                     'input_ICMP_TYPE[0][0]',     \n",
      "                                                                     'input_PROTOCOL[0][0]',      \n",
      "                                                                     'input_SERVER_TCP_FLAGS[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'input_L4_DST_PORT[0][0]',   \n",
      "                                                                     'input_L7_PROTO[0][0]',      \n",
      "                                                                     'input_Attack[0][0]']        \n",
      "                                                                                                  \n",
      " embed (Dense)               (None, 8, 64)                19584     ['feature_concat[0][0]']      \n",
      "                                                                                                  \n",
      " block_0_transformer_encode  (None, 8, 64)                83200     ['embed[0][0]']               \n",
      " r (TransformerEncoderBlock                                                                       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_1_transformer_encode  (None, 8, 64)                83200     ['block_0_transformer_encoder[\n",
      " r (TransformerEncoderBlock                                         0][0]']                       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " featurewise_embed (Dense)   (None, 8, 1)                 65        ['block_1_transformer_encoder[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)         (None, 8)                    0         ['featurewise_embed[0][0]']   \n",
      "                                                                                                  \n",
      " classification_mlp_0_128 (  (None, 128)                  1152      ['flatten_3[0][0]']           \n",
      " Dense)                                                                                           \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 128)                  0         ['classification_mlp_0_128[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " multiclass_classification_  (None, 15)                   1935      ['dropout_3[0][0]']           \n",
      " out (Dense)                                                                                      \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 189136 (738.81 KB)\n",
      "Trainable params: 189136 (738.81 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "DEBOG: still at ft main (4558141,)\n",
      "Building eval dataset...\n",
      "Splitting dataset to featurewise...\n",
      "Evaluation dataset is built!\n",
      "Epoch = 0 / 5 (early stop in 5), step = 0, loss = 21.22828, results = [21.22827911376953, 0.0] -- elapsed (train): 0.00s\n",
      "Epoch = 0 / 5 (early stop in 5), step = 18, loss = 53.41638, results = [53.4163818359375, 0.0] -- elapsed (train): 0.82s\n",
      "Epoch = 0 / 5 (early stop in 5), step = 37, loss = 73.87440, results = [73.87440490722656, 0.0] -- elapsed (train): 1.66s\n",
      "Epoch = 0 / 5 (early stop in 5), step = 55, loss = 104.55495, results = [104.55494689941406, 0.0078125] -- elapsed (train): 2.44s\n",
      "Epoch = 1 / 5 (early stop in 5), step = 9, loss = 140.44611, results = [140.44610595703125, 0.0078125] -- elapsed (train): 3.23s\n",
      "Epoch = 1 / 5 (early stop in 5), step = 27, loss = 211.84329, results = [211.84329223632812, 0.03125] -- elapsed (train): 4.01s\n",
      "Epoch = 1 / 5 (early stop in 5), step = 45, loss = 294.12759, results = [294.1275939941406, 0.015625] -- elapsed (train): 4.80s\n",
      "Epoch = 2 / 5 (early stop in 4), step = 0, loss = 419.88867, results = [419.888671875, 0.0234375] -- elapsed (train): 5.63s\n",
      "Epoch = 2 / 5 (early stop in 4), step = 18, loss = 548.82599, results = [548.8259887695312, 0.03125] -- elapsed (train): 6.42s\n",
      "Epoch = 2 / 5 (early stop in 4), step = 37, loss = 706.80402, results = [706.8040161132812, 0.046875] -- elapsed (train): 7.24s\n",
      "Epoch = 2 / 5 (early stop in 4), step = 56, loss = 926.47437, results = [926.474365234375, 0.0625] -- elapsed (train): 8.05s\n",
      "Epoch = 3 / 5 (early stop in 3), step = 10, loss = 1195.09668, results = [1195.0966796875, 0.046875] -- elapsed (train): 8.84s\n",
      "Epoch = 3 / 5 (early stop in 3), step = 29, loss = 1535.07251, results = [1535.072509765625, 0.046875] -- elapsed (train): 9.67s\n",
      "Epoch = 3 / 5 (early stop in 3), step = 47, loss = 1896.04028, results = [1896.040283203125, 0.078125] -- elapsed (train): 10.47s\n",
      "Epoch = 4 / 5 (early stop in 2), step = 1, loss = 2210.04541, results = [2210.04541015625, 0.015625] -- elapsed (train): 11.26s\n",
      "Epoch = 4 / 5 (early stop in 2), step = 18, loss = 2570.41162, results = [2570.41162109375, 0.03125] -- elapsed (train): 12.03s\n",
      "Epoch = 4 / 5 (early stop in 2), step = 36, loss = 3130.68213, results = [3130.68212890625, 0.015625] -- elapsed (train): 12.82s\n",
      "Epoch = 4 / 5 (early stop in 2), step = 55, loss = 3723.84863, results = [3723.8486328125, 0.0546875] -- elapsed (train): 13.64s\n",
      "1425/1425 [==============================] - 7s 4ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multilabel-indicator and binary targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Get the evaluation results\u001b[39;00m\n\u001b[0;32m     48\u001b[0m eval_results: pd\u001b[38;5;241m.\u001b[39mDataFrame\n\u001b[1;32m---> 49\u001b[0m (train_results, eval_results, final_epoch) \u001b[38;5;241m=\u001b[39m \u001b[43mft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(eval_results)\n",
      "Cell \u001b[1;32mIn[104], line 580\u001b[0m, in \u001b[0;36mFlowTransformer.evaluate\u001b[1;34m(self, m, batch_size, early_stopping_patience, epochs, steps_per_epoch)\u001b[0m\n\u001b[0;32m    577\u001b[0m run_eval \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m6\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m is_last_epoch \u001b[38;5;129;01mor\u001b[39;00m do_early_stop\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_eval:\n\u001b[1;32m--> 580\u001b[0m     \u001b[43mrun_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_early_stop:\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEarly stopping at epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[104], line 452\u001b[0m, in \u001b[0;36mFlowTransformer.evaluate.<locals>.run_evaluation\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m    449\u001b[0m true_y \u001b[38;5;241m=\u001b[39m eval_y   \u001b[38;5;66;03m# Convert one-hot to indices if necessary\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# Calculate confusion matrix and classification report\u001b[39;00m\n\u001b[1;32m--> 452\u001b[0m confusion \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(true_y, pred_y, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Per-class precision\u001b[39;00m\n\u001b[0;32m    454\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(true_y, pred_y, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)        \u001b[38;5;66;03m# Per-class recall\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:342\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    248\u001b[0m     {\n\u001b[0;32m    249\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    259\u001b[0m ):\n\u001b[0;32m    260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m    (np.int64(0), np.int64(2), np.int64(1), np.int64(1))\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 342\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    344\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:112\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m    109\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    114\u001b[0m             type_true, type_pred\n\u001b[0;32m    115\u001b[0m         )\n\u001b[0;32m    116\u001b[0m     )\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    119\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and binary targets"
     ]
    }
   ],
   "source": [
    "encodings = [\n",
    "    RecordLevelEmbed(64),\n",
    "    RecordLevelEmbed(64, project=True)\n",
    "]\n",
    "\n",
    "classification_heads = [\n",
    "    FeaturewiseEmbedding(project=False),\n",
    "    LastTokenClassificationHead()\n",
    "]\n",
    "\n",
    "transformers: List[FunctionalComponent] = [\n",
    "    BasicTransformer(2, 128, n_heads=2),\n",
    "    GPTSmallTransformer()\n",
    "]\n",
    "\n",
    "flow_file_path = r\"dataset/\"\n",
    "\n",
    "datasets = [\n",
    "    (\"CSE_CIC_IDS\", os.path.join(flow_file_path, \"downsampled/downsampled_df_shuffled.csv\"), NamedDatasetSpecifications.cse_cic_ids_2018, 0.01, EvaluationDatasetSampling.RandomRows)\n",
    "]\n",
    "\n",
    "print(\"Imported and defined\")\n",
    "pre_processing = StandardPreProcessing(n_categorical_levels=32)\n",
    "\n",
    "# Define the transformer\n",
    "ft = FlowTransformer(pre_processing=pre_processing,\n",
    "                     input_encoding=encodings[0],\n",
    "                     sequential_model=transformers[0],\n",
    "                     classification_head=classification_heads[0],\n",
    "                     params=FlowTransformerParameters(window_size=8, mlp_layer_sizes=[128], mlp_dropout=0.1))\n",
    "\n",
    "print(\"Defined flowtransformer, starting load\")\n",
    "\n",
    "# Load the specific dataset\n",
    "dataset_name, dataset_path, dataset_specification, eval_percent, eval_method = datasets[0]\n",
    "ft.load_dataset(dataset_name, dataset_path, dataset_specification, evaluation_dataset_sampling=eval_method, evaluation_percent=eval_percent)\n",
    "\n",
    "print(\"Loaded dataset, building model\")\n",
    "\n",
    "# Build the transformer model\n",
    "m = ft.build_model()\n",
    "m.summary()\n",
    "\n",
    "# Compile the model\n",
    "m.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['categorical_accuracy'], jit_compile=True)\n",
    "\n",
    "# Get the evaluation results\n",
    "eval_results: pd.DataFrame\n",
    "(train_results, eval_results, final_epoch) = ft.evaluate(m, batch_size=128, epochs=5, steps_per_epoch=64, early_stopping_patience=5)\n",
    "\n",
    "\n",
    "print(eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
