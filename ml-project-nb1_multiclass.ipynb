{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports done\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import time\n",
    "from enum import Enum\n",
    "from typing import Optional, Tuple, List, Union\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "try:\n",
    "    from tensorflow._api.v2.v2 import keras\n",
    "except ImportError:\n",
    "    from tensorflow import keras\n",
    "\n",
    "from keras import Input, Model\n",
    "import keras.layers as layers\n",
    "from keras.layers import Dense, Conv1D, Layer, MultiHeadAttention, Dropout, LayerNormalization, Embedding, Concatenate, Reshape, Lambda, Flatten, GlobalAveragePooling1D\n",
    "\n",
    "print(\"Imports done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import hashlib\n",
    "import json\n",
    "import pickle\n",
    "from typing import Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_identifier(d:dict):\n",
    "    raw_json = json.dumps(d, sort_keys=True, indent=False)\n",
    "    hash = hashlib.sha1(raw_json.encode(\"utf8\")).digest()\n",
    "    x = base64.b64encode(hash)\n",
    "    x = x.decode(\"ASCII\")\n",
    "    x = x.replace(\"+\", \"0\").replace(\"=\", \"0\").replace(\"/\", \"0\")\n",
    "    return x\n",
    "\n",
    "def save_feather_plus_metadata(save_path:str, df:pd.DataFrame, metadata:object):\n",
    "    metadata_path = save_path + \".metadata.pickle\"\n",
    "    df.to_feather(save_path)\n",
    "    with open(metadata_path, \"wb\") as w:\n",
    "        pickle.dump(metadata, w)\n",
    "\n",
    "def save_pickle(save_path:str, obj:dict):\n",
    "    with open(save_path, \"wb\") as w:\n",
    "        pickle.dump(obj, w)\n",
    "\n",
    "def load_pickle(save_path:str):\n",
    "    with open(save_path, \"rb\") as r:\n",
    "        return pickle.load(r, fix_imports=True)\n",
    "\n",
    "def load_feather_plus_metadata(load_path:str) -> Tuple[pd.DataFrame, object]:\n",
    "    metadata_path = load_path + \".metadata.pickle\"\n",
    "    with open(metadata_path, \"rb\") as r:\n",
    "        metadata = pickle.load(r, fix_imports=True)\n",
    "    data = pd.read_feather(load_path)\n",
    "    return data, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dataset Specifications: List some metadata, including column names for ease of use**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DatasetSpecification:\n",
    "    def  __init__(self, include_fields:List[str], categorical_fields:List[str], class_column:str, benign_label:str, test_column:Optional[str]=None):\n",
    "        \"\"\"\n",
    "        Defines the format of specific NIDS dataset\n",
    "        :param include_fields: The fields to include as part of classification\n",
    "        :param categorical_fields: Fields that should be treated as categorical\n",
    "        :param class_column: The column name that includes the class of the flow, eg. DDoS or Benign\n",
    "        :param benign_label: The label of benign traffic, eg. Benign or 0\n",
    "        :param test_column: The column indicating if this row is a member of the test or training dataset\n",
    "        \"\"\"\n",
    "        self.include_fields:List[str] = include_fields\n",
    "        self.categorical_fields:List[str] = categorical_fields\n",
    "        self.class_column = class_column\n",
    "        self.benign_label = benign_label\n",
    "        self.test_column:Optional[str] = test_column\n",
    "\n",
    "class NamedDatasetSpecifications:\n",
    "    \"\"\"\n",
    "    Example specifications of some common datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    cse_cic_ids_2018 = DatasetSpecification(\n",
    "            include_fields=['NUM_PKTS_UP_TO_128_BYTES', 'SRC_TO_DST_SECOND_BYTES', 'OUT_PKTS', 'OUT_BYTES', 'NUM_PKTS_128_TO_256_BYTES', 'DST_TO_SRC_AVG_THROUGHPUT', 'DURATION_IN', 'L4_SRC_PORT', 'ICMP_TYPE', 'PROTOCOL', 'SERVER_TCP_FLAGS', 'IN_PKTS', 'NUM_PKTS_512_TO_1024_BYTES', 'CLIENT_TCP_FLAGS', 'TCP_WIN_MAX_IN', 'NUM_PKTS_256_TO_512_BYTES', 'SHORTEST_FLOW_PKT', 'MIN_IP_PKT_LEN', 'LONGEST_FLOW_PKT', 'L4_DST_PORT', 'MIN_TTL', 'DST_TO_SRC_SECOND_BYTES', 'NUM_PKTS_1024_TO_1514_BYTES', 'DURATION_OUT', 'FLOW_DURATION_MILLISECONDS', 'TCP_FLAGS', 'MAX_TTL', 'SRC_TO_DST_AVG_THROUGHPUT', 'ICMP_IPV4_TYPE', 'MAX_IP_PKT_LEN', 'RETRANSMITTED_OUT_BYTES', 'IN_BYTES', 'RETRANSMITTED_IN_BYTES', 'TCP_WIN_MAX_OUT', 'L7_PROTO', 'RETRANSMITTED_OUT_PKTS', 'RETRANSMITTED_IN_PKTS'],\n",
    "            categorical_fields=['CLIENT_TCP_FLAGS', 'L4_SRC_PORT', 'TCP_FLAGS', 'ICMP_IPV4_TYPE', 'ICMP_TYPE', 'PROTOCOL', 'SERVER_TCP_FLAGS', 'L4_DST_PORT', 'L7_PROTO'],\n",
    "            class_column=\"Attack\",\n",
    "            benign_label=\"Benign\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Some helper classes**\n",
    "\n",
    "these are useful in, including but not limited to: dataset column datatypes, main class parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CategoricalFormat(Enum):\n",
    "    \"\"\"\n",
    "    The format of variables expected by the model as input\n",
    "    \"\"\"\n",
    "    Integers = 0,\n",
    "    \"\"\"\n",
    "    If categorical values should be dictionary encoded as integers\n",
    "    \"\"\"\n",
    "    OneHot = 1\n",
    "    \"\"\"\n",
    "    If categorical values should be one-hot encoded\n",
    "    \"\"\"\n",
    "\n",
    "class EvaluationDatasetSampling(Enum):\n",
    "    \"\"\"\n",
    "    How to choose evaluation samples from the raw dataset\n",
    "    \"\"\"\n",
    "    LastRows = 0\n",
    "    \"\"\"\n",
    "    Take the last rows in the dataset to form the evaluation dataset\n",
    "    \"\"\"\n",
    "    RandomRows  = 1\n",
    "    \"\"\"\n",
    "    Randomly sample rows to make up the evaluation dataset\n",
    "    \"\"\"\n",
    "    FilterColumn = 2\n",
    "    \"\"\"\n",
    "    Define a column that contains a flag indicating if this row is part of the evaluation set\n",
    "    \"\"\"\n",
    "\n",
    "class FlowTransformerParameters:\n",
    "    \"\"\"\n",
    "    Allows the configuration of overall parameters of the FlowTransformer\n",
    "    :param window_size: The number of flows to use in each window\n",
    "    :param mlp_layer_sizes: The number of nodes in each layer of the outer classification MLP of FlowTransformer\n",
    "    :param mlp_dropout: The amount of dropout to be applied between the layers of the outer classification MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, window_size:int, mlp_layer_sizes:List[int], mlp_dropout:float=0.1):\n",
    "        self.window_size:int = window_size\n",
    "        self.mlp_layer_sizes = mlp_layer_sizes\n",
    "        self.mlp_dropout = mlp_dropout\n",
    "\n",
    "        # Is the order of flows important within any individual window\n",
    "        self._train_ensure_flows_are_ordered_within_windows = True\n",
    "\n",
    "        # Should windows be sampled sequentially during training\n",
    "        self._train_draw_sequential_windows = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Input Enum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ModelInputSpecification:\n",
    "    def __init__(self, feature_names:List[str], n_numeric_features:int, levels_per_categorical_feature:List[int], categorical_format:CategoricalFormat):\n",
    "        self.feature_names = feature_names\n",
    "\n",
    "        self.numeric_feature_names = feature_names[:n_numeric_features]\n",
    "        self.categorical_feature_names = feature_names[n_numeric_features:]\n",
    "        self.categorical_format:CategoricalFormat = categorical_format\n",
    "\n",
    "        self.n_numeric_features = n_numeric_features\n",
    "        self.levels_per_categorical_feature = levels_per_categorical_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Framework Component class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Component():\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def parameters(self) -> dict:\n",
    "        warnings.warn(\"Parameters have not been implemented for this class!\")\n",
    "        return {}\n",
    "class FunctionalComponent(Component):\n",
    "    def __init__(self):\n",
    "        self.sequence_length: Optional[int] = None\n",
    "        self.model_input_specification: Optional[ModelInputSpecification] = None\n",
    "        self.input_shape: Optional[Tuple[int]] = None\n",
    "\n",
    "    def apply(self, X, prefix: str = None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def build(self, sequence_length:int, model_input_specification:ModelInputSpecification):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.model_input_specification = model_input_specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Base Classification Head**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BaseClassificationHead(FunctionalComponent):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def apply_before_transformer(self, X, prefix:str=None):\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Classification Head: Last token, FeatureWise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LastTokenClassificationHead(BaseClassificationHead):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def apply(self, X, prefix: str = None):\n",
    "        if prefix is None:\n",
    "            prefix = \"\"\n",
    "\n",
    "        x = Lambda(lambda x: x[..., -1, :], name=f\"{prefix}slice_last\")(X)\n",
    "        #x = Flatten(name=f\"{prefix}flatten_last\")(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"Last Token\"\n",
    "\n",
    "    @property\n",
    "    def parameters(self) -> dict:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturewiseEmbedding(BaseClassificationHead):\n",
    "    def __init__(self, project:bool=False):\n",
    "        super().__init__()\n",
    "        self.project: bool = project\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        if self.project:\n",
    "            return f\"Featurewise Embed - Projection\"\n",
    "        else:\n",
    "            return f\"Featurewise Embed - Dense\"\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return {}\n",
    "\n",
    "\n",
    "    def apply(self, X, prefix:str=None):\n",
    "        if prefix is None:\n",
    "            prefix = \"\"\n",
    "\n",
    "        if self.model_input_specification is None:\n",
    "            raise Exception(\"Please call build() before calling apply!\")\n",
    "\n",
    "        x = Dense(1,\n",
    "                  activation=\"linear\",\n",
    "                  use_bias=(not self.project),\n",
    "                  name=f\"{prefix}featurewise_embed\")(X)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Input Encoding:Record Level Projection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BaseInputEncoding(FunctionalComponent):\n",
    "    def apply(self, X:List[\"keras.Input\"], prefix: str = None):\n",
    "        raise NotImplementedError(\"Please override this with a custom implementation\")\n",
    "\n",
    "    @property\n",
    "    def required_input_format(self) -> CategoricalFormat:\n",
    "        raise NotImplementedError(\"Please override this with a custom implementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EmbedLayerType(Enum):\n",
    "    Dense = 0,\n",
    "    Lookup = 1,\n",
    "    Projection = 2\n",
    "\n",
    "class RecordLevelEmbed(BaseInputEncoding):\n",
    "    def __init__(self, embed_dimension: int, project:bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dimension: int = embed_dimension\n",
    "        self.project: bool = project\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        if self.project:\n",
    "            return \"Record Level Projection\"\n",
    "        return \"Record Level Embedding\"\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return {\n",
    "            \"dimensions_per_feature\": self.embed_dimension\n",
    "        }\n",
    "\n",
    "    def apply(self, X:List[keras.Input], prefix: str = None):\n",
    "        if prefix is None:\n",
    "            prefix = \"\"\n",
    "\n",
    "        assert self.model_input_specification.categorical_format == CategoricalFormat.OneHot\n",
    "\n",
    "        x = Concatenate(name=f\"{prefix}feature_concat\", axis=-1)(X)\n",
    "        x = Dense(self.embed_dimension, activation=\"linear\", use_bias=not self.project, name=f\"{prefix}embed\")(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def required_input_format(self) -> CategoricalFormat:\n",
    "        return CategoricalFormat.OneHot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Input Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BasePreProcessing(Component):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit_numerical(self, column_name:str, values:np.array):\n",
    "        raise NotImplementedError(\"Please override this base class with a custom implementation\")\n",
    "\n",
    "    def transform_numerical(self, column_name:str, values: np.array):\n",
    "        raise NotImplementedError(\"Please override this base class with a custom implementation\")\n",
    "\n",
    "    def fit_categorical(self, column_name:str, values:np.array):\n",
    "        raise NotImplementedError(\"Please override this base class with a custom implementation\")\n",
    "\n",
    "    def transform_categorical(self, column_name:str, values:np.array, expected_categorical_format:CategoricalFormat):\n",
    "        raise NotImplementedError(\"Please override this base class with a custom implementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class StandardPreProcessing(BasePreProcessing):\n",
    "    def __init__(self, n_categorical_levels: int, clip_numerical_values:bool=False):\n",
    "        super().__init__()\n",
    "        self.n_categorical_levels:int = n_categorical_levels\n",
    "        self.clip_numerical_values:bool = clip_numerical_values\n",
    "        self.min_range = {}\n",
    "        self.encoded_levels = {}\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"Standard Preprocessing\"\n",
    "\n",
    "    @property\n",
    "    def parameters(self) -> dict:\n",
    "        return {\n",
    "            \"n_categorical_levels\": self.n_categorical_levels,\n",
    "            \"clip_numerical_values\": self.clip_numerical_values\n",
    "        }\n",
    "\n",
    "    def fit_numerical(self, column_name: str, values: np.array):\n",
    "\n",
    "        v0 = np.min(values)\n",
    "        v1 = np.max(values)\n",
    "        r = v1 - v0\n",
    "\n",
    "        self.min_range[column_name] = (v0, r)\n",
    "\n",
    "    def transform_numerical(self, column_name: str, values: np.array):\n",
    "        col_min, col_range = self.min_range[column_name]\n",
    "\n",
    "        if col_range == 0:\n",
    "            return np.zeros_like(values, dtype=\"float32\")\n",
    "\n",
    "        # center on zero\n",
    "        values -= col_min\n",
    "\n",
    "        # apply a logarithm\n",
    "        col_values = np.log(values + 1)\n",
    "\n",
    "        # scale max to 1\n",
    "        col_values *= 1. / np.log(col_range + 1)\n",
    "\n",
    "        if self.clip_numerical_values:\n",
    "            col_values = np.clip(col_values, 0., 1.)\n",
    "\n",
    "        return col_values\n",
    "\n",
    "    def fit_categorical(self, column_name: str, values: np.array):\n",
    "        levels, level_counts = np.unique(values, return_counts=True)\n",
    "        sorted_levels = list(sorted(zip(levels, level_counts), key=lambda x: x[1], reverse=True))\n",
    "        self.encoded_levels[column_name] = [s[0] for s in sorted_levels[:self.n_categorical_levels]]\n",
    "\n",
    "\n",
    "    def transform_categorical(self, column_name:str, values: np.array, expected_categorical_format: CategoricalFormat):\n",
    "        encoded_levels = self.encoded_levels[column_name]\n",
    "        print(f\"Encoding the {len(encoded_levels)} levels for {column_name}\")\n",
    "\n",
    "        result_values = np.ones(len(values), dtype=\"uint32\")\n",
    "        for level_i, level in enumerate(encoded_levels):\n",
    "            level_mask = values == level\n",
    "\n",
    "            # we use +1 here, as 0 = previously unseen, and 1 to (n + 1) are the encoded levels\n",
    "            result_values[level_mask] = level_i + 1\n",
    "\n",
    "        if expected_categorical_format == CategoricalFormat.Integers:\n",
    "            return result_values\n",
    "\n",
    "        v = pd.get_dummies(result_values, prefix=column_name)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Transformer classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BaseSequential(FunctionalComponent):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(Layer):\n",
    "    def __init__(self, input_dimension:int, inner_dimension:int, num_heads:int, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.input_dimension = input_dimension\n",
    "        self.inner_dimension = inner_dimension\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=input_dimension)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(inner_dimension, activation='relu'),\n",
    "            Dense(input_dimension)\n",
    "        ])\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    # noinspection PyMethodOverriding\n",
    "    def call(self, inputs, training, mask=None):\n",
    "        # inputs = (target_seq, enc_output)\n",
    "        target_seq = inputs\n",
    "        enc_output = inputs\n",
    "\n",
    "        # self attention of target_seq\n",
    "        attn_output = self.mha(target_seq, target_seq)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = target_seq + attn_output\n",
    "        out1 = self.layernorm1(out1)\n",
    "\n",
    "        # multi-head attention with encoder output as the key and value, and target_seq as the query\n",
    "        attn_output = self.mha(out1, enc_output)\n",
    "        attn_output = self.dropout2(attn_output, training=training)\n",
    "        out2 = out1 + attn_output\n",
    "        out2 = self.layernorm2(out2)\n",
    "\n",
    "        # feed forward network\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out3 = out2 + ffn_output\n",
    "        out3 = self.layernorm2(out3)\n",
    "\n",
    "        return out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GPT3Attention(layers.Layer):\n",
    "    def __init__(self, n_heads, d_model, dropout_rate=0.1):\n",
    "        super(GPT3Attention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // n_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.n_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # noinspection PyMethodOverriding\n",
    "    def call(self, q, k, v, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        scaled_attention_logits = tf.matmul(q, k, transpose_b=True)\n",
    "        scaled_attention_logits = scaled_attention_logits / tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
    "        output = tf.reshape(output, (batch_size, -1, self.d_model))\n",
    "\n",
    "        output = self.dense(output)\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class MultiHeadAttentionImplementation:\n",
    "    Keras = 0,\n",
    "    GPT3 = 1\n",
    "\n",
    "class TransformerEncoderBlock(layers.Layer):\n",
    "    def __init__(self, input_dimension:int, inner_dimension:int, num_heads:int, dropout_rate=0.1, use_conv:bool=False, prefix:str=None, attn_implementation:MultiHeadAttentionImplementation = MultiHeadAttentionImplementation.Keras):\n",
    "\n",
    "        if prefix is None:\n",
    "            prefix = \"\"\n",
    "\n",
    "        super().__init__(name=f\"{prefix}transformer_encoder\")\n",
    "\n",
    "        if inner_dimension < input_dimension:\n",
    "            warnings.warn(f\"Typically inner_dimension should be greater than or equal to the input_dimension!\")\n",
    "\n",
    "        self.attn_implementation = attn_implementation\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.attention = \\\n",
    "            layers.MultiHeadAttention(num_heads=num_heads, key_dim=inner_dimension, name=f\"{prefix}multi_head_attn\") \\\n",
    "                if attn_implementation == MultiHeadAttentionImplementation.Keras else\\\n",
    "                GPT3Attention(num_heads, inner_dimension, dropout_rate=0.0)\n",
    "\n",
    "        layer_norm = 1e-6\n",
    "\n",
    "        self.attention_dropout = layers.Dropout(dropout_rate, name=f\"{prefix}attention_dropout\")\n",
    "        self.attention_layer_norm = layers.LayerNormalization(epsilon=layer_norm, name=f\"{prefix}attention_layer_norm\")\n",
    "\n",
    "        self.feed_forward_0 = Conv1D(filters=inner_dimension, kernel_size=1, activation=\"relu\", name=f\"{prefix}feed_forward_0\") \\\n",
    "            if use_conv else Dense(inner_dimension, activation=\"relu\", name=f\"{prefix}feed_forward_0\")\n",
    "        self.feed_forward_1 = Conv1D(filters=input_dimension, kernel_size=1, activation=\"relu\", name=f\"{prefix}feed_forward_1\") \\\n",
    "            if use_conv else Dense(input_dimension, activation=\"relu\", name=f\"{prefix}feed_forward_1\")\n",
    "\n",
    "        self.feed_forward_dropout = layers.Dropout(dropout_rate, name=f\"{prefix}feed_forward_dropout\")\n",
    "        self.feed_forward_layer_norm = layers.LayerNormalization(epsilon=layer_norm, name=f\"{prefix}feed_forward_layer_norm\")\n",
    "\n",
    "    # noinspection PyMethodOverriding\n",
    "    def call(self, inputs, training, mask=None):\n",
    "        x = inputs\n",
    "        x = self.attention(x, x) if self.attn_implementation == MultiHeadAttentionImplementation.Keras else self.attention(x, x, x, mask)\n",
    "\n",
    "        attention_output = self.attention_dropout(x, training=training) if self.dropout_rate > 0 else x\n",
    "\n",
    "        x = inputs + attention_output\n",
    "        x = self.attention_layer_norm(x)\n",
    "        x = self.feed_forward_0(x)\n",
    "        x = self.feed_forward_1(x)\n",
    "        x = self.feed_forward_dropout(x, training=training) if self.dropout_rate > 0 else x\n",
    "        feed_forward_output = x\n",
    "\n",
    "        return self.feed_forward_layer_norm(attention_output + feed_forward_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BasicTransformer(BaseSequential):\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        if self.use_conv:\n",
    "            return f\"Basic Conv Transformer\" + (\" Decoder\" if self.is_decoder else \"\")\n",
    "        else:\n",
    "            return f\"Basic Dense Transformer\" + (\" Decoder\" if self.is_decoder else \"\")\n",
    "\n",
    "    @property\n",
    "    def parameters(self) -> dict:\n",
    "        return {\n",
    "            \"n_layers\": self.n_layers,\n",
    "            \"internal_size\": self.internal_size,\n",
    "            \"use_conv\": self.use_conv,\n",
    "            \"n_heads\": self.n_heads,\n",
    "            \"dropout_rate\": self.dropout_rate,\n",
    "            \"head_size\": self.internal_size\n",
    "        }\n",
    "\n",
    "    def __init__(self, n_layers:int, internal_size:int, n_heads:int, use_conv:bool=False, dropout_rate:float=0.1, is_decoder=False):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.internal_size = internal_size\n",
    "        self.use_conv = use_conv\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "    def apply(self, X, prefix: str = None):\n",
    "        #window_size = self.sequence_length\n",
    "        real_size = X.shape[-1]\n",
    "\n",
    "        m_x = X\n",
    "\n",
    "        for layer_i in range(self.n_layers):\n",
    "            if self.is_decoder:\n",
    "                if self.use_conv:\n",
    "                    raise NotImplementedError()\n",
    "                m_x = TransformerDecoderBlock(real_size, self.internal_size, self.n_heads, dropout_rate=self.dropout_rate)(m_x)\n",
    "            else:\n",
    "                m_x = TransformerEncoderBlock(real_size, self.internal_size, self.n_heads, dropout_rate=self.dropout_rate, use_conv=self.use_conv, prefix=f\"{prefix}block_{layer_i}_\")(m_x)\n",
    "\n",
    "        return m_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GPTSmallTransformer(BaseSequential):\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"GPT Model\"\n",
    "\n",
    "    @property\n",
    "    def parameters(self) -> dict:\n",
    "        return {\n",
    "            \"n_layers\": self.n_layers,\n",
    "            \"internal_size\": self.internal_size,\n",
    "            \"n_heads\": self.n_heads,\n",
    "            \"dropout_rate\": self.dropout_rate,\n",
    "            \"head_size\": self.head_size\n",
    "        }\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_layers = 12\n",
    "        self.internal_size = 768\n",
    "        self.n_heads = 12\n",
    "        self.head_size = self.internal_size / self.n_heads\n",
    "        self.dropout_rate = 0.02\n",
    "        self.is_decoder = True\n",
    "\n",
    "    def apply(self, X, prefix: str = None):\n",
    "        #window_size = self.sequence_length\n",
    "        real_size = X.shape[-1]\n",
    "\n",
    "        m_x = X\n",
    "\n",
    "        for layer_i in range(self.n_layers):\n",
    "            m_x = TransformerDecoderBlock(real_size, self.internal_size, self.n_heads, dropout_rate=self.dropout_rate)(m_x)\n",
    "\n",
    "        return m_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main FlowTransformer T_T**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FlowTransformer:\n",
    "    retain_inmem_cache = False\n",
    "    inmem_cache = None\n",
    "\n",
    "    def  __init__(self, pre_processing:BasePreProcessing,\n",
    "                  input_encoding:BaseInputEncoding,\n",
    "                  sequential_model:FunctionalComponent,\n",
    "                  classification_head:BaseClassificationHead,\n",
    "                  params:FlowTransformerParameters,\n",
    "                  rs:np.random.RandomState=None):\n",
    "\n",
    "        self.rs = np.random.RandomState() if rs is None else rs\n",
    "        self.classification_head = classification_head\n",
    "        self.sequential_model = sequential_model\n",
    "        self.input_encoding = input_encoding\n",
    "        self.pre_processing = pre_processing\n",
    "        self.parameters = params\n",
    "\n",
    "        self.dataset_specification: Optional[DatasetSpecification] = None\n",
    "\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "        self.training_mask = None\n",
    "        self.model_input_spec: Optional[ModelInputSpecification] = None\n",
    "\n",
    "        self.experiment_key = {}\n",
    "\n",
    "    def build_model(self, prefix:str=None):\n",
    "        if prefix is None:\n",
    "            prefix = \"\"\n",
    "\n",
    "        if self.X is None:\n",
    "            raise Exception(\"Please call load_dataset before calling build_model()\")\n",
    "\n",
    "        m_inputs = []\n",
    "        for numeric_feature in self.model_input_spec.numeric_feature_names:\n",
    "            m_input = Input((self.parameters.window_size, 1), name=f\"{prefix}input_{numeric_feature}\", dtype=\"float32\")\n",
    "            m_inputs.append(m_input)\n",
    "\n",
    "        for categorical_feature_name, categorical_feature_levels in \\\n",
    "            zip(self.model_input_spec.categorical_feature_names, self.model_input_spec.levels_per_categorical_feature):\n",
    "            m_input = Input(\n",
    "                (self.parameters.window_size, 1 if self.model_input_spec.categorical_format == CategoricalFormat.Integers else categorical_feature_levels),\n",
    "                name=f\"{prefix}input_{categorical_feature_name}\",\n",
    "                dtype=\"int32\" if self.model_input_spec.categorical_format == CategoricalFormat.Integers else \"float32\"\n",
    "            )\n",
    "            m_inputs.append(m_input)\n",
    "\n",
    "        self.input_encoding.build(self.parameters.window_size, self.model_input_spec)\n",
    "        self.sequential_model.build(self.parameters.window_size, self.model_input_spec)\n",
    "        self.classification_head.build(self.parameters.window_size, self.model_input_spec)\n",
    "\n",
    "        m_x = self.input_encoding.apply(m_inputs, prefix)\n",
    "\n",
    "        # in case the classification head needs to add tokens at this stage\n",
    "        m_x = self.classification_head.apply_before_transformer(m_x, prefix)\n",
    "\n",
    "        m_x = self.sequential_model.apply(m_x, prefix)\n",
    "        m_x = self.classification_head.apply(m_x, prefix)\n",
    "\n",
    "        for layer_i, layer_size in enumerate(self.parameters.mlp_layer_sizes):\n",
    "            m_x = Dense(layer_size, activation=\"relu\", name=f\"{prefix}classification_mlp_{layer_i}_{layer_size}\")(m_x)\n",
    "            m_x = Dropout(self.parameters.mlp_dropout)(m_x) if self.parameters.mlp_dropout > 0 else m_x\n",
    "\n",
    "        m_x = Dense(1, activation=\"sigmoid\", name=f\"{prefix}binary_classification_out\")(m_x)\n",
    "        m = Model(m_inputs, m_x)\n",
    "        #m.summary()\n",
    "        return m\n",
    "\n",
    "    def _load_preprocessed_dataset(self, dataset_name:str,\n",
    "                     dataset:Union[pd.DataFrame, str],\n",
    "                     specification:DatasetSpecification,\n",
    "                     cache_folder:Optional[str]=None,\n",
    "                     n_rows:int=0,\n",
    "                     evaluation_dataset_sampling:EvaluationDatasetSampling=EvaluationDatasetSampling.LastRows,\n",
    "                     evaluation_percent:float=0.2,\n",
    "                     numerical_filter=1_000_000_000) -> Tuple[pd.DataFrame, ModelInputSpecification]:\n",
    "\n",
    "        cache_file_path = None\n",
    "\n",
    "        if dataset_name is None:\n",
    "            raise Exception(f\"Dataset name must be specified so FlowTransformer can optimise operations between subsequent calls!\")\n",
    "\n",
    "        pp_key = get_identifier(\n",
    "            {\n",
    "                \"__preprocessing_name\": self.pre_processing.name,\n",
    "                **self.pre_processing.parameters\n",
    "            }\n",
    "        )\n",
    "\n",
    "        local_key = get_identifier({\n",
    "            \"evaluation_percent\": evaluation_percent,\n",
    "            \"numerical_filter\": numerical_filter,\n",
    "            \"categorical_method\": str(self.input_encoding.required_input_format),\n",
    "            \"n_rows\": n_rows,\n",
    "        })\n",
    "\n",
    "        cache_key = f\"{dataset_name}_{n_rows}_{pp_key}_{local_key}\"\n",
    "\n",
    "        if FlowTransformer.retain_inmem_cache:\n",
    "            if FlowTransformer.inmem_cache is not None and cache_key in FlowTransformer.inmem_cache:\n",
    "                print(f\"Using in-memory cached version of this pre-processed dataset. To turn off this functionality set FlowTransformer.retain_inmem_cache = False\")\n",
    "                return FlowTransformer.inmem_cache[cache_key]\n",
    "\n",
    "        if cache_folder is not None:\n",
    "            cache_file_name = f\"{cache_key}.feather\"\n",
    "            cache_file_path = os.path.join(cache_folder, cache_file_name)\n",
    "\n",
    "            print(f\"Using cache file path: {cache_file_path}\")\n",
    "\n",
    "            if os.path.exists(cache_file_path):\n",
    "                print(f\"Reading directly from cache {cache_file_path}...\")\n",
    "                model_input_spec: ModelInputSpecification\n",
    "                dataset, model_input_spec = load_feather_plus_metadata(cache_file_path)\n",
    "                return dataset, model_input_spec\n",
    "\n",
    "        if isinstance(dataset, str):\n",
    "            print(f\"Attempting to read dataset from path {dataset}...\")\n",
    "            if dataset.lower().endswith(\".feather\"):\n",
    "                # read as a feather file\n",
    "                dataset = pd.read_feather(dataset, columns=specification.include_fields+[specification.class_column])\n",
    "            elif dataset.lower().endswith(\".csv\"):\n",
    "                dataset = pd.read_csv(dataset, nrows=n_rows if n_rows > 0 else None)\n",
    "            else:\n",
    "                raise Exception(\"Unrecognised dataset filetype!\")\n",
    "        elif not isinstance(dataset, pd.DataFrame):\n",
    "            raise Exception(\"Unrecognised dataset input type, should be a path to a CSV or feather file, or a pandas dataframe!\")\n",
    "\n",
    "        assert isinstance(dataset, pd.DataFrame)\n",
    "\n",
    "        if 0 < n_rows < len(dataset):\n",
    "            dataset = dataset.iloc[:n_rows]\n",
    "\n",
    "        training_mask = np.ones(len(dataset),  dtype=bool)\n",
    "        eval_n = int(len(dataset) * evaluation_percent)\n",
    "\n",
    "        if evaluation_dataset_sampling == EvaluationDatasetSampling.FilterColumn:\n",
    "            if dataset.columns[-1] != specification.test_column:\n",
    "                raise Exception(f\"Ensure that the 'test' ({specification.test_column}) column is the last column of the dataset being loaded, and that the name of this column is provided as part of the dataset specification\")\n",
    "\n",
    "        if evaluation_dataset_sampling != EvaluationDatasetSampling.LastRows:\n",
    "            warnings.warn(\"Using EvaluationDatasetSampling options other than LastRows might leak some information during training, if for example the context window leading up to a particular flow contains an evaluation flow, and this flow has out of range values (out of range to when pre-processing was applied on the training flows), then the model might potentially learn to handle these. In any case, no class leakage is present.\")\n",
    "\n",
    "        if evaluation_dataset_sampling == EvaluationDatasetSampling.LastRows:\n",
    "            training_mask[-eval_n:] = False\n",
    "        elif evaluation_dataset_sampling == EvaluationDatasetSampling.RandomRows:\n",
    "            index = np.arange(self.parameters.window_size, len(dataset))\n",
    "            sample = self.rs.choice(index, eval_n, replace=False)\n",
    "            training_mask[sample] = False\n",
    "        elif evaluation_dataset_sampling == EvaluationDatasetSampling.FilterColumn:\n",
    "            # must be the last column of the dataset\n",
    "            training_column = dataset.columns[-1]\n",
    "            print(f\"Using the last column {training_column} as the training mask column\")\n",
    "\n",
    "            v, c = np.unique(dataset[training_column].values,  return_counts=True)\n",
    "            min_index = np.argmin(c)\n",
    "            min_v = v[min_index]\n",
    "\n",
    "            warnings.warn(f\"Autodetected class {min_v} of {training_column} to represent the evaluation class!\")\n",
    "\n",
    "            eval_indices = np.argwhere(dataset[training_column].values == min_v).reshape(-1)\n",
    "            eval_indices = eval_indices[(eval_indices > self.parameters.window_size)]\n",
    "\n",
    "            training_mask[eval_indices] = False\n",
    "            del dataset[training_column]\n",
    "\n",
    "        numerical_columns = set(specification.include_fields).difference(specification.categorical_fields)\n",
    "        categorical_columns = specification.categorical_fields\n",
    "\n",
    "        print(f\"Set y to = {specification.class_column}\")\n",
    "        new_df = {\"__training\": training_mask, \"__y\": dataset[specification.class_column].values}\n",
    "        new_features = []\n",
    "\n",
    "        print(\"Converting numerical columns to floats, and removing out of range values...\")\n",
    "        for col_name in numerical_columns:\n",
    "            assert col_name in dataset.columns\n",
    "            new_features.append(col_name)\n",
    "\n",
    "            col_values = dataset[col_name].values\n",
    "            col_values[~np.isfinite(col_values)] = 0\n",
    "            col_values[col_values < -numerical_filter] = 0\n",
    "            col_values[col_values > numerical_filter] = 0\n",
    "            col_values = col_values.astype(\"float32\")\n",
    "\n",
    "            if not np.all(np.isfinite(col_values)):\n",
    "                raise Exception(\"Flow format data had non finite values after float transformation!\")\n",
    "\n",
    "            new_df[col_name] = col_values\n",
    "\n",
    "        print(f\"Applying pre-processing to numerical values\")\n",
    "        for i, col_name in enumerate(numerical_columns):\n",
    "            print(f\"[Numerical {i+1:,} / {len(numerical_columns)}] Processing numerical column {col_name}...\")\n",
    "            all_data = new_df[col_name]\n",
    "            training_data = all_data[training_mask]\n",
    "\n",
    "            self.pre_processing.fit_numerical(col_name, training_data)\n",
    "            new_df[col_name] = self.pre_processing.transform_numerical(col_name, all_data)\n",
    "\n",
    "        print(f\"Applying pre-processing to categorical values\")\n",
    "        levels_per_categorical_feature = []\n",
    "        for i, col_name in enumerate(categorical_columns):\n",
    "            new_features.append(col_name)\n",
    "            if col_name == specification.class_column:\n",
    "                continue\n",
    "            print(f\"[Categorical {i+1:,} / {len(categorical_columns)}] Processing categorical column {col_name}...\")\n",
    "\n",
    "            all_data = dataset[col_name].values\n",
    "            training_data = all_data[training_mask]\n",
    "\n",
    "            self.pre_processing.fit_categorical(col_name, training_data)\n",
    "            new_values = self.pre_processing.transform_categorical(col_name, all_data, self.input_encoding.required_input_format)\n",
    "\n",
    "            if self.input_encoding.required_input_format == CategoricalFormat.OneHot:\n",
    "                # multiple columns of one hot values\n",
    "                if isinstance(new_values, pd.DataFrame):\n",
    "                    levels_per_categorical_feature.append(len(new_values.columns))\n",
    "                    for c in new_values.columns:\n",
    "                        new_df[c] = new_values[c]\n",
    "                else:\n",
    "                    n_one_hot_levels = new_values.shape[1]\n",
    "                    levels_per_categorical_feature.append(n_one_hot_levels)\n",
    "                    for z in range(n_one_hot_levels):\n",
    "                        new_df[f\"{col_name}_{z}\"] = new_values[:, z]\n",
    "            else:\n",
    "                # single column of integers\n",
    "                levels_per_categorical_feature.append(len(np.unique(new_values)))\n",
    "                new_df[col_name] = new_values\n",
    "\n",
    "        print(f\"Generating pre-processed dataframe...\")\n",
    "        new_df = pd.DataFrame(new_df)\n",
    "        model_input_spec = ModelInputSpecification(new_features, len(numerical_columns), levels_per_categorical_feature, self.input_encoding.required_input_format)\n",
    "\n",
    "        print(f\"Input data frame had shape ({len(dataset)},{len(dataset.columns)}), output data frame has shape ({len(new_df)},{len(new_df.columns)}) after pre-processing...\")\n",
    "\n",
    "        if cache_file_path is not None:\n",
    "            print(f\"Writing to cache file path: {cache_file_path}...\")\n",
    "            save_feather_plus_metadata(cache_file_path, new_df, model_input_spec)\n",
    "\n",
    "        if FlowTransformer.retain_inmem_cache:\n",
    "            if FlowTransformer.inmem_cache is None:\n",
    "                FlowTransformer.inmem_cache = {}\n",
    "\n",
    "            FlowTransformer.inmem_cache.clear()\n",
    "            FlowTransformer.inmem_cache[cache_key] = (new_df, model_input_spec)\n",
    "\n",
    "        return new_df, model_input_spec\n",
    "\n",
    "    def load_dataset(self, dataset_name:str,\n",
    "                     dataset:Union[pd.DataFrame, str],\n",
    "                     specification:DatasetSpecification,\n",
    "                     cache_path:Optional[str]=None,\n",
    "                     n_rows:int=0,\n",
    "                     evaluation_dataset_sampling:EvaluationDatasetSampling=EvaluationDatasetSampling.LastRows,\n",
    "                     evaluation_percent:float=0.2,\n",
    "                     numerical_filter=1_000_000_000) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load a dataset and prepare it for training\n",
    "\n",
    "        :param dataset: The path to a CSV dataset to load from, or a dataframe\n",
    "        :param cache_path: Where to store a cached version of this file\n",
    "        :param n_rows: The number of rows to ingest from the dataset, or 0 to ingest all\n",
    "        \"\"\"\n",
    "\n",
    "        if cache_path is None:\n",
    "            cache_path = \"cache\"\n",
    "\n",
    "        if not os.path.exists(cache_path):\n",
    "            warnings.warn(f\"Could not find cache folder: {cache_path}, attempting to create\")\n",
    "            os.mkdir(cache_path)\n",
    "\n",
    "        self.dataset_specification = specification\n",
    "        df, model_input_spec = self._load_preprocessed_dataset(dataset_name, dataset, specification, cache_path, n_rows, evaluation_dataset_sampling, evaluation_percent, numerical_filter)\n",
    "\n",
    "        training_mask = df[\"__training\"].values\n",
    "        del df[\"__training\"]\n",
    "\n",
    "        y = df[\"__y\"].values\n",
    "        del df[\"__y\"]\n",
    "\n",
    "        self.X = df\n",
    "        self.y = y\n",
    "        self.training_mask = training_mask\n",
    "        self.model_input_spec = model_input_spec\n",
    "\n",
    "        return df\n",
    "\n",
    "    def evaluate(self, m:keras.Model, batch_size, early_stopping_patience:int=5, epochs:int=100, steps_per_epoch:int=128):\n",
    "        n_malicious_per_batch = int(0.5 * batch_size)\n",
    "        n_legit_per_batch = batch_size - n_malicious_per_batch\n",
    "\n",
    "        overall_y_preserve = np.zeros(dtype=\"float32\", shape=(n_malicious_per_batch + n_legit_per_batch,))\n",
    "        overall_y_preserve[:n_malicious_per_batch] = 1.\n",
    "\n",
    "        selectable_mask = np.zeros(len(self.X), dtype=bool)\n",
    "        selectable_mask[self.parameters.window_size:-self.parameters.window_size] = True\n",
    "        train_mask = self.training_mask\n",
    "\n",
    "        y_mask = ~(self.y.astype('str') == str(self.dataset_specification.benign_label))\n",
    "\n",
    "        indices_train = np.argwhere(train_mask).reshape(-1)\n",
    "        malicious_indices_train = np.argwhere(train_mask & y_mask & selectable_mask).reshape(-1)\n",
    "        legit_indices_train = np.argwhere(train_mask & ~y_mask & selectable_mask).reshape(-1)\n",
    "\n",
    "        indices_test:np.ndarray = np.argwhere(~train_mask).reshape(-1)\n",
    "\n",
    "        def get_windows_for_indices(indices:np.ndarray, ordered) -> List[pd.DataFrame]:\n",
    "            X: List[pd.DataFrame] = []\n",
    "\n",
    "            if ordered:\n",
    "                # we don't really want to include eval samples as part of context, because out of range values might be learned\n",
    "                # by the model, _but_ we are forced to in the windowed approach, if users haven't just selected the\n",
    "                # \"take last 10%\" as eval option. We warn them prior to this though.\n",
    "                for i1 in indices:\n",
    "                    X.append(self.X.iloc[(i1 - self.parameters.window_size) + 1:i1 + 1])\n",
    "            else:\n",
    "                context_indices_batch = np.random.choice(indices_train, size=(batch_size, self.parameters.window_size),\n",
    "                                                         replace=False).reshape(-1)\n",
    "                context_indices_batch[:, -1] = indices\n",
    "\n",
    "                for index in context_indices_batch:\n",
    "                    X.append(self.X.iloc[index])\n",
    "\n",
    "            return X\n",
    "\n",
    "        feature_columns_map = {}\n",
    "\n",
    "        def samplewise_to_featurewise(X):\n",
    "            sequence_length = len(X[0])\n",
    "\n",
    "            combined_df = pd.concat(X)\n",
    "\n",
    "            featurewise_X = []\n",
    "\n",
    "            if len(feature_columns_map) == 0:\n",
    "                for feature in self.model_input_spec.feature_names:\n",
    "                    if feature in self.model_input_spec.numeric_feature_names or self.model_input_spec.categorical_format == CategoricalFormat.Integers:\n",
    "                        feature_columns_map[feature] = feature\n",
    "                    else:\n",
    "                        # this is a one-hot encoded categorical feature\n",
    "                        feature_columns_map[feature] = [c for c in X[0].columns if str(c).startswith(feature)]\n",
    "\n",
    "            for feature in self.model_input_spec.feature_names:\n",
    "                feature_columns = feature_columns_map[feature]\n",
    "                combined_values = combined_df[feature_columns].values\n",
    "\n",
    "                # maybe this can be faster with a reshape but I couldn't get it to work\n",
    "                combined_values = np.array([combined_values[i:i+sequence_length] for i in range(0, len(combined_values), sequence_length)])\n",
    "                featurewise_X.append(combined_values)\n",
    "\n",
    "            return featurewise_X\n",
    "\n",
    "        print(f\"Building eval dataset...\")\n",
    "        eval_X = get_windows_for_indices(indices_test, True)\n",
    "        print(f\"Splitting dataset to featurewise...\")\n",
    "        eval_featurewise_X = samplewise_to_featurewise(eval_X)\n",
    "        eval_y = y_mask[indices_test]\n",
    "        eval_P = eval_y\n",
    "        n_eval_P = np.count_nonzero(eval_P)\n",
    "        eval_N = ~eval_y\n",
    "        n_eval_N = np.count_nonzero(eval_N)\n",
    "        print(f\"Evaluation dataset is built!\")\n",
    "\n",
    "        print(f\"Positive samples in eval set: {n_eval_P}\")\n",
    "        print(f\"Negative samples in eval set: {n_eval_N}\")\n",
    "\n",
    "        epoch_results = []\n",
    "\n",
    "        def run_evaluation(epoch):\n",
    "            pred_y = m.predict(eval_featurewise_X, verbose=True)\n",
    "            pred_y = pred_y.reshape(-1) > 0.5\n",
    "\n",
    "            pred_P = pred_y\n",
    "            n_pred_P = np.count_nonzero(pred_P)\n",
    "\n",
    "            pred_N = ~pred_y\n",
    "            n_pred_N = np.count_nonzero(pred_N)\n",
    "\n",
    "            TP = np.count_nonzero(pred_P & eval_P)\n",
    "            FP = np.count_nonzero(pred_P & ~eval_P)\n",
    "            TN = np.count_nonzero(pred_N & eval_N)\n",
    "            FN = np.count_nonzero(pred_N & ~eval_N)\n",
    "\n",
    "            sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "            specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "            balanced_accuracy = (sensitivity + specificity) / 2\n",
    "\n",
    "            precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "            recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            print(f\"Epoch {epoch} yielded predictions: {pred_y.shape}, overall balanced accuracy: {balanced_accuracy * 100:.2f}%, TP = {TP:,} / {n_eval_P:,}, TN = {TN:,} / {n_eval_N:,}\")\n",
    "\n",
    "            epoch_results.append({\n",
    "                \"epoch\": epoch,\n",
    "                \"P\": n_eval_P,\n",
    "                \"N\": n_eval_N,\n",
    "                \"pred_P\": n_pred_P,\n",
    "                \"pred_N\": n_pred_N,\n",
    "                \"TP\": TP,\n",
    "                \"FP\": FP,\n",
    "                \"TN\": TN,\n",
    "                \"FN\": FN,\n",
    "                \"bal_acc\": balanced_accuracy,\n",
    "                \"f1\": f1_score\n",
    "            })\n",
    "\n",
    "\n",
    "        class BatchYielder():\n",
    "            def __init__(self, ordered, random, rs):\n",
    "                self.ordered = ordered\n",
    "                self.random = random\n",
    "                self.cursor_malicious = 0\n",
    "                self.cursor_legit = 0\n",
    "                self.rs = rs\n",
    "\n",
    "            def get_batch(self):\n",
    "                malicious_indices_batch = self.rs.choice(malicious_indices_train, size=n_malicious_per_batch,\n",
    "                                                         replace=False) \\\n",
    "                    if self.random else \\\n",
    "                    malicious_indices_train[self.cursor_malicious:self.cursor_malicious + n_malicious_per_batch]\n",
    "\n",
    "                legitimate_indices_batch = self.rs.choice(legit_indices_train, size=n_legit_per_batch, replace=False) \\\n",
    "                    if self.random else \\\n",
    "                    legit_indices_train[self.cursor_legit:self.cursor_legit + n_legit_per_batch]\n",
    "\n",
    "                indices = np.concatenate([malicious_indices_batch, legitimate_indices_batch])\n",
    "\n",
    "                self.cursor_malicious = self.cursor_malicious + n_malicious_per_batch\n",
    "                self.cursor_malicious = self.cursor_malicious % (len(malicious_indices_train) - n_malicious_per_batch)\n",
    "\n",
    "                self.cursor_legit = self.cursor_legit + n_legit_per_batch\n",
    "                self.cursor_legit = self.cursor_legit % (len(legit_indices_train) - n_legit_per_batch)\n",
    "\n",
    "                X = get_windows_for_indices(indices, self.ordered)\n",
    "                # each x in X contains a dataframe, with window_size rows and all the features of the flows. There are batch_size of these.\n",
    "\n",
    "                # we have a dataframe containing batch_size x (window_size, features)\n",
    "                # we actually want a result of features x (batch_size, sequence_length, feature_dimension)\n",
    "                featurewise_X = samplewise_to_featurewise(X)\n",
    "\n",
    "                return featurewise_X, overall_y_preserve\n",
    "\n",
    "        batch_yielder = BatchYielder(self.parameters._train_ensure_flows_are_ordered_within_windows, not self.parameters._train_draw_sequential_windows, self.rs)\n",
    "\n",
    "        min_loss = 100\n",
    "        iters_since_loss_decrease = 0\n",
    "\n",
    "        train_results = []\n",
    "        final_epoch = 0\n",
    "\n",
    "        last_print = time.time()\n",
    "        elapsed_time = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            final_epoch = epoch\n",
    "\n",
    "            has_reduced_loss = False\n",
    "            for step in range(steps_per_epoch):\n",
    "                batch_X, batch_y = batch_yielder.get_batch()\n",
    "\n",
    "                t0 = time.time()\n",
    "                batch_results = m.train_on_batch(batch_X, batch_y)\n",
    "                t1 = time.time()\n",
    "\n",
    "                if epoch > 0 or step > 0:\n",
    "                    elapsed_time += (t1 - t0)\n",
    "                    if epoch == 0 and step == 1:\n",
    "                        # include time for last \"step\" that we skipped with step > 0 for epoch == 0\n",
    "                        elapsed_time *= 2\n",
    "\n",
    "                train_results.append(batch_results + [elapsed_time, epoch])\n",
    "\n",
    "                batch_loss = batch_results[0] if isinstance(batch_results, list) else batch_results\n",
    "\n",
    "                if time.time() - last_print > 3:\n",
    "                    last_print = time.time()\n",
    "                    early_stop_phrase = \"\" if early_stopping_patience <= 0 else f\" (early stop in {early_stopping_patience - iters_since_loss_decrease:,})\"\n",
    "                    print(f\"Epoch = {epoch:,} / {epochs:,}{early_stop_phrase}, step = {step}, loss = {batch_loss:.5f}, results = {batch_results} -- elapsed (train): {elapsed_time:.2f}s\")\n",
    "\n",
    "                if batch_loss < min_loss:\n",
    "                    has_reduced_loss = True\n",
    "                    min_loss = batch_loss\n",
    "\n",
    "            if has_reduced_loss:\n",
    "                iters_since_loss_decrease = 0\n",
    "            else:\n",
    "                iters_since_loss_decrease += 1\n",
    "\n",
    "            do_early_stop = early_stopping_patience > 0 and iters_since_loss_decrease > early_stopping_patience\n",
    "            is_last_epoch = epoch == epochs - 1\n",
    "            run_eval = epoch in [6] or is_last_epoch or do_early_stop\n",
    "\n",
    "            if run_eval:\n",
    "                run_evaluation(epoch)\n",
    "\n",
    "            if do_early_stop:\n",
    "                print(f\"Early stopping at epoch: {epoch}\")\n",
    "                break\n",
    "\n",
    "        eval_results = pd.DataFrame(epoch_results)\n",
    "\n",
    "        return (train_results, eval_results, final_epoch)\n",
    "\n",
    "\n",
    "    def time(self, m:keras.Model, batch_size, n_steps=128, n_repeats=4):\n",
    "        n_malicious_per_batch = int(0.5 * batch_size)\n",
    "        n_legit_per_batch = batch_size - n_malicious_per_batch\n",
    "\n",
    "        overall_y_preserve = np.zeros(dtype=\"float32\", shape=(n_malicious_per_batch + n_legit_per_batch,))\n",
    "        overall_y_preserve[:n_malicious_per_batch] = 1.\n",
    "\n",
    "        selectable_mask = np.zeros(len(self.X), dtype=bool)\n",
    "        selectable_mask[self.parameters.window_size:-self.parameters.window_size] = True\n",
    "        train_mask = self.training_mask\n",
    "\n",
    "        y_mask = ~(self.y.astype('str') == str(self.dataset_specification.benign_label))\n",
    "\n",
    "        indices_train = np.argwhere(train_mask).reshape(-1)\n",
    "        malicious_indices_train = np.argwhere(train_mask & y_mask & selectable_mask).reshape(-1)\n",
    "        legit_indices_train = np.argwhere(train_mask & ~y_mask & selectable_mask).reshape(-1)\n",
    "\n",
    "        indices_test:np.ndarray = np.argwhere(~train_mask).reshape(-1)\n",
    "\n",
    "        def get_windows_for_indices(indices:np.ndarray, ordered) -> List[pd.DataFrame]:\n",
    "            X: List[pd.DataFrame] = []\n",
    "\n",
    "            if ordered:\n",
    "                # we don't really want to include eval samples as part of context, because out of range values might be learned\n",
    "                # by the model, _but_ we are forced to in the windowed approach, if users haven't just selected the\n",
    "                # \"take last 10%\" as eval option. We warn them prior to this though.\n",
    "                for i1 in indices:\n",
    "                    X.append(self.X.iloc[(i1 - self.parameters.window_size) + 1:i1 + 1])\n",
    "            else:\n",
    "                context_indices_batch = np.random.choice(indices_train, size=(batch_size, self.parameters.window_size),\n",
    "                                                         replace=False).reshape(-1)\n",
    "                context_indices_batch[:, -1] = indices\n",
    "\n",
    "                for index in context_indices_batch:\n",
    "                    X.append(self.X.iloc[index])\n",
    "\n",
    "            return X\n",
    "\n",
    "        feature_columns_map = {}\n",
    "\n",
    "        def samplewise_to_featurewise(X):\n",
    "            sequence_length = len(X[0])\n",
    "\n",
    "            combined_df = pd.concat(X)\n",
    "\n",
    "            featurewise_X = []\n",
    "\n",
    "            if len(feature_columns_map) == 0:\n",
    "                for feature in self.model_input_spec.feature_names:\n",
    "                    if feature in self.model_input_spec.numeric_feature_names or self.model_input_spec.categorical_format == CategoricalFormat.Integers:\n",
    "                        feature_columns_map[feature] = feature\n",
    "                    else:\n",
    "                        # this is a one-hot encoded categorical feature\n",
    "                        feature_columns_map[feature] = [c for c in X[0].columns if str(c).startswith(feature)]\n",
    "\n",
    "            for feature in self.model_input_spec.feature_names:\n",
    "                feature_columns = feature_columns_map[feature]\n",
    "                combined_values = combined_df[feature_columns].values\n",
    "\n",
    "                # maybe this can be faster with a reshape but I couldn't get it to work\n",
    "                combined_values = np.array([combined_values[i:i+sequence_length] for i in range(0, len(combined_values), sequence_length)])\n",
    "                featurewise_X.append(combined_values)\n",
    "\n",
    "            return featurewise_X\n",
    "\n",
    "\n",
    "        epoch_results = []\n",
    "\n",
    "\n",
    "        class BatchYielder():\n",
    "            def __init__(self, ordered, random, rs):\n",
    "                self.ordered = ordered\n",
    "                self.random = random\n",
    "                self.cursor_malicious = 0\n",
    "                self.cursor_legit = 0\n",
    "                self.rs = rs\n",
    "\n",
    "            def get_batch(self):\n",
    "                malicious_indices_batch = self.rs.choice(malicious_indices_train, size=n_malicious_per_batch,\n",
    "                                                         replace=False) \\\n",
    "                    if self.random else \\\n",
    "                    malicious_indices_train[self.cursor_malicious:self.cursor_malicious + n_malicious_per_batch]\n",
    "\n",
    "                legitimate_indices_batch = self.rs.choice(legit_indices_train, size=n_legit_per_batch, replace=False) \\\n",
    "                    if self.random else \\\n",
    "                    legit_indices_train[self.cursor_legit:self.cursor_legit + n_legit_per_batch]\n",
    "\n",
    "                indices = np.concatenate([malicious_indices_batch, legitimate_indices_batch])\n",
    "\n",
    "                self.cursor_malicious = self.cursor_malicious + n_malicious_per_batch\n",
    "                self.cursor_malicious = self.cursor_malicious % (len(malicious_indices_train) - n_malicious_per_batch)\n",
    "\n",
    "                self.cursor_legit = self.cursor_legit + n_legit_per_batch\n",
    "                self.cursor_legit = self.cursor_legit % (len(legit_indices_train) - n_legit_per_batch)\n",
    "\n",
    "                X = get_windows_for_indices(indices, self.ordered)\n",
    "                # each x in X contains a dataframe, with window_size rows and all the features of the flows. There are batch_size of these.\n",
    "\n",
    "                # we have a dataframe containing batch_size x (window_size, features)\n",
    "                # we actually want a result of features x (batch_size, sequence_length, feature_dimension)\n",
    "                featurewise_X = samplewise_to_featurewise(X)\n",
    "\n",
    "                return featurewise_X, overall_y_preserve\n",
    "\n",
    "        batch_yielder = BatchYielder(self.parameters._train_ensure_flows_are_ordered_within_windows, not self.parameters._train_draw_sequential_windows, self.rs)\n",
    "\n",
    "        min_loss = 100\n",
    "        iters_since_loss_decrease = 0\n",
    "\n",
    "        final_epoch = 0\n",
    "\n",
    "        last_print = time.time()\n",
    "        elapsed_time = 0\n",
    "\n",
    "        batch_times = []\n",
    "\n",
    "\n",
    "        for step in range(n_steps):\n",
    "            batch_X, batch_y = batch_yielder.get_batch()\n",
    "\n",
    "            local_batch_times = []\n",
    "            for i in range(n_repeats):\n",
    "                t0 = time.time()\n",
    "                batch_results = m.predict_on_batch(batch_X)\n",
    "                t1 = time.time()\n",
    "                local_batch_times.append(t1 - t0)\n",
    "\n",
    "            batch_times.append(local_batch_times)\n",
    "\n",
    "            if time.time() - last_print > 3:\n",
    "                last_print = time.time()\n",
    "                print(f\"Step = {step}, running model evaluation... Average times = {np.mean(np.array(batch_times).reshape(-1))}\")\n",
    "\n",
    "        return batch_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported and defined\n",
      "Defined flowtransformer, starting load\n",
      "Using cache file path: cache\\CSE_CIC_IDS_0_QdLmZHuh8yOmlGcKBEkf7hepImY0_VHNk9ujbqtTXGSrgVayeqG486IQ0.feather\n",
      "Attempting to read dataset from path dataset/downsampled/downsampled_df_shuffled.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\AppData\\Local\\Temp\\ipykernel_18048\\3467500244.py:269: UserWarning: Could not find cache folder: cache, attempting to create\n",
      "  warnings.warn(f\"Could not find cache folder: {cache_path}, attempting to create\")\n",
      "C:\\Users\\ahmed\\AppData\\Local\\Temp\\ipykernel_18048\\3467500244.py:143: UserWarning: Using EvaluationDatasetSampling options other than LastRows might leak some information during training, if for example the context window leading up to a particular flow contains an evaluation flow, and this flow has out of range values (out of range to when pre-processing was applied on the training flows), then the model might potentially learn to handle these. In any case, no class leakage is present.\n",
      "  warnings.warn(\"Using EvaluationDatasetSampling options other than LastRows might leak some information during training, if for example the context window leading up to a particular flow contains an evaluation flow, and this flow has out of range values (out of range to when pre-processing was applied on the training flows), then the model might potentially learn to handle these. In any case, no class leakage is present.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set y to = Attack\n",
      "Converting numerical columns to floats, and removing out of range values...\n",
      "Applying pre-processing to numerical values\n",
      "[Numerical 1 / 28] Processing numerical column DST_TO_SRC_AVG_THROUGHPUT...\n",
      "[Numerical 2 / 28] Processing numerical column OUT_BYTES...\n",
      "[Numerical 3 / 28] Processing numerical column RETRANSMITTED_OUT_PKTS...\n",
      "[Numerical 4 / 28] Processing numerical column MIN_TTL...\n",
      "[Numerical 5 / 28] Processing numerical column SRC_TO_DST_AVG_THROUGHPUT...\n",
      "[Numerical 6 / 28] Processing numerical column MAX_IP_PKT_LEN...\n",
      "[Numerical 7 / 28] Processing numerical column RETRANSMITTED_IN_PKTS...\n",
      "[Numerical 8 / 28] Processing numerical column RETRANSMITTED_IN_BYTES...\n",
      "[Numerical 9 / 28] Processing numerical column NUM_PKTS_512_TO_1024_BYTES...\n",
      "[Numerical 10 / 28] Processing numerical column NUM_PKTS_UP_TO_128_BYTES...\n",
      "[Numerical 11 / 28] Processing numerical column FLOW_DURATION_MILLISECONDS...\n",
      "[Numerical 12 / 28] Processing numerical column SHORTEST_FLOW_PKT...\n",
      "[Numerical 13 / 28] Processing numerical column MIN_IP_PKT_LEN...\n",
      "[Numerical 14 / 28] Processing numerical column RETRANSMITTED_OUT_BYTES...\n",
      "[Numerical 15 / 28] Processing numerical column IN_BYTES...\n",
      "[Numerical 16 / 28] Processing numerical column IN_PKTS...\n",
      "[Numerical 17 / 28] Processing numerical column NUM_PKTS_1024_TO_1514_BYTES...\n",
      "[Numerical 18 / 28] Processing numerical column NUM_PKTS_128_TO_256_BYTES...\n",
      "[Numerical 19 / 28] Processing numerical column LONGEST_FLOW_PKT...\n",
      "[Numerical 20 / 28] Processing numerical column NUM_PKTS_256_TO_512_BYTES...\n",
      "[Numerical 21 / 28] Processing numerical column SRC_TO_DST_SECOND_BYTES...\n",
      "[Numerical 22 / 28] Processing numerical column DST_TO_SRC_SECOND_BYTES...\n",
      "[Numerical 23 / 28] Processing numerical column DURATION_OUT...\n",
      "[Numerical 24 / 28] Processing numerical column OUT_PKTS...\n",
      "[Numerical 25 / 28] Processing numerical column TCP_WIN_MAX_OUT...\n",
      "[Numerical 26 / 28] Processing numerical column MAX_TTL...\n",
      "[Numerical 27 / 28] Processing numerical column DURATION_IN...\n",
      "[Numerical 28 / 28] Processing numerical column TCP_WIN_MAX_IN...\n",
      "Applying pre-processing to categorical values\n",
      "[Categorical 1 / 9] Processing categorical column CLIENT_TCP_FLAGS...\n",
      "Encoding the 32 levels for CLIENT_TCP_FLAGS\n",
      "[Categorical 2 / 9] Processing categorical column L4_SRC_PORT...\n",
      "Encoding the 32 levels for L4_SRC_PORT\n",
      "[Categorical 3 / 9] Processing categorical column TCP_FLAGS...\n",
      "Encoding the 32 levels for TCP_FLAGS\n",
      "[Categorical 4 / 9] Processing categorical column ICMP_IPV4_TYPE...\n",
      "Encoding the 32 levels for ICMP_IPV4_TYPE\n",
      "[Categorical 5 / 9] Processing categorical column ICMP_TYPE...\n",
      "Encoding the 32 levels for ICMP_TYPE\n",
      "[Categorical 6 / 9] Processing categorical column PROTOCOL...\n",
      "Encoding the 6 levels for PROTOCOL\n",
      "[Categorical 7 / 9] Processing categorical column SERVER_TCP_FLAGS...\n",
      "Encoding the 32 levels for SERVER_TCP_FLAGS\n",
      "[Categorical 8 / 9] Processing categorical column L4_DST_PORT...\n",
      "Encoding the 32 levels for L4_DST_PORT\n",
      "[Categorical 9 / 9] Processing categorical column L7_PROTO...\n",
      "Encoding the 32 levels for L7_PROTO\n",
      "Generating pre-processed dataframe...\n",
      "Input data frame had shape (4558141,46), output data frame has shape (4558141,292) after pre-processing...\n",
      "Writing to cache file path: cache\\CSE_CIC_IDS_0_QdLmZHuh8yOmlGcKBEkf7hepImY0_VHNk9ujbqtTXGSrgVayeqG486IQ0.feather...\n",
      "Loaded dataset, building model\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_DST_TO_SRC_AVG_THROU  [(None, 8, 1)]               0         []                            \n",
      " GHPUT (InputLayer)                                                                               \n",
      "                                                                                                  \n",
      " input_OUT_BYTES (InputLaye  [(None, 8, 1)]               0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " input_RETRANSMITTED_OUT_PK  [(None, 8, 1)]               0         []                            \n",
      " TS (InputLayer)                                                                                  \n",
      "                                                                                                  \n",
      " input_MIN_TTL (InputLayer)  [(None, 8, 1)]               0         []                            \n",
      "                                                                                                  \n",
      " input_SRC_TO_DST_AVG_THROU  [(None, 8, 1)]               0         []                            \n",
      " GHPUT (InputLayer)                                                                               \n",
      "                                                                                                  \n",
      " input_MAX_IP_PKT_LEN (Inpu  [(None, 8, 1)]               0         []                            \n",
      " tLayer)                                                                                          \n",
      "                                                                                                  \n",
      " input_RETRANSMITTED_IN_PKT  [(None, 8, 1)]               0         []                            \n",
      " S (InputLayer)                                                                                   \n",
      "                                                                                                  \n",
      " input_RETRANSMITTED_IN_BYT  [(None, 8, 1)]               0         []                            \n",
      " ES (InputLayer)                                                                                  \n",
      "                                                                                                  \n",
      " input_NUM_PKTS_512_TO_1024  [(None, 8, 1)]               0         []                            \n",
      " _BYTES (InputLayer)                                                                              \n",
      "                                                                                                  \n",
      " input_NUM_PKTS_UP_TO_128_B  [(None, 8, 1)]               0         []                            \n",
      " YTES (InputLayer)                                                                                \n",
      "                                                                                                  \n",
      " input_FLOW_DURATION_MILLIS  [(None, 8, 1)]               0         []                            \n",
      " ECONDS (InputLayer)                                                                              \n",
      "                                                                                                  \n",
      " input_SHORTEST_FLOW_PKT (I  [(None, 8, 1)]               0         []                            \n",
      " nputLayer)                                                                                       \n",
      "                                                                                                  \n",
      " input_MIN_IP_PKT_LEN (Inpu  [(None, 8, 1)]               0         []                            \n",
      " tLayer)                                                                                          \n",
      "                                                                                                  \n",
      " input_RETRANSMITTED_OUT_BY  [(None, 8, 1)]               0         []                            \n",
      " TES (InputLayer)                                                                                 \n",
      "                                                                                                  \n",
      " input_IN_BYTES (InputLayer  [(None, 8, 1)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_IN_PKTS (InputLayer)  [(None, 8, 1)]               0         []                            \n",
      "                                                                                                  \n",
      " input_NUM_PKTS_1024_TO_151  [(None, 8, 1)]               0         []                            \n",
      " 4_BYTES (InputLayer)                                                                             \n",
      "                                                                                                  \n",
      " input_NUM_PKTS_128_TO_256_  [(None, 8, 1)]               0         []                            \n",
      " BYTES (InputLayer)                                                                               \n",
      "                                                                                                  \n",
      " input_LONGEST_FLOW_PKT (In  [(None, 8, 1)]               0         []                            \n",
      " putLayer)                                                                                        \n",
      "                                                                                                  \n",
      " input_NUM_PKTS_256_TO_512_  [(None, 8, 1)]               0         []                            \n",
      " BYTES (InputLayer)                                                                               \n",
      "                                                                                                  \n",
      " input_SRC_TO_DST_SECOND_BY  [(None, 8, 1)]               0         []                            \n",
      " TES (InputLayer)                                                                                 \n",
      "                                                                                                  \n",
      " input_DST_TO_SRC_SECOND_BY  [(None, 8, 1)]               0         []                            \n",
      " TES (InputLayer)                                                                                 \n",
      "                                                                                                  \n",
      " input_DURATION_OUT (InputL  [(None, 8, 1)]               0         []                            \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " input_OUT_PKTS (InputLayer  [(None, 8, 1)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_TCP_WIN_MAX_OUT (Inp  [(None, 8, 1)]               0         []                            \n",
      " utLayer)                                                                                         \n",
      "                                                                                                  \n",
      " input_MAX_TTL (InputLayer)  [(None, 8, 1)]               0         []                            \n",
      "                                                                                                  \n",
      " input_DURATION_IN (InputLa  [(None, 8, 1)]               0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " input_TCP_WIN_MAX_IN (Inpu  [(None, 8, 1)]               0         []                            \n",
      " tLayer)                                                                                          \n",
      "                                                                                                  \n",
      " input_CLIENT_TCP_FLAGS (In  [(None, 8, 32)]              0         []                            \n",
      " putLayer)                                                                                        \n",
      "                                                                                                  \n",
      " input_L4_SRC_PORT (InputLa  [(None, 8, 32)]              0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " input_TCP_FLAGS (InputLaye  [(None, 8, 32)]              0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " input_ICMP_IPV4_TYPE (Inpu  [(None, 8, 32)]              0         []                            \n",
      " tLayer)                                                                                          \n",
      "                                                                                                  \n",
      " input_ICMP_TYPE (InputLaye  [(None, 8, 32)]              0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " input_PROTOCOL (InputLayer  [(None, 8, 6)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_SERVER_TCP_FLAGS (In  [(None, 8, 32)]              0         []                            \n",
      " putLayer)                                                                                        \n",
      "                                                                                                  \n",
      " input_L4_DST_PORT (InputLa  [(None, 8, 32)]              0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " input_L7_PROTO (InputLayer  [(None, 8, 32)]              0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " feature_concat (Concatenat  (None, 8, 290)               0         ['input_DST_TO_SRC_AVG_THROUGH\n",
      " e)                                                                 PUT[0][0]',                   \n",
      "                                                                     'input_OUT_BYTES[0][0]',     \n",
      "                                                                     'input_RETRANSMITTED_OUT_PKTS\n",
      "                                                                    [0][0]',                      \n",
      "                                                                     'input_MIN_TTL[0][0]',       \n",
      "                                                                     'input_SRC_TO_DST_AVG_THROUGH\n",
      "                                                                    PUT[0][0]',                   \n",
      "                                                                     'input_MAX_IP_PKT_LEN[0][0]',\n",
      "                                                                     'input_RETRANSMITTED_IN_PKTS[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'input_RETRANSMITTED_IN_BYTES\n",
      "                                                                    [0][0]',                      \n",
      "                                                                     'input_NUM_PKTS_512_TO_1024_B\n",
      "                                                                    YTES[0][0]',                  \n",
      "                                                                     'input_NUM_PKTS_UP_TO_128_BYT\n",
      "                                                                    ES[0][0]',                    \n",
      "                                                                     'input_FLOW_DURATION_MILLISEC\n",
      "                                                                    ONDS[0][0]',                  \n",
      "                                                                     'input_SHORTEST_FLOW_PKT[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'input_MIN_IP_PKT_LEN[0][0]',\n",
      "                                                                     'input_RETRANSMITTED_OUT_BYTE\n",
      "                                                                    S[0][0]',                     \n",
      "                                                                     'input_IN_BYTES[0][0]',      \n",
      "                                                                     'input_IN_PKTS[0][0]',       \n",
      "                                                                     'input_NUM_PKTS_1024_TO_1514_\n",
      "                                                                    BYTES[0][0]',                 \n",
      "                                                                     'input_NUM_PKTS_128_TO_256_BY\n",
      "                                                                    TES[0][0]',                   \n",
      "                                                                     'input_LONGEST_FLOW_PKT[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'input_NUM_PKTS_256_TO_512_BY\n",
      "                                                                    TES[0][0]',                   \n",
      "                                                                     'input_SRC_TO_DST_SECOND_BYTE\n",
      "                                                                    S[0][0]',                     \n",
      "                                                                     'input_DST_TO_SRC_SECOND_BYTE\n",
      "                                                                    S[0][0]',                     \n",
      "                                                                     'input_DURATION_OUT[0][0]',  \n",
      "                                                                     'input_OUT_PKTS[0][0]',      \n",
      "                                                                     'input_TCP_WIN_MAX_OUT[0][0]'\n",
      "                                                                    , 'input_MAX_TTL[0][0]',      \n",
      "                                                                     'input_DURATION_IN[0][0]',   \n",
      "                                                                     'input_TCP_WIN_MAX_IN[0][0]',\n",
      "                                                                     'input_CLIENT_TCP_FLAGS[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'input_L4_SRC_PORT[0][0]',   \n",
      "                                                                     'input_TCP_FLAGS[0][0]',     \n",
      "                                                                     'input_ICMP_IPV4_TYPE[0][0]',\n",
      "                                                                     'input_ICMP_TYPE[0][0]',     \n",
      "                                                                     'input_PROTOCOL[0][0]',      \n",
      "                                                                     'input_SERVER_TCP_FLAGS[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'input_L4_DST_PORT[0][0]',   \n",
      "                                                                     'input_L7_PROTO[0][0]']      \n",
      "                                                                                                  \n",
      " embed (Dense)               (None, 8, 64)                18624     ['feature_concat[0][0]']      \n",
      "                                                                                                  \n",
      " block_0_transformer_encode  (None, 8, 64)                83200     ['embed[0][0]']               \n",
      " r (TransformerEncoderBlock                                                                       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_1_transformer_encode  (None, 8, 64)                83200     ['block_0_transformer_encoder[\n",
      " r (TransformerEncoderBlock                                         0][0]']                       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " featurewise_embed (Dense)   (None, 8, 1)                 65        ['block_1_transformer_encoder[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 8)                    0         ['featurewise_embed[0][0]']   \n",
      "                                                                                                  \n",
      " classification_mlp_0_128 (  (None, 128)                  1152      ['flatten_1[0][0]']           \n",
      " Dense)                                                                                           \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 128)                  0         ['classification_mlp_0_128[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " binary_classification_out   (None, 1)                    129       ['dropout_2[0][0]']           \n",
      " (Dense)                                                                                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 186370 (728.01 KB)\n",
      "Trainable params: 186370 (728.01 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Building eval dataset...\n",
      "Splitting dataset to featurewise...\n",
      "Evaluation dataset is built!\n",
      "Positive samples in eval set: 22433\n",
      "Negative samples in eval set: 23148\n",
      "Epoch = 0 / 5 (early stop in 5), step = 0, loss = 0.70709, results = [0.7070915699005127, 0.484375] -- elapsed (train): 0.00s\n",
      "Epoch = 0 / 5 (early stop in 5), step = 16, loss = 0.66054, results = [0.6605372428894043, 0.671875] -- elapsed (train): 0.95s\n",
      "Epoch = 0 / 5 (early stop in 5), step = 32, loss = 0.31803, results = [0.318034291267395, 0.9140625] -- elapsed (train): 1.84s\n",
      "Epoch = 0 / 5 (early stop in 5), step = 48, loss = 0.13423, results = [0.13422754406929016, 0.96875] -- elapsed (train): 2.73s\n",
      "Epoch = 1 / 5 (early stop in 5), step = 2, loss = 0.21291, results = [0.21291331946849823, 0.921875] -- elapsed (train): 3.71s\n",
      "Epoch = 1 / 5 (early stop in 5), step = 20, loss = 0.13131, results = [0.13130658864974976, 0.96875] -- elapsed (train): 4.67s\n",
      "Epoch = 1 / 5 (early stop in 5), step = 38, loss = 0.20269, results = [0.20268657803535461, 0.953125] -- elapsed (train): 5.65s\n",
      "Epoch = 1 / 5 (early stop in 5), step = 56, loss = 0.23352, results = [0.2335207760334015, 0.9453125] -- elapsed (train): 6.62s\n",
      "Epoch = 2 / 5 (early stop in 5), step = 10, loss = 0.15593, results = [0.1559307873249054, 0.9609375] -- elapsed (train): 7.58s\n",
      "Epoch = 2 / 5 (early stop in 5), step = 28, loss = 0.10131, results = [0.10131224989891052, 0.9765625] -- elapsed (train): 8.54s\n",
      "Epoch = 2 / 5 (early stop in 5), step = 46, loss = 0.18058, results = [0.180583655834198, 0.953125] -- elapsed (train): 9.51s\n",
      "Epoch = 3 / 5 (early stop in 5), step = 0, loss = 0.09329, results = [0.0932854488492012, 0.96875] -- elapsed (train): 10.48s\n",
      "Epoch = 3 / 5 (early stop in 5), step = 18, loss = 0.13713, results = [0.1371273398399353, 0.96875] -- elapsed (train): 11.46s\n",
      "Epoch = 3 / 5 (early stop in 5), step = 36, loss = 0.04603, results = [0.04603397846221924, 0.9921875] -- elapsed (train): 12.44s\n",
      "Epoch = 3 / 5 (early stop in 5), step = 54, loss = 0.06013, results = [0.06012634560465813, 0.984375] -- elapsed (train): 13.41s\n",
      "Epoch = 4 / 5 (early stop in 4), step = 8, loss = 0.13588, results = [0.13588348031044006, 0.96875] -- elapsed (train): 14.36s\n",
      "Epoch = 4 / 5 (early stop in 4), step = 26, loss = 0.04126, results = [0.04125514626502991, 0.9921875] -- elapsed (train): 15.33s\n",
      "Epoch = 4 / 5 (early stop in 4), step = 44, loss = 0.07996, results = [0.07995526492595673, 0.9765625] -- elapsed (train): 16.28s\n",
      "Epoch = 4 / 5 (early stop in 4), step = 62, loss = 0.03304, results = [0.033043622970581055, 0.9921875] -- elapsed (train): 17.23s\n",
      "1425/1425 [==============================] - 7s 4ms/step\n",
      "Epoch 4 yielded predictions: (45581,), overall balanced accuracy: 97.38%, TP = 21,315 / 22,433, TN = 23,088 / 23,148\n",
      "   epoch      P      N  pred_P  pred_N     TP  FP     TN    FN   bal_acc  \\\n",
      "0      4  22433  23148   21375   24206  21315  60  23088  1118  0.973785   \n",
      "\n",
      "        f1  \n",
      "0  0.97311  \n"
     ]
    }
   ],
   "source": [
    "encodings = [\n",
    "    RecordLevelEmbed(64),\n",
    "    RecordLevelEmbed(64, project=True)\n",
    "]\n",
    "\n",
    "classification_heads = [\n",
    "    FeaturewiseEmbedding(project=False),\n",
    "    LastTokenClassificationHead()\n",
    "]\n",
    "\n",
    "transformers: List[FunctionalComponent] = [\n",
    "    BasicTransformer(2, 128, n_heads=2),\n",
    "    GPTSmallTransformer()\n",
    "]\n",
    "\n",
    "flow_file_path = r\"dataset/\"\n",
    "\n",
    "datasets = [\n",
    "    (\"CSE_CIC_IDS\", os.path.join(flow_file_path, \"downsampled/downsampled_df_shuffled.csv\"), NamedDatasetSpecifications.cse_cic_ids_2018, 0.01, EvaluationDatasetSampling.RandomRows)\n",
    "]\n",
    "\n",
    "print(\"Imported and defined\")\n",
    "pre_processing = StandardPreProcessing(n_categorical_levels=32)\n",
    "\n",
    "# Define the transformer\n",
    "ft = FlowTransformer(pre_processing=pre_processing,\n",
    "                     input_encoding=encodings[0],\n",
    "                     sequential_model=transformers[0],\n",
    "                     classification_head=classification_heads[0],\n",
    "                     params=FlowTransformerParameters(window_size=8, mlp_layer_sizes=[128], mlp_dropout=0.1))\n",
    "\n",
    "print(\"Defined flowtransformer, starting load\")\n",
    "\n",
    "# Load the specific dataset\n",
    "dataset_name, dataset_path, dataset_specification, eval_percent, eval_method = datasets[0]\n",
    "ft.load_dataset(dataset_name, dataset_path, dataset_specification, evaluation_dataset_sampling=eval_method, evaluation_percent=eval_percent)\n",
    "\n",
    "print(\"Loaded dataset, building model\")\n",
    "\n",
    "# Build the transformer model\n",
    "m = ft.build_model()\n",
    "m.summary()\n",
    "\n",
    "# Compile the model\n",
    "m.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['binary_accuracy'], jit_compile=True)\n",
    "\n",
    "# Get the evaluation results\n",
    "eval_results: pd.DataFrame\n",
    "(train_results, eval_results, final_epoch) = ft.evaluate(m, batch_size=128, epochs=5, steps_per_epoch=64, early_stopping_patience=5)\n",
    "\n",
    "\n",
    "print(eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
