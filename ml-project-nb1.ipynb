{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['TF_CUDA_PATHS'] = \"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.3\"\n",
    "# os.environ['PATH'] += \";C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.3/bin;C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.3/libnvvp\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports done\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import time\n",
    "from enum import Enum\n",
    "from typing import Optional, Tuple, List, Union\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "try:\n",
    "    from tensorflow._api.v2.v2 import keras\n",
    "except ImportError:\n",
    "    from tensorflow import keras\n",
    "\n",
    "from keras import Input, Model\n",
    "import keras.layers as layers\n",
    "from keras.layers import Dense, Conv1D, Layer, MultiHeadAttention, Dropout, LayerNormalization, Embedding, Concatenate, Reshape, Lambda, Flatten, GlobalAveragePooling1D\n",
    "\n",
    "print(\"Imports done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import hashlib\n",
    "import json\n",
    "import pickle\n",
    "from typing import Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_identifier(d:dict):\n",
    "    raw_json = json.dumps(d, sort_keys=True, indent=False)\n",
    "    hash = hashlib.sha1(raw_json.encode(\"utf8\")).digest()\n",
    "    x = base64.b64encode(hash)\n",
    "    x = x.decode(\"ASCII\")\n",
    "    x = x.replace(\"+\", \"0\").replace(\"=\", \"0\").replace(\"/\", \"0\")\n",
    "    return x\n",
    "\n",
    "def save_feather_plus_metadata(save_path:str, df:pd.DataFrame, metadata:object):\n",
    "    metadata_path = save_path + \".metadata.pickle\"\n",
    "    df.to_feather(save_path)\n",
    "    with open(metadata_path, \"wb\") as w:\n",
    "        pickle.dump(metadata, w)\n",
    "\n",
    "def save_pickle(save_path:str, obj:dict):\n",
    "    with open(save_path, \"wb\") as w:\n",
    "        pickle.dump(obj, w)\n",
    "\n",
    "def load_pickle(save_path:str):\n",
    "    with open(save_path, \"rb\") as r:\n",
    "        return pickle.load(r, fix_imports=True)\n",
    "\n",
    "def load_feather_plus_metadata(load_path:str) -> Tuple[pd.DataFrame, object]:\n",
    "    metadata_path = load_path + \".metadata.pickle\"\n",
    "    with open(metadata_path, \"rb\") as r:\n",
    "        metadata = pickle.load(r, fix_imports=True)\n",
    "    data = pd.read_feather(load_path)\n",
    "    return data, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dataset Specifications: List some metadata, including column names for ease of use**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DatasetSpecification:\n",
    "    def  __init__(self, include_fields:List[str], categorical_fields:List[str], class_column:str, benign_label:str, test_column:Optional[str]=None):\n",
    "        \"\"\"\n",
    "        Defines the format of specific NIDS dataset\n",
    "        :param include_fields: The fields to include as part of classification\n",
    "        :param categorical_fields: Fields that should be treated as categorical\n",
    "        :param class_column: The column name that includes the class of the flow, eg. DDoS or Benign\n",
    "        :param benign_label: The label of benign traffic, eg. Benign or 0\n",
    "        :param test_column: The column indicating if this row is a member of the test or training dataset\n",
    "        \"\"\"\n",
    "        self.include_fields:List[str] = include_fields\n",
    "        self.categorical_fields:List[str] = categorical_fields\n",
    "        self.class_column = class_column\n",
    "        self.benign_label = benign_label\n",
    "        self.test_column:Optional[str] = test_column\n",
    "\n",
    "class NamedDatasetSpecifications:\n",
    "    \"\"\"\n",
    "    Example specifications of some common datasets\n",
    "    \"\"\"\n",
    "    cse_cic_ids_2018_improved = DatasetSpecification(\n",
    "        include_fields=['Src Port', 'Dst Port', 'Protocol', 'Flow Duration', 'Total Fwd Packet', 'Total Bwd packets', 'Total Length of Fwd Packet', 'Total Length of Bwd Packet', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd RST Flags', 'Bwd RST Flags', 'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Packet Length Min', 'Packet Length Max', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'CWR Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Fwd Segment Size Avg', 'Bwd Segment Size Avg', 'Fwd Bytes/Bulk Avg', 'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg', 'Bwd Bytes/Bulk Avg', 'Bwd Packet/Bulk Avg', 'Bwd Bulk Rate Avg', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'FWD Init Win Bytes', 'Bwd Init Win Bytes', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'ICMP Code', 'ICMP Type', 'Total TCP Flow Time'],\n",
    "        categorical_fields=['Src Port', 'Dst Port', 'Protocol', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd RST Flags', 'Bwd RST Flags', 'ICMP Code', 'ICMP Type'],\n",
    "        class_column=\"Label\",\n",
    "        benign_label=\"BENIGN\"\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Some helper classes**\n",
    "\n",
    "these are useful in, including but not limited to: dataset column datatypes, main class parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CategoricalFormat(Enum):\n",
    "    \"\"\"\n",
    "    The format of variables expected by the model as input\n",
    "    \"\"\"\n",
    "    Integers = 0,\n",
    "    \"\"\"\n",
    "    If categorical values should be dictionary encoded as integers\n",
    "    \"\"\"\n",
    "    OneHot = 1\n",
    "    \"\"\"\n",
    "    If categorical values should be one-hot encoded\n",
    "    \"\"\"\n",
    "\n",
    "class EvaluationDatasetSampling(Enum):\n",
    "    \"\"\"\n",
    "    How to choose evaluation samples from the raw dataset\n",
    "    \"\"\"\n",
    "    LastRows = 0\n",
    "    \"\"\"\n",
    "    Take the last rows in the dataset to form the evaluation dataset\n",
    "    \"\"\"\n",
    "    RandomRows  = 1\n",
    "    \"\"\"\n",
    "    Randomly sample rows to make up the evaluation dataset\n",
    "    \"\"\"\n",
    "    FilterColumn = 2\n",
    "    \"\"\"\n",
    "    Define a column that contains a flag indicating if this row is part of the evaluation set\n",
    "    \"\"\"\n",
    "\n",
    "class FlowTransformerParameters:\n",
    "    \"\"\"\n",
    "    Allows the configuration of overall parameters of the FlowTransformer\n",
    "    :param window_size: The number of flows to use in each window\n",
    "    :param mlp_layer_sizes: The number of nodes in each layer of the outer classification MLP of FlowTransformer\n",
    "    :param mlp_dropout: The amount of dropout to be applied between the layers of the outer classification MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, window_size:int, mlp_layer_sizes:List[int], mlp_dropout:float=0.1):\n",
    "        self.window_size:int = window_size\n",
    "        self.mlp_layer_sizes = mlp_layer_sizes\n",
    "        self.mlp_dropout = mlp_dropout\n",
    "\n",
    "        # Is the order of flows important within any individual window\n",
    "        self._train_ensure_flows_are_ordered_within_windows = True\n",
    "\n",
    "        # Should windows be sampled sequentially during training\n",
    "        self._train_draw_sequential_windows = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Input Enum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ModelInputSpecification:\n",
    "    def __init__(self, feature_names:List[str], n_numeric_features:int, levels_per_categorical_feature:List[int], categorical_format:CategoricalFormat):\n",
    "        self.feature_names = feature_names\n",
    "\n",
    "        self.numeric_feature_names = feature_names[:n_numeric_features]\n",
    "        self.categorical_feature_names = feature_names[n_numeric_features:]\n",
    "        self.categorical_format:CategoricalFormat = categorical_format\n",
    "\n",
    "        self.n_numeric_features = n_numeric_features\n",
    "        self.levels_per_categorical_feature = levels_per_categorical_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Framework Component class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Component():\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def parameters(self) -> dict:\n",
    "        warnings.warn(\"Parameters have not been implemented for this class!\")\n",
    "        return {}\n",
    "class FunctionalComponent(Component):\n",
    "    def __init__(self):\n",
    "        self.sequence_length: Optional[int] = None\n",
    "        self.model_input_specification: Optional[ModelInputSpecification] = None\n",
    "        self.input_shape: Optional[Tuple[int]] = None\n",
    "\n",
    "    def apply(self, X, prefix: str = None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def build(self, sequence_length:int, model_input_specification:ModelInputSpecification):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.model_input_specification = model_input_specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Base Classification Head**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BaseClassificationHead(FunctionalComponent):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def apply_before_transformer(self, X, prefix:str=None):\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Classification Head: Last token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LastTokenClassificationHead(BaseClassificationHead):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def apply(self, X, prefix: str = None):\n",
    "        if prefix is None:\n",
    "            prefix = \"\"\n",
    "\n",
    "        x = Lambda(lambda x: x[..., -1, :], name=f\"{prefix}slice_last\")(X)\n",
    "        #x = Flatten(name=f\"{prefix}flatten_last\")(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"Last Token\"\n",
    "\n",
    "    @property\n",
    "    def parameters(self) -> dict:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Input Encoding:Record Level Projection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BaseInputEncoding(FunctionalComponent):\n",
    "    def apply(self, X:List[\"keras.Input\"], prefix: str = None):\n",
    "        raise NotImplementedError(\"Please override this with a custom implementation\")\n",
    "\n",
    "    @property\n",
    "    def required_input_format(self) -> CategoricalFormat:\n",
    "        raise NotImplementedError(\"Please override this with a custom implementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EmbedLayerType(Enum):\n",
    "    Dense = 0,\n",
    "    Lookup = 1,\n",
    "    Projection = 2\n",
    "\n",
    "class RecordLevelEmbed(BaseInputEncoding):\n",
    "    def __init__(self, embed_dimension: int, project:bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dimension: int = embed_dimension\n",
    "        self.project: bool = project\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        if self.project:\n",
    "            return \"Record Level Projection\"\n",
    "        return \"Record Level Embedding\"\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return {\n",
    "            \"dimensions_per_feature\": self.embed_dimension\n",
    "        }\n",
    "\n",
    "    def apply(self, X:List[keras.Input], prefix: str = None):\n",
    "        if prefix is None:\n",
    "            prefix = \"\"\n",
    "\n",
    "        assert self.model_input_specification.categorical_format == CategoricalFormat.OneHot\n",
    "\n",
    "        x = Concatenate(name=f\"{prefix}feature_concat\", axis=-1)(X)\n",
    "        x = Dense(self.embed_dimension, activation=\"linear\", use_bias=not self.project, name=f\"{prefix}embed\")(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def required_input_format(self) -> CategoricalFormat:\n",
    "        return CategoricalFormat.OneHot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Input Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BasePreProcessing(Component):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit_numerical(self, column_name:str, values:np.array):\n",
    "        raise NotImplementedError(\"Please override this base class with a custom implementation\")\n",
    "\n",
    "    def transform_numerical(self, column_name:str, values: np.array):\n",
    "        raise NotImplementedError(\"Please override this base class with a custom implementation\")\n",
    "\n",
    "    def fit_categorical(self, column_name:str, values:np.array):\n",
    "        raise NotImplementedError(\"Please override this base class with a custom implementation\")\n",
    "\n",
    "    def transform_categorical(self, column_name:str, values:np.array, expected_categorical_format:CategoricalFormat):\n",
    "        raise NotImplementedError(\"Please override this base class with a custom implementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class StandardPreProcessing(BasePreProcessing):\n",
    "    def __init__(self, n_categorical_levels: int, clip_numerical_values:bool=False):\n",
    "        super().__init__()\n",
    "        self.n_categorical_levels:int = n_categorical_levels\n",
    "        self.clip_numerical_values:bool = clip_numerical_values\n",
    "        self.min_range = {}\n",
    "        self.encoded_levels = {}\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"Standard Preprocessing\"\n",
    "\n",
    "    @property\n",
    "    def parameters(self) -> dict:\n",
    "        return {\n",
    "            \"n_categorical_levels\": self.n_categorical_levels,\n",
    "            \"clip_numerical_values\": self.clip_numerical_values\n",
    "        }\n",
    "\n",
    "    def fit_numerical(self, column_name: str, values: np.array):\n",
    "\n",
    "        v0 = np.min(values)\n",
    "        v1 = np.max(values)\n",
    "        r = v1 - v0\n",
    "\n",
    "        self.min_range[column_name] = (v0, r)\n",
    "\n",
    "    def transform_numerical(self, column_name: str, values: np.array):\n",
    "        col_min, col_range = self.min_range[column_name]\n",
    "\n",
    "        if col_range == 0:\n",
    "            return np.zeros_like(values, dtype=\"float32\")\n",
    "\n",
    "        # center on zero\n",
    "        values -= col_min\n",
    "\n",
    "        # apply a logarithm\n",
    "        col_values = np.log(values + 1)\n",
    "\n",
    "        # scale max to 1\n",
    "        col_values *= 1. / np.log(col_range + 1)\n",
    "\n",
    "        if self.clip_numerical_values:\n",
    "            col_values = np.clip(col_values, 0., 1.)\n",
    "\n",
    "        return col_values\n",
    "\n",
    "    def fit_categorical(self, column_name: str, values: np.array):\n",
    "        levels, level_counts = np.unique(values, return_counts=True)\n",
    "        sorted_levels = list(sorted(zip(levels, level_counts), key=lambda x: x[1], reverse=True))\n",
    "        self.encoded_levels[column_name] = [s[0] for s in sorted_levels[:self.n_categorical_levels]]\n",
    "\n",
    "\n",
    "    def transform_categorical(self, column_name:str, values: np.array, expected_categorical_format: CategoricalFormat):\n",
    "        encoded_levels = self.encoded_levels[column_name]\n",
    "        print(f\"Encoding the {len(encoded_levels)} levels for {column_name}\")\n",
    "\n",
    "        result_values = np.ones(len(values), dtype=\"uint32\")\n",
    "        for level_i, level in enumerate(encoded_levels):\n",
    "            level_mask = values == level\n",
    "\n",
    "            # we use +1 here, as 0 = previously unseen, and 1 to (n + 1) are the encoded levels\n",
    "            result_values[level_mask] = level_i + 1\n",
    "\n",
    "        if expected_categorical_format == CategoricalFormat.Integers:\n",
    "            return result_values\n",
    "\n",
    "        v = pd.get_dummies(result_values, prefix=column_name)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Transformer classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BaseSequential(FunctionalComponent):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(Layer):\n",
    "    def __init__(self, input_dimension:int, inner_dimension:int, num_heads:int, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.input_dimension = input_dimension\n",
    "        self.inner_dimension = inner_dimension\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=input_dimension)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(inner_dimension, activation='relu'),\n",
    "            Dense(input_dimension)\n",
    "        ])\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    # noinspection PyMethodOverriding\n",
    "    def call(self, inputs, training, mask=None):\n",
    "        # inputs = (target_seq, enc_output)\n",
    "        target_seq = inputs\n",
    "        enc_output = inputs\n",
    "\n",
    "        # self attention of target_seq\n",
    "        attn_output = self.mha(target_seq, target_seq)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = target_seq + attn_output\n",
    "        out1 = self.layernorm1(out1)\n",
    "\n",
    "        # multi-head attention with encoder output as the key and value, and target_seq as the query\n",
    "        attn_output = self.mha(out1, enc_output)\n",
    "        attn_output = self.dropout2(attn_output, training=training)\n",
    "        out2 = out1 + attn_output\n",
    "        out2 = self.layernorm2(out2)\n",
    "\n",
    "        # feed forward network\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out3 = out2 + ffn_output\n",
    "        out3 = self.layernorm2(out3)\n",
    "\n",
    "        return out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GPT3Attention(layers.Layer):\n",
    "    def __init__(self, n_heads, d_model, dropout_rate=0.1):\n",
    "        super(GPT3Attention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // n_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.n_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # noinspection PyMethodOverriding\n",
    "    def call(self, q, k, v, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        scaled_attention_logits = tf.matmul(q, k, transpose_b=True)\n",
    "        scaled_attention_logits = scaled_attention_logits / tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
    "        output = tf.reshape(output, (batch_size, -1, self.d_model))\n",
    "\n",
    "        output = self.dense(output)\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class MultiHeadAttentionImplementation:\n",
    "    Keras = 0,\n",
    "    GPT3 = 1\n",
    "\n",
    "class TransformerEncoderBlock(layers.Layer):\n",
    "    def __init__(self, input_dimension:int, inner_dimension:int, num_heads:int, dropout_rate=0.1, use_conv:bool=False, prefix:str=None, attn_implementation:MultiHeadAttentionImplementation = MultiHeadAttentionImplementation.Keras):\n",
    "\n",
    "        if prefix is None:\n",
    "            prefix = \"\"\n",
    "\n",
    "        super().__init__(name=f\"{prefix}transformer_encoder\")\n",
    "\n",
    "        if inner_dimension < input_dimension:\n",
    "            warnings.warn(f\"Typically inner_dimension should be greater than or equal to the input_dimension!\")\n",
    "\n",
    "        self.attn_implementation = attn_implementation\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.attention = \\\n",
    "            layers.MultiHeadAttention(num_heads=num_heads, key_dim=inner_dimension, name=f\"{prefix}multi_head_attn\") \\\n",
    "                if attn_implementation == MultiHeadAttentionImplementation.Keras else\\\n",
    "                GPT3Attention(num_heads, inner_dimension, dropout_rate=0.0)\n",
    "\n",
    "        layer_norm = 1e-6\n",
    "\n",
    "        self.attention_dropout = layers.Dropout(dropout_rate, name=f\"{prefix}attention_dropout\")\n",
    "        self.attention_layer_norm = layers.LayerNormalization(epsilon=layer_norm, name=f\"{prefix}attention_layer_norm\")\n",
    "\n",
    "        self.feed_forward_0 = Conv1D(filters=inner_dimension, kernel_size=1, activation=\"relu\", name=f\"{prefix}feed_forward_0\") \\\n",
    "            if use_conv else Dense(inner_dimension, activation=\"relu\", name=f\"{prefix}feed_forward_0\")\n",
    "        self.feed_forward_1 = Conv1D(filters=input_dimension, kernel_size=1, activation=\"relu\", name=f\"{prefix}feed_forward_1\") \\\n",
    "            if use_conv else Dense(input_dimension, activation=\"relu\", name=f\"{prefix}feed_forward_1\")\n",
    "\n",
    "        self.feed_forward_dropout = layers.Dropout(dropout_rate, name=f\"{prefix}feed_forward_dropout\")\n",
    "        self.feed_forward_layer_norm = layers.LayerNormalization(epsilon=layer_norm, name=f\"{prefix}feed_forward_layer_norm\")\n",
    "\n",
    "    # noinspection PyMethodOverriding\n",
    "    def call(self, inputs, training, mask=None):\n",
    "        x = inputs\n",
    "        x = self.attention(x, x) if self.attn_implementation == MultiHeadAttentionImplementation.Keras else self.attention(x, x, x, mask)\n",
    "\n",
    "        attention_output = self.attention_dropout(x, training=training) if self.dropout_rate > 0 else x\n",
    "\n",
    "        x = inputs + attention_output\n",
    "        x = self.attention_layer_norm(x)\n",
    "        x = self.feed_forward_0(x)\n",
    "        x = self.feed_forward_1(x)\n",
    "        x = self.feed_forward_dropout(x, training=training) if self.dropout_rate > 0 else x\n",
    "        feed_forward_output = x\n",
    "\n",
    "        return self.feed_forward_layer_norm(attention_output + feed_forward_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BasicTransformer(BaseSequential):\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        if self.use_conv:\n",
    "            return f\"Basic Conv Transformer\" + (\" Decoder\" if self.is_decoder else \"\")\n",
    "        else:\n",
    "            return f\"Basic Dense Transformer\" + (\" Decoder\" if self.is_decoder else \"\")\n",
    "\n",
    "    @property\n",
    "    def parameters(self) -> dict:\n",
    "        return {\n",
    "            \"n_layers\": self.n_layers,\n",
    "            \"internal_size\": self.internal_size,\n",
    "            \"use_conv\": self.use_conv,\n",
    "            \"n_heads\": self.n_heads,\n",
    "            \"dropout_rate\": self.dropout_rate,\n",
    "            \"head_size\": self.internal_size\n",
    "        }\n",
    "\n",
    "    def __init__(self, n_layers:int, internal_size:int, n_heads:int, use_conv:bool=False, dropout_rate:float=0.1, is_decoder=False):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.internal_size = internal_size\n",
    "        self.use_conv = use_conv\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "    def apply(self, X, prefix: str = None):\n",
    "        #window_size = self.sequence_length\n",
    "        real_size = X.shape[-1]\n",
    "\n",
    "        m_x = X\n",
    "\n",
    "        for layer_i in range(self.n_layers):\n",
    "            if self.is_decoder:\n",
    "                if self.use_conv:\n",
    "                    raise NotImplementedError()\n",
    "                m_x = TransformerDecoderBlock(real_size, self.internal_size, self.n_heads, dropout_rate=self.dropout_rate)(m_x)\n",
    "            else:\n",
    "                m_x = TransformerEncoderBlock(real_size, self.internal_size, self.n_heads, dropout_rate=self.dropout_rate, use_conv=self.use_conv, prefix=f\"{prefix}block_{layer_i}_\")(m_x)\n",
    "\n",
    "        return m_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GPTSmallTransformer(BaseSequential):\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"GPT Model\"\n",
    "\n",
    "    @property\n",
    "    def parameters(self) -> dict:\n",
    "        return {\n",
    "            \"n_layers\": self.n_layers,\n",
    "            \"internal_size\": self.internal_size,\n",
    "            \"n_heads\": self.n_heads,\n",
    "            \"dropout_rate\": self.dropout_rate,\n",
    "            \"head_size\": self.head_size\n",
    "        }\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_layers = 12\n",
    "        self.internal_size = 768\n",
    "        self.n_heads = 12\n",
    "        self.head_size = self.internal_size / self.n_heads\n",
    "        self.dropout_rate = 0.02\n",
    "        self.is_decoder = True\n",
    "\n",
    "    def apply(self, X, prefix: str = None):\n",
    "        #window_size = self.sequence_length\n",
    "        real_size = X.shape[-1]\n",
    "\n",
    "        m_x = X\n",
    "\n",
    "        for layer_i in range(self.n_layers):\n",
    "            m_x = TransformerDecoderBlock(real_size, self.internal_size, self.n_heads, dropout_rate=self.dropout_rate)(m_x)\n",
    "\n",
    "        return m_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main FlowTransformer T_T**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FlowTransformer:\n",
    "    retain_inmem_cache = False\n",
    "    inmem_cache = None\n",
    "\n",
    "    def  __init__(self, pre_processing:BasePreProcessing,\n",
    "                  input_encoding:BaseInputEncoding,\n",
    "                  sequential_model:FunctionalComponent,\n",
    "                  classification_head:BaseClassificationHead,\n",
    "                  params:FlowTransformerParameters,\n",
    "                  rs:np.random.RandomState=None):\n",
    "\n",
    "        self.rs = np.random.RandomState(seed=42) if rs is None else rs\n",
    "        self.classification_head = classification_head\n",
    "        self.sequential_model = sequential_model\n",
    "        self.input_encoding = input_encoding\n",
    "        self.pre_processing = pre_processing\n",
    "        self.parameters = params\n",
    "\n",
    "        self.dataset_specification: Optional[DatasetSpecification] = None\n",
    "\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "        self.training_mask = None\n",
    "        self.model_input_spec: Optional[ModelInputSpecification] = None\n",
    "\n",
    "        self.experiment_key = {}\n",
    "\n",
    "    def build_model(self, prefix:str=None):\n",
    "        if prefix is None:\n",
    "            prefix = \"\"\n",
    "\n",
    "        if self.X is None:\n",
    "            raise Exception(\"Please call load_dataset before calling build_model()\")\n",
    "\n",
    "        m_inputs = []\n",
    "        for numeric_feature in self.model_input_spec.numeric_feature_names:\n",
    "            m_input = Input((self.parameters.window_size, 1), name=f\"{prefix}input_{numeric_feature}\", dtype=\"float32\")\n",
    "            m_inputs.append(m_input)\n",
    "\n",
    "        for categorical_feature_name, categorical_feature_levels in \\\n",
    "            zip(self.model_input_spec.categorical_feature_names, self.model_input_spec.levels_per_categorical_feature):\n",
    "            m_input = Input(\n",
    "                (self.parameters.window_size, 1 if self.model_input_spec.categorical_format == CategoricalFormat.Integers else categorical_feature_levels),\n",
    "                name=f\"{prefix}input_{categorical_feature_name}\",\n",
    "                dtype=\"int32\" if self.model_input_spec.categorical_format == CategoricalFormat.Integers else \"float32\"\n",
    "            )\n",
    "            m_inputs.append(m_input)\n",
    "\n",
    "        self.input_encoding.build(self.parameters.window_size, self.model_input_spec)\n",
    "        self.sequential_model.build(self.parameters.window_size, self.model_input_spec)\n",
    "        self.classification_head.build(self.parameters.window_size, self.model_input_spec)\n",
    "\n",
    "        m_x = self.input_encoding.apply(m_inputs, prefix)\n",
    "\n",
    "        # in case the classification head needs to add tokens at this stage\n",
    "        m_x = self.classification_head.apply_before_transformer(m_x, prefix)\n",
    "\n",
    "        m_x = self.sequential_model.apply(m_x, prefix)\n",
    "        m_x = self.classification_head.apply(m_x, prefix)\n",
    "\n",
    "        for layer_i, layer_size in enumerate(self.parameters.mlp_layer_sizes):\n",
    "            m_x = Dense(layer_size, activation=\"relu\", name=f\"{prefix}classification_mlp_{layer_i}_{layer_size}\")(m_x)\n",
    "            m_x = Dropout(self.parameters.mlp_dropout)(m_x) if self.parameters.mlp_dropout > 0 else m_x\n",
    "\n",
    "        m_x = Dense(1, activation=\"sigmoid\", name=f\"{prefix}binary_classification_out\")(m_x)\n",
    "        m = Model(m_inputs, m_x)\n",
    "        #m.summary()\n",
    "        return m\n",
    "\n",
    "    def _load_preprocessed_dataset(self, dataset_name:str,\n",
    "                     dataset:Union[pd.DataFrame, str],\n",
    "                     specification:DatasetSpecification,\n",
    "                     cache_folder:Optional[str]=None,\n",
    "                     n_rows:int=0,\n",
    "                     evaluation_dataset_sampling:EvaluationDatasetSampling=EvaluationDatasetSampling.LastRows,\n",
    "                     evaluation_percent:float=0.2,\n",
    "                     numerical_filter=1_000_000_000) -> Tuple[pd.DataFrame, ModelInputSpecification]:\n",
    "\n",
    "        cache_file_path = None\n",
    "\n",
    "        if dataset_name is None:\n",
    "            raise Exception(f\"Dataset name must be specified so FlowTransformer can optimise operations between subsequent calls!\")\n",
    "\n",
    "        pp_key = get_identifier(\n",
    "            {\n",
    "                \"__preprocessing_name\": self.pre_processing.name,\n",
    "                **self.pre_processing.parameters\n",
    "            }\n",
    "        )\n",
    "\n",
    "        local_key = get_identifier({\n",
    "            \"evaluation_percent\": evaluation_percent,\n",
    "            \"numerical_filter\": numerical_filter,\n",
    "            \"categorical_method\": str(self.input_encoding.required_input_format),\n",
    "            \"n_rows\": n_rows,\n",
    "        })\n",
    "\n",
    "        cache_key = f\"{dataset_name}_{n_rows}_{pp_key}_{local_key}\"\n",
    "\n",
    "        if FlowTransformer.retain_inmem_cache:\n",
    "            if FlowTransformer.inmem_cache is not None and cache_key in FlowTransformer.inmem_cache:\n",
    "                print(f\"Using in-memory cached version of this pre-processed dataset. To turn off this functionality set FlowTransformer.retain_inmem_cache = False\")\n",
    "                return FlowTransformer.inmem_cache[cache_key]\n",
    "\n",
    "        if cache_folder is not None:\n",
    "            cache_file_name = f\"{cache_key}.feather\"\n",
    "            cache_file_path = os.path.join(cache_folder, cache_file_name)\n",
    "\n",
    "            print(f\"Using cache file path: {cache_file_path}\")\n",
    "\n",
    "            if os.path.exists(cache_file_path):\n",
    "                print(f\"Reading directly from cache {cache_file_path}...\")\n",
    "                model_input_spec: ModelInputSpecification\n",
    "                dataset, model_input_spec = load_feather_plus_metadata(cache_file_path)\n",
    "                return dataset, model_input_spec\n",
    "\n",
    "        if isinstance(dataset, str):\n",
    "            print(f\"Attempting to read dataset from path {dataset}...\")\n",
    "            if dataset.lower().endswith(\".feather\"):\n",
    "                # read as a feather file\n",
    "                dataset = pd.read_feather(dataset, columns=specification.include_fields+[specification.class_column])\n",
    "            elif dataset.lower().endswith(\".csv\"):\n",
    "                dataset = pd.read_csv(dataset, nrows=n_rows if n_rows > 0 else None)\n",
    "            else:\n",
    "                raise Exception(\"Unrecognised dataset filetype!\")\n",
    "        elif not isinstance(dataset, pd.DataFrame):\n",
    "            raise Exception(\"Unrecognised dataset input type, should be a path to a CSV or feather file, or a pandas dataframe!\")\n",
    "\n",
    "        assert isinstance(dataset, pd.DataFrame)\n",
    "\n",
    "        if 0 < n_rows < len(dataset):\n",
    "            dataset = dataset.iloc[:n_rows]\n",
    "\n",
    "        training_mask = np.ones(len(dataset),  dtype=bool)\n",
    "        eval_n = int(len(dataset) * evaluation_percent)\n",
    "\n",
    "        if evaluation_dataset_sampling == EvaluationDatasetSampling.FilterColumn:\n",
    "            if dataset.columns[-1] != specification.test_column:\n",
    "                raise Exception(f\"Ensure that the 'test' ({specification.test_column}) column is the last column of the dataset being loaded, and that the name of this column is provided as part of the dataset specification\")\n",
    "\n",
    "        if evaluation_dataset_sampling != EvaluationDatasetSampling.LastRows:\n",
    "            warnings.warn(\"Using EvaluationDatasetSampling options other than LastRows might leak some information during training, if for example the context window leading up to a particular flow contains an evaluation flow, and this flow has out of range values (out of range to when pre-processing was applied on the training flows), then the model might potentially learn to handle these. In any case, no class leakage is present.\")\n",
    "\n",
    "        if evaluation_dataset_sampling == EvaluationDatasetSampling.LastRows:\n",
    "            training_mask[-eval_n:] = False\n",
    "        elif evaluation_dataset_sampling == EvaluationDatasetSampling.RandomRows:\n",
    "            index = np.arange(self.parameters.window_size, len(dataset))\n",
    "            sample = self.rs.choice(index, eval_n, replace=False)\n",
    "            training_mask[sample] = False\n",
    "        elif evaluation_dataset_sampling == EvaluationDatasetSampling.FilterColumn:\n",
    "            # must be the last column of the dataset\n",
    "            training_column = dataset.columns[-1]\n",
    "            print(f\"Using the last column {training_column} as the training mask column\")\n",
    "\n",
    "            v, c = np.unique(dataset[training_column].values,  return_counts=True)\n",
    "            min_index = np.argmin(c)\n",
    "            min_v = v[min_index]\n",
    "\n",
    "            warnings.warn(f\"Autodetected class {min_v} of {training_column} to represent the evaluation class!\")\n",
    "\n",
    "            eval_indices = np.argwhere(dataset[training_column].values == min_v).reshape(-1)\n",
    "            eval_indices = eval_indices[(eval_indices > self.parameters.window_size)]\n",
    "\n",
    "            training_mask[eval_indices] = False\n",
    "            del dataset[training_column]\n",
    "\n",
    "        numerical_columns = set(specification.include_fields).difference(specification.categorical_fields)\n",
    "        categorical_columns = specification.categorical_fields\n",
    "\n",
    "        print(f\"Set y to = {specification.class_column}\")\n",
    "        new_df = {\"__training\": training_mask, \"__y\": dataset[specification.class_column].values}\n",
    "        new_features = []\n",
    "\n",
    "        print(\"Converting numerical columns to floats, and removing out of range values...\")\n",
    "        for col_name in numerical_columns:\n",
    "            assert col_name in dataset.columns\n",
    "            new_features.append(col_name)\n",
    "\n",
    "            col_values = dataset[col_name].values\n",
    "            col_values[~np.isfinite(col_values)] = 0\n",
    "            col_values[col_values < -numerical_filter] = 0\n",
    "            col_values[col_values > numerical_filter] = 0\n",
    "            col_values = col_values.astype(\"float32\")\n",
    "\n",
    "            if not np.all(np.isfinite(col_values)):\n",
    "                raise Exception(\"Flow format data had non finite values after float transformation!\")\n",
    "\n",
    "            new_df[col_name] = col_values\n",
    "\n",
    "        print(f\"Applying pre-processing to numerical values\")\n",
    "        for i, col_name in enumerate(numerical_columns):\n",
    "            print(f\"[Numerical {i+1:,} / {len(numerical_columns)}] Processing numerical column {col_name}...\")\n",
    "            all_data = new_df[col_name]\n",
    "            training_data = all_data[training_mask]\n",
    "\n",
    "            self.pre_processing.fit_numerical(col_name, training_data)\n",
    "            new_df[col_name] = self.pre_processing.transform_numerical(col_name, all_data)\n",
    "\n",
    "        print(f\"Applying pre-processing to categorical values\")\n",
    "        levels_per_categorical_feature = []\n",
    "        for i, col_name in enumerate(categorical_columns):\n",
    "            new_features.append(col_name)\n",
    "            if col_name == specification.class_column:\n",
    "                continue\n",
    "            print(f\"[Categorical {i+1:,} / {len(categorical_columns)}] Processing categorical column {col_name}...\")\n",
    "\n",
    "            all_data = dataset[col_name].values\n",
    "            training_data = all_data[training_mask]\n",
    "\n",
    "            self.pre_processing.fit_categorical(col_name, training_data)\n",
    "            new_values = self.pre_processing.transform_categorical(col_name, all_data, self.input_encoding.required_input_format)\n",
    "\n",
    "            if self.input_encoding.required_input_format == CategoricalFormat.OneHot:\n",
    "                # multiple columns of one hot values\n",
    "                if isinstance(new_values, pd.DataFrame):\n",
    "                    levels_per_categorical_feature.append(len(new_values.columns))\n",
    "                    for c in new_values.columns:\n",
    "                        new_df[c] = new_values[c]\n",
    "                else:\n",
    "                    n_one_hot_levels = new_values.shape[1]\n",
    "                    levels_per_categorical_feature.append(n_one_hot_levels)\n",
    "                    for z in range(n_one_hot_levels):\n",
    "                        new_df[f\"{col_name}_{z}\"] = new_values[:, z]\n",
    "            else:\n",
    "                # single column of integers\n",
    "                levels_per_categorical_feature.append(len(np.unique(new_values)))\n",
    "                new_df[col_name] = new_values\n",
    "\n",
    "        print(f\"Generating pre-processed dataframe...\")\n",
    "        new_df = pd.DataFrame(new_df)\n",
    "        model_input_spec = ModelInputSpecification(new_features, len(numerical_columns), levels_per_categorical_feature, self.input_encoding.required_input_format)\n",
    "\n",
    "        print(f\"Input data frame had shape ({len(dataset)},{len(dataset.columns)}), output data frame has shape ({len(new_df)},{len(new_df.columns)}) after pre-processing...\")\n",
    "\n",
    "        if cache_file_path is not None:\n",
    "            print(f\"Writing to cache file path: {cache_file_path}...\")\n",
    "            save_feather_plus_metadata(cache_file_path, new_df, model_input_spec)\n",
    "\n",
    "        if FlowTransformer.retain_inmem_cache:\n",
    "            if FlowTransformer.inmem_cache is None:\n",
    "                FlowTransformer.inmem_cache = {}\n",
    "\n",
    "            FlowTransformer.inmem_cache.clear()\n",
    "            FlowTransformer.inmem_cache[cache_key] = (new_df, model_input_spec)\n",
    "\n",
    "        return new_df, model_input_spec\n",
    "\n",
    "    def load_dataset(self, dataset_name:str,\n",
    "                     dataset:Union[pd.DataFrame, str],\n",
    "                     specification:DatasetSpecification,\n",
    "                     cache_path:Optional[str]=None,\n",
    "                     n_rows:int=0,\n",
    "                     evaluation_dataset_sampling:EvaluationDatasetSampling=EvaluationDatasetSampling.LastRows,\n",
    "                     evaluation_percent:float=0.2,\n",
    "                     numerical_filter=1_000_000_000) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load a dataset and prepare it for training\n",
    "\n",
    "        :param dataset: The path to a CSV dataset to load from, or a dataframe\n",
    "        :param cache_path: Where to store a cached version of this file\n",
    "        :param n_rows: The number of rows to ingest from the dataset, or 0 to ingest all\n",
    "        \"\"\"\n",
    "\n",
    "        if cache_path is None:\n",
    "            cache_path = \"cache\"\n",
    "\n",
    "        if not os.path.exists(cache_path):\n",
    "            warnings.warn(f\"Could not find cache folder: {cache_path}, attempting to create\")\n",
    "            os.mkdir(cache_path)\n",
    "\n",
    "        self.dataset_specification = specification\n",
    "        df, model_input_spec = self._load_preprocessed_dataset(dataset_name, dataset, specification, cache_path, n_rows, evaluation_dataset_sampling, evaluation_percent, numerical_filter)\n",
    "\n",
    "        training_mask = df[\"__training\"].values\n",
    "        del df[\"__training\"]\n",
    "\n",
    "        y = df[\"__y\"].values\n",
    "        del df[\"__y\"]\n",
    "\n",
    "        self.X = df\n",
    "        self.y = y\n",
    "        self.training_mask = training_mask\n",
    "        self.model_input_spec = model_input_spec\n",
    "\n",
    "        return df\n",
    "\n",
    "    def evaluate(self, m:keras.Model, batch_size, early_stopping_patience:int=5, epochs:int=100, steps_per_epoch:int=128):\n",
    "        n_malicious_per_batch = int(0.5 * batch_size)\n",
    "        n_legit_per_batch = batch_size - n_malicious_per_batch\n",
    "\n",
    "        overall_y_preserve = np.zeros(dtype=\"float32\", shape=(n_malicious_per_batch + n_legit_per_batch,))\n",
    "        overall_y_preserve[:n_malicious_per_batch] = 1.\n",
    "\n",
    "        selectable_mask = np.zeros(len(self.X), dtype=bool)\n",
    "        selectable_mask[self.parameters.window_size:-self.parameters.window_size] = True\n",
    "        train_mask = self.training_mask\n",
    "\n",
    "        y_mask = ~(self.y.astype('str') == str(self.dataset_specification.benign_label))\n",
    "\n",
    "        indices_train = np.argwhere(train_mask).reshape(-1)\n",
    "        malicious_indices_train = np.argwhere(train_mask & y_mask & selectable_mask).reshape(-1)\n",
    "        legit_indices_train = np.argwhere(train_mask & ~y_mask & selectable_mask).reshape(-1)\n",
    "\n",
    "        indices_test:np.ndarray = np.argwhere(~train_mask).reshape(-1)\n",
    "\n",
    "        def get_windows_for_indices(indices:np.ndarray, ordered) -> List[pd.DataFrame]:\n",
    "            X: List[pd.DataFrame] = []\n",
    "\n",
    "            if ordered:\n",
    "                # we don't really want to include eval samples as part of context, because out of range values might be learned\n",
    "                # by the model, _but_ we are forced to in the windowed approach, if users haven't just selected the\n",
    "                # \"take last 10%\" as eval option. We warn them prior to this though.\n",
    "                for i1 in indices:\n",
    "                    X.append(self.X.iloc[(i1 - self.parameters.window_size) + 1:i1 + 1])\n",
    "            else:\n",
    "                context_indices_batch = np.random.choice(indices_train, size=(batch_size, self.parameters.window_size),\n",
    "                                                         replace=False).reshape(-1)\n",
    "                context_indices_batch[:, -1] = indices\n",
    "\n",
    "                for index in context_indices_batch:\n",
    "                    X.append(self.X.iloc[index])\n",
    "\n",
    "            return X\n",
    "\n",
    "        feature_columns_map = {}\n",
    "\n",
    "        def samplewise_to_featurewise(X):\n",
    "            sequence_length = len(X[0])\n",
    "\n",
    "            combined_df = pd.concat(X)\n",
    "\n",
    "            featurewise_X = []\n",
    "\n",
    "            if len(feature_columns_map) == 0:\n",
    "                for feature in self.model_input_spec.feature_names:\n",
    "                    if feature in self.model_input_spec.numeric_feature_names or self.model_input_spec.categorical_format == CategoricalFormat.Integers:\n",
    "                        feature_columns_map[feature] = feature\n",
    "                    else:\n",
    "                        # this is a one-hot encoded categorical feature\n",
    "                        feature_columns_map[feature] = [c for c in X[0].columns if str(c).startswith(feature)]\n",
    "\n",
    "            for feature in self.model_input_spec.feature_names:\n",
    "                feature_columns = feature_columns_map[feature]\n",
    "                combined_values = combined_df[feature_columns].values\n",
    "\n",
    "                # maybe this can be faster with a reshape but I couldn't get it to work\n",
    "                combined_values = np.array([combined_values[i:i+sequence_length] for i in range(0, len(combined_values), sequence_length)])\n",
    "                featurewise_X.append(combined_values)\n",
    "\n",
    "            return featurewise_X\n",
    "\n",
    "        print(f\"Building eval dataset...\")\n",
    "        eval_X = get_windows_for_indices(indices_test, True)\n",
    "        print(f\"Splitting dataset to featurewise...\")\n",
    "        eval_featurewise_X = samplewise_to_featurewise(eval_X)\n",
    "        eval_y = y_mask[indices_test]\n",
    "        eval_P = eval_y\n",
    "        n_eval_P = np.count_nonzero(eval_P)\n",
    "        eval_N = ~eval_y\n",
    "        n_eval_N = np.count_nonzero(eval_N)\n",
    "        print(f\"Evaluation dataset is built!\")\n",
    "\n",
    "        print(f\"Positive samples in eval set: {n_eval_P}\")\n",
    "        print(f\"Negative samples in eval set: {n_eval_N}\")\n",
    "\n",
    "        epoch_results = []\n",
    "\n",
    "        def run_evaluation(epoch):\n",
    "            pred_y = m.predict(eval_featurewise_X, verbose=True)\n",
    "            pred_y = pred_y.reshape(-1) > 0.5\n",
    "\n",
    "            pred_P = pred_y\n",
    "            n_pred_P = np.count_nonzero(pred_P)\n",
    "\n",
    "            pred_N = ~pred_y\n",
    "            n_pred_N = np.count_nonzero(pred_N)\n",
    "\n",
    "            TP = np.count_nonzero(pred_P & eval_P)\n",
    "            FP = np.count_nonzero(pred_P & ~eval_P)\n",
    "            TN = np.count_nonzero(pred_N & eval_N)\n",
    "            FN = np.count_nonzero(pred_N & ~eval_N)\n",
    "\n",
    "            sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "            specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "            balanced_accuracy = (sensitivity + specificity) / 2\n",
    "\n",
    "            precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "            recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            print(f\"Epoch {epoch} yielded predictions: {pred_y.shape}, overall balanced accuracy: {balanced_accuracy * 100:.2f}%, TP = {TP:,} / {n_eval_P:,}, TN = {TN:,} / {n_eval_N:,}\")\n",
    "\n",
    "            epoch_results.append({\n",
    "                \"epoch\": epoch,\n",
    "                \"P\": n_eval_P,\n",
    "                \"N\": n_eval_N,\n",
    "                \"pred_P\": n_pred_P,\n",
    "                \"pred_N\": n_pred_N,\n",
    "                \"TP\": TP,\n",
    "                \"FP\": FP,\n",
    "                \"TN\": TN,\n",
    "                \"FN\": FN,\n",
    "                \"bal_acc\": balanced_accuracy,\n",
    "                \"f1\": f1_score\n",
    "            })\n",
    "\n",
    "\n",
    "        class BatchYielder():\n",
    "            def __init__(self, ordered, random, rs):\n",
    "                self.ordered = ordered\n",
    "                self.random = random\n",
    "                self.cursor_malicious = 0\n",
    "                self.cursor_legit = 0\n",
    "                self.rs = rs\n",
    "\n",
    "            def get_batch(self):\n",
    "                malicious_indices_batch = self.rs.choice(malicious_indices_train, size=n_malicious_per_batch,\n",
    "                                                         replace=False) \\\n",
    "                    if self.random else \\\n",
    "                    malicious_indices_train[self.cursor_malicious:self.cursor_malicious + n_malicious_per_batch]\n",
    "                \n",
    "                print(\"Legit indice size print:  \"+str(len(legit_indices_train)))\n",
    "\n",
    "                legitimate_indices_batch = self.rs.choice(legit_indices_train, size=n_legit_per_batch, replace=False) \\\n",
    "                    if self.random else \\\n",
    "                    legit_indices_train[self.cursor_legit:self.cursor_legit + n_legit_per_batch]\n",
    "\n",
    "                indices = np.concatenate([malicious_indices_batch, legitimate_indices_batch])\n",
    "\n",
    "                self.cursor_malicious = self.cursor_malicious + n_malicious_per_batch\n",
    "                self.cursor_malicious = self.cursor_malicious % (len(malicious_indices_train) - n_malicious_per_batch)\n",
    "\n",
    "                self.cursor_legit = self.cursor_legit + n_legit_per_batch\n",
    "                self.cursor_legit = self.cursor_legit % (len(legit_indices_train) - n_legit_per_batch)\n",
    "\n",
    "                X = get_windows_for_indices(indices, self.ordered)\n",
    "                # each x in X contains a dataframe, with window_size rows and all the features of the flows. There are batch_size of these.\n",
    "\n",
    "                # we have a dataframe containing batch_size x (window_size, features)\n",
    "                # we actually want a result of features x (batch_size, sequence_length, feature_dimension)\n",
    "                featurewise_X = samplewise_to_featurewise(X)\n",
    "\n",
    "                return featurewise_X, overall_y_preserve\n",
    "\n",
    "        batch_yielder = BatchYielder(self.parameters._train_ensure_flows_are_ordered_within_windows, not self.parameters._train_draw_sequential_windows, self.rs)\n",
    "\n",
    "        min_loss = 100\n",
    "        iters_since_loss_decrease = 0\n",
    "\n",
    "        train_results = []\n",
    "        final_epoch = 0\n",
    "\n",
    "        last_print = time.time()\n",
    "        elapsed_time = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            final_epoch = epoch\n",
    "\n",
    "            has_reduced_loss = False\n",
    "            for step in range(steps_per_epoch):\n",
    "                batch_X, batch_y = batch_yielder.get_batch()\n",
    "\n",
    "                t0 = time.time()\n",
    "                batch_results = m.train_on_batch(batch_X, batch_y)\n",
    "                t1 = time.time()\n",
    "\n",
    "                if epoch > 0 or step > 0:\n",
    "                    elapsed_time += (t1 - t0)\n",
    "                    if epoch == 0 and step == 1:\n",
    "                        # include time for last \"step\" that we skipped with step > 0 for epoch == 0\n",
    "                        elapsed_time *= 2\n",
    "\n",
    "                train_results.append(batch_results + [elapsed_time, epoch])\n",
    "\n",
    "                batch_loss = batch_results[0] if isinstance(batch_results, list) else batch_results\n",
    "\n",
    "                if time.time() - last_print > 3:\n",
    "                    last_print = time.time()\n",
    "                    early_stop_phrase = \"\" if early_stopping_patience <= 0 else f\" (early stop in {early_stopping_patience - iters_since_loss_decrease:,})\"\n",
    "                    print(f\"Epoch = {epoch:,} / {epochs:,}{early_stop_phrase}, step = {step}, loss = {batch_loss:.5f}, results = {batch_results} -- elapsed (train): {elapsed_time:.2f}s\")\n",
    "\n",
    "                if batch_loss < min_loss:\n",
    "                    has_reduced_loss = True\n",
    "                    min_loss = batch_loss\n",
    "\n",
    "            if has_reduced_loss:\n",
    "                iters_since_loss_decrease = 0\n",
    "            else:\n",
    "                iters_since_loss_decrease += 1\n",
    "\n",
    "            do_early_stop = early_stopping_patience > 0 and iters_since_loss_decrease > early_stopping_patience\n",
    "            is_last_epoch = epoch == epochs - 1\n",
    "            run_eval = epoch in [6] or is_last_epoch or do_early_stop\n",
    "\n",
    "            if run_eval:\n",
    "                run_evaluation(epoch)\n",
    "\n",
    "            if do_early_stop:\n",
    "                print(f\"Early stopping at epoch: {epoch}\")\n",
    "                break\n",
    "\n",
    "        eval_results = pd.DataFrame(epoch_results)\n",
    "\n",
    "        return (train_results, eval_results, final_epoch)\n",
    "\n",
    "\n",
    "    def time(self, m:keras.Model, batch_size, n_steps=128, n_repeats=4):\n",
    "        n_malicious_per_batch = int(0.5 * batch_size)\n",
    "        n_legit_per_batch = batch_size - n_malicious_per_batch\n",
    "\n",
    "        overall_y_preserve = np.zeros(dtype=\"float32\", shape=(n_malicious_per_batch + n_legit_per_batch,))\n",
    "        overall_y_preserve[:n_malicious_per_batch] = 1.\n",
    "\n",
    "        selectable_mask = np.zeros(len(self.X), dtype=bool)\n",
    "        selectable_mask[self.parameters.window_size:-self.parameters.window_size] = True\n",
    "        train_mask = self.training_mask\n",
    "\n",
    "        y_mask = ~(self.y.astype('str') == str(self.dataset_specification.benign_label))\n",
    "\n",
    "        indices_train = np.argwhere(train_mask).reshape(-1)\n",
    "        malicious_indices_train = np.argwhere(train_mask & y_mask & selectable_mask).reshape(-1)\n",
    "        legit_indices_train = np.argwhere(train_mask & ~y_mask & selectable_mask).reshape(-1)\n",
    "\n",
    "        indices_test:np.ndarray = np.argwhere(~train_mask).reshape(-1)\n",
    "\n",
    "        def get_windows_for_indices(indices:np.ndarray, ordered) -> List[pd.DataFrame]:\n",
    "            X: List[pd.DataFrame] = []\n",
    "\n",
    "            if ordered:\n",
    "                # we don't really want to include eval samples as part of context, because out of range values might be learned\n",
    "                # by the model, _but_ we are forced to in the windowed approach, if users haven't just selected the\n",
    "                # \"take last 10%\" as eval option. We warn them prior to this though.\n",
    "                for i1 in indices:\n",
    "                    X.append(self.X.iloc[(i1 - self.parameters.window_size) + 1:i1 + 1])\n",
    "            else:\n",
    "                context_indices_batch = np.random.choice(indices_train, size=(batch_size, self.parameters.window_size),\n",
    "                                                         replace=False).reshape(-1)\n",
    "                context_indices_batch[:, -1] = indices\n",
    "\n",
    "                for index in context_indices_batch:\n",
    "                    X.append(self.X.iloc[index])\n",
    "\n",
    "            return X\n",
    "\n",
    "        feature_columns_map = {}\n",
    "\n",
    "        def samplewise_to_featurewise(X):\n",
    "            sequence_length = len(X[0])\n",
    "\n",
    "            combined_df = pd.concat(X)\n",
    "\n",
    "            featurewise_X = []\n",
    "\n",
    "            if len(feature_columns_map) == 0:\n",
    "                for feature in self.model_input_spec.feature_names:\n",
    "                    if feature in self.model_input_spec.numeric_feature_names or self.model_input_spec.categorical_format == CategoricalFormat.Integers:\n",
    "                        feature_columns_map[feature] = feature\n",
    "                    else:\n",
    "                        # this is a one-hot encoded categorical feature\n",
    "                        feature_columns_map[feature] = [c for c in X[0].columns if str(c).startswith(feature)]\n",
    "\n",
    "            for feature in self.model_input_spec.feature_names:\n",
    "                feature_columns = feature_columns_map[feature]\n",
    "                combined_values = combined_df[feature_columns].values\n",
    "\n",
    "                # maybe this can be faster with a reshape but I couldn't get it to work\n",
    "                combined_values = np.array([combined_values[i:i+sequence_length] for i in range(0, len(combined_values), sequence_length)])\n",
    "                featurewise_X.append(combined_values)\n",
    "\n",
    "            return featurewise_X\n",
    "\n",
    "\n",
    "        epoch_results = []\n",
    "\n",
    "\n",
    "        class BatchYielder():\n",
    "            def __init__(self, ordered, random, rs):\n",
    "                self.ordered = ordered\n",
    "                self.random = random\n",
    "                self.cursor_malicious = 0\n",
    "                self.cursor_legit = 0\n",
    "                self.rs = rs\n",
    "\n",
    "            def get_batch(self):\n",
    "                malicious_indices_batch = self.rs.choice(malicious_indices_train, size=n_malicious_per_batch,\n",
    "                                                         replace=False) \\\n",
    "                    if self.random else \\\n",
    "                    malicious_indices_train[self.cursor_malicious:self.cursor_malicious + n_malicious_per_batch]\n",
    "\n",
    "                legitimate_indices_batch = self.rs.choice(legit_indices_train, size=n_legit_per_batch, replace=False) \\\n",
    "                    if self.random else \\\n",
    "                    legit_indices_train[self.cursor_legit:self.cursor_legit + n_legit_per_batch]\n",
    "\n",
    "                indices = np.concatenate([malicious_indices_batch, legitimate_indices_batch])\n",
    "\n",
    "                self.cursor_malicious = self.cursor_malicious + n_malicious_per_batch\n",
    "                self.cursor_malicious = self.cursor_malicious % (len(malicious_indices_train) - n_malicious_per_batch)\n",
    "\n",
    "                self.cursor_legit = self.cursor_legit + n_legit_per_batch\n",
    "                self.cursor_legit = self.cursor_legit % (len(legit_indices_train) - n_legit_per_batch)\n",
    "\n",
    "                X = get_windows_for_indices(indices, self.ordered)\n",
    "                # each x in X contains a dataframe, with window_size rows and all the features of the flows. There are batch_size of these.\n",
    "\n",
    "                # we have a dataframe containing batch_size x (window_size, features)\n",
    "                # we actually want a result of features x (batch_size, sequence_length, feature_dimension)\n",
    "                featurewise_X = samplewise_to_featurewise(X)\n",
    "\n",
    "                return featurewise_X, overall_y_preserve\n",
    "\n",
    "        batch_yielder = BatchYielder(self.parameters._train_ensure_flows_are_ordered_within_windows, not self.parameters._train_draw_sequential_windows, self.rs)\n",
    "\n",
    "        min_loss = 100\n",
    "        iters_since_loss_decrease = 0\n",
    "\n",
    "        final_epoch = 0\n",
    "\n",
    "        last_print = time.time()\n",
    "        elapsed_time = 0\n",
    "\n",
    "        batch_times = []\n",
    "\n",
    "\n",
    "        for step in range(n_steps):\n",
    "            batch_X, batch_y = batch_yielder.get_batch()\n",
    "\n",
    "            local_batch_times = []\n",
    "            for i in range(n_repeats):\n",
    "                t0 = time.time()\n",
    "                batch_results = m.predict_on_batch(batch_X)\n",
    "                t1 = time.time()\n",
    "                local_batch_times.append(t1 - t0)\n",
    "\n",
    "            batch_times.append(local_batch_times)\n",
    "\n",
    "            if time.time() - last_print > 3:\n",
    "                last_print = time.time()\n",
    "                print(f\"Step = {step}, running model evaluation... Average times = {np.mean(np.array(batch_times).reshape(-1))}\")\n",
    "\n",
    "        return batch_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache file path: cache\\CSE_CIC_IDS_0_QdLmZHuh8yOmlGcKBEkf7hepImY0_VHNk9ujbqtTXGSrgVayeqG486IQ0.feather\n",
      "Reading directly from cache cache\\CSE_CIC_IDS_0_QdLmZHuh8yOmlGcKBEkf7hepImY0_VHNk9ujbqtTXGSrgVayeqG486IQ0.feather...\n",
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_URG Flag Count (Inpu  [(None, 8, 1)]               0         []                            \n",
      " tLayer)                                                                                          \n",
      "                                                                                                  \n",
      " input_Fwd Act Data Pkts (I  [(None, 8, 1)]               0         []                            \n",
      " nputLayer)                                                                                       \n",
      "                                                                                                  \n",
      " input_Bwd Packet/Bulk Avg   [(None, 8, 1)]               0         []                            \n",
      " (InputLayer)                                                                                     \n",
      "                                                                                                  \n",
      " input_Active Max (InputLay  [(None, 8, 1)]               0         []                            \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " input_Fwd IAT Mean (InputL  [(None, 8, 1)]               0         []                            \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " input_Flow Bytes/s (InputL  [(None, 8, 1)]               0         []                            \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " input_Bwd IAT Min (InputLa  [(None, 8, 1)]               0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " input_Subflow Fwd Packets   [(None, 8, 1)]               0         []                            \n",
      " (InputLayer)                                                                                     \n",
      "                                                                                                  \n",
      " input_Subflow Fwd Bytes (I  [(None, 8, 1)]               0         []                            \n",
      " nputLayer)                                                                                       \n",
      "                                                                                                  \n",
      " input_Fwd Packet Length Me  [(None, 8, 1)]               0         []                            \n",
      " an (InputLayer)                                                                                  \n",
      "                                                                                                  \n",
      " input_Idle Mean (InputLaye  [(None, 8, 1)]               0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " input_Idle Max (InputLayer  [(None, 8, 1)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_Bwd IAT Total (Input  [(None, 8, 1)]               0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " input_ECE Flag Count (Inpu  [(None, 8, 1)]               0         []                            \n",
      " tLayer)                                                                                          \n",
      "                                                                                                  \n",
      " input_Fwd Segment Size Avg  [(None, 8, 1)]               0         []                            \n",
      "  (InputLayer)                                                                                    \n",
      "                                                                                                  \n",
      " input_Total Bwd packets (I  [(None, 8, 1)]               0         []                            \n",
      " nputLayer)                                                                                       \n",
      "                                                                                                  \n",
      " input_Bwd Packet Length St  [(None, 8, 1)]               0         []                            \n",
      " d (InputLayer)                                                                                   \n",
      "                                                                                                  \n",
      " input_Subflow Bwd Packets   [(None, 8, 1)]               0         []                            \n",
      " (InputLayer)                                                                                     \n",
      "                                                                                                  \n",
      " input_Active Mean (InputLa  [(None, 8, 1)]               0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " input_Bwd IAT Max (InputLa  [(None, 8, 1)]               0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " input_Packet Length Min (I  [(None, 8, 1)]               0         []                            \n",
      " nputLayer)                                                                                       \n",
      "                                                                                                  \n",
      " input_Packet Length Std (I  [(None, 8, 1)]               0         []                            \n",
      " nputLayer)                                                                                       \n",
      "                                                                                                  \n",
      " input_Bwd Packet Length Ma  [(None, 8, 1)]               0         []                            \n",
      " x (InputLayer)                                                                                   \n",
      "                                                                                                  \n",
      " input_Active Min (InputLay  [(None, 8, 1)]               0         []                            \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " input_Total Length of Bwd   [(None, 8, 1)]               0         []                            \n",
      " Packet (InputLayer)                                                                              \n",
      "                                                                                                  \n",
      " input_Total Length of Fwd   [(None, 8, 1)]               0         []                            \n",
      " Packet (InputLayer)                                                                              \n",
      "                                                                                                  \n",
      " input_Idle Min (InputLayer  [(None, 8, 1)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_Fwd Packet Length St  [(None, 8, 1)]               0         []                            \n",
      " d (InputLayer)                                                                                   \n",
      "                                                                                                  \n",
      " input_Bwd Packet Length Mi  [(None, 8, 1)]               0         []                            \n",
      " n (InputLayer)                                                                                   \n",
      "                                                                                                  \n",
      " input_Active Std (InputLay  [(None, 8, 1)]               0         []                            \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " input_Flow IAT Min (InputL  [(None, 8, 1)]               0         []                            \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " input_Fwd Bytes/Bulk Avg (  [(None, 8, 1)]               0         []                            \n",
      " InputLayer)                                                                                      \n",
      "                                                                                                  \n",
      " input_FWD Init Win Bytes (  [(None, 8, 1)]               0         []                            \n",
      " InputLayer)                                                                                      \n",
      "                                                                                                  \n",
      " input_Total Fwd Packet (In  [(None, 8, 1)]               0         []                            \n",
      " putLayer)                                                                                        \n",
      "                                                                                                  \n",
      " input_Bwd Bytes/Bulk Avg (  [(None, 8, 1)]               0         []                            \n",
      " InputLayer)                                                                                      \n",
      "                                                                                                  \n",
      " input_RST Flag Count (Inpu  [(None, 8, 1)]               0         []                            \n",
      " tLayer)                                                                                          \n",
      "                                                                                                  \n",
      " input_Subflow Bwd Bytes (I  [(None, 8, 1)]               0         []                            \n",
      " nputLayer)                                                                                       \n",
      "                                                                                                  \n",
      " input_Fwd Seg Size Min (In  [(None, 8, 1)]               0         []                            \n",
      " putLayer)                                                                                        \n",
      "                                                                                                  \n",
      " input_Bwd Init Win Bytes (  [(None, 8, 1)]               0         []                            \n",
      " InputLayer)                                                                                      \n",
      "                                                                                                  \n",
      " input_Fwd IAT Max (InputLa  [(None, 8, 1)]               0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " input_Fwd Bulk Rate Avg (I  [(None, 8, 1)]               0         []                            \n",
      " nputLayer)                                                                                       \n",
      "                                                                                                  \n",
      " input_Fwd Packet/Bulk Avg   [(None, 8, 1)]               0         []                            \n",
      " (InputLayer)                                                                                     \n",
      "                                                                                                  \n",
      " input_Fwd Packet Length Ma  [(None, 8, 1)]               0         []                            \n",
      " x (InputLayer)                                                                                   \n",
      "                                                                                                  \n",
      " input_Bwd IAT Std (InputLa  [(None, 8, 1)]               0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " input_Fwd Packet Length Mi  [(None, 8, 1)]               0         []                            \n",
      " n (InputLayer)                                                                                   \n",
      "                                                                                                  \n",
      " input_Bwd IAT Mean (InputL  [(None, 8, 1)]               0         []                            \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " input_Fwd Packets/s (Input  [(None, 8, 1)]               0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " input_Fwd Header Length (I  [(None, 8, 1)]               0         []                            \n",
      " nputLayer)                                                                                       \n",
      "                                                                                                  \n",
      " input_Packet Length Varian  [(None, 8, 1)]               0         []                            \n",
      " ce (InputLayer)                                                                                  \n",
      "                                                                                                  \n",
      " input_FIN Flag Count (Inpu  [(None, 8, 1)]               0         []                            \n",
      " tLayer)                                                                                          \n",
      "                                                                                                  \n",
      " input_Fwd IAT Total (Input  [(None, 8, 1)]               0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " input_SYN Flag Count (Inpu  [(None, 8, 1)]               0         []                            \n",
      " tLayer)                                                                                          \n",
      "                                                                                                  \n",
      " input_Bwd Header Length (I  [(None, 8, 1)]               0         []                            \n",
      " nputLayer)                                                                                       \n",
      "                                                                                                  \n",
      " input_Fwd IAT Std (InputLa  [(None, 8, 1)]               0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " input_Idle Std (InputLayer  [(None, 8, 1)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_Bwd Segment Size Avg  [(None, 8, 1)]               0         []                            \n",
      "  (InputLayer)                                                                                    \n",
      "                                                                                                  \n",
      " input_Total TCP Flow Time   [(None, 8, 1)]               0         []                            \n",
      " (InputLayer)                                                                                     \n",
      "                                                                                                  \n",
      " input_Bwd Packet Length Me  [(None, 8, 1)]               0         []                            \n",
      " an (InputLayer)                                                                                  \n",
      "                                                                                                  \n",
      " input_Packet Length Mean (  [(None, 8, 1)]               0         []                            \n",
      " InputLayer)                                                                                      \n",
      "                                                                                                  \n",
      " input_Fwd IAT Min (InputLa  [(None, 8, 1)]               0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " input_Flow IAT Max (InputL  [(None, 8, 1)]               0         []                            \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " input_Average Packet Size   [(None, 8, 1)]               0         []                            \n",
      " (InputLayer)                                                                                     \n",
      "                                                                                                  \n",
      " input_Bwd Bulk Rate Avg (I  [(None, 8, 1)]               0         []                            \n",
      " nputLayer)                                                                                       \n",
      "                                                                                                  \n",
      " input_ACK Flag Count (Inpu  [(None, 8, 1)]               0         []                            \n",
      " tLayer)                                                                                          \n",
      "                                                                                                  \n",
      " input_CWR Flag Count (Inpu  [(None, 8, 1)]               0         []                            \n",
      " tLayer)                                                                                          \n",
      "                                                                                                  \n",
      " input_PSH Flag Count (Inpu  [(None, 8, 1)]               0         []                            \n",
      " tLayer)                                                                                          \n",
      "                                                                                                  \n",
      " input_Packet Length Max (I  [(None, 8, 1)]               0         []                            \n",
      " nputLayer)                                                                                       \n",
      "                                                                                                  \n",
      " input_Flow Duration (Input  [(None, 8, 1)]               0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " input_Down/Up Ratio (Input  [(None, 8, 1)]               0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " input_Flow IAT Std (InputL  [(None, 8, 1)]               0         []                            \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " input_Bwd Packets/s (Input  [(None, 8, 1)]               0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " input_Flow Packets/s (Inpu  [(None, 8, 1)]               0         []                            \n",
      " tLayer)                                                                                          \n",
      "                                                                                                  \n",
      " input_Flow IAT Mean (Input  [(None, 8, 1)]               0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " input_Src Port (InputLayer  [(None, 8, 32)]              0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_Dst Port (InputLayer  [(None, 8, 32)]              0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_Protocol (InputLayer  [(None, 8, 4)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_Fwd PSH Flags (Input  [(None, 8, 32)]              0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " input_Bwd PSH Flags (Input  [(None, 8, 32)]              0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " input_Fwd URG Flags (Input  [(None, 8, 5)]               0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " input_Bwd URG Flags (Input  [(None, 8, 1)]               0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " input_Fwd RST Flags (Input  [(None, 8, 32)]              0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " input_Bwd RST Flags (Input  [(None, 8, 8)]               0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " input_ICMP Code (InputLaye  [(None, 8, 6)]               0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " input_ICMP Type (InputLaye  [(None, 8, 5)]               0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " feature_concat (Concatenat  (None, 8, 262)               0         ['input_URG Flag Count[0][0]',\n",
      " e)                                                                  'input_Fwd Act Data Pkts[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'input_Bwd Packet/Bulk Avg[0]\n",
      "                                                                    [0]',                         \n",
      "                                                                     'input_Active Max[0][0]',    \n",
      "                                                                     'input_Fwd IAT Mean[0][0]',  \n",
      "                                                                     'input_Flow Bytes/s[0][0]',  \n",
      "                                                                     'input_Bwd IAT Min[0][0]',   \n",
      "                                                                     'input_Subflow Fwd Packets[0]\n",
      "                                                                    [0]',                         \n",
      "                                                                     'input_Subflow Fwd Bytes[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'input_Fwd Packet Length Mean\n",
      "                                                                    [0][0]',                      \n",
      "                                                                     'input_Idle Mean[0][0]',     \n",
      "                                                                     'input_Idle Max[0][0]',      \n",
      "                                                                     'input_Bwd IAT Total[0][0]', \n",
      "                                                                     'input_ECE Flag Count[0][0]',\n",
      "                                                                     'input_Fwd Segment Size Avg[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'input_Total Bwd packets[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'input_Bwd Packet Length Std[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'input_Subflow Bwd Packets[0]\n",
      "                                                                    [0]',                         \n",
      "                                                                     'input_Active Mean[0][0]',   \n",
      "                                                                     'input_Bwd IAT Max[0][0]',   \n",
      "                                                                     'input_Packet Length Min[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'input_Packet Length Std[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'input_Bwd Packet Length Max[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'input_Active Min[0][0]',    \n",
      "                                                                     'input_Total Length of Bwd Pa\n",
      "                                                                    cket[0][0]',                  \n",
      "                                                                     'input_Total Length of Fwd Pa\n",
      "                                                                    cket[0][0]',                  \n",
      "                                                                     'input_Idle Min[0][0]',      \n",
      "                                                                     'input_Fwd Packet Length Std[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'input_Bwd Packet Length Min[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'input_Active Std[0][0]',    \n",
      "                                                                     'input_Flow IAT Min[0][0]',  \n",
      "                                                                     'input_Fwd Bytes/Bulk Avg[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'input_FWD Init Win Bytes[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'input_Total Fwd Packet[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'input_Bwd Bytes/Bulk Avg[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'input_RST Flag Count[0][0]',\n",
      "                                                                     'input_Subflow Bwd Bytes[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'input_Fwd Seg Size Min[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'input_Bwd Init Win Bytes[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'input_Fwd IAT Max[0][0]',   \n",
      "                                                                     'input_Fwd Bulk Rate Avg[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'input_Fwd Packet/Bulk Avg[0]\n",
      "                                                                    [0]',                         \n",
      "                                                                     'input_Fwd Packet Length Max[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'input_Bwd IAT Std[0][0]',   \n",
      "                                                                     'input_Fwd Packet Length Min[\n",
      "                                                                    0][0]',                       \n",
      "                                                                     'input_Bwd IAT Mean[0][0]',  \n",
      "                                                                     'input_Fwd Packets/s[0][0]', \n",
      "                                                                     'input_Fwd Header Length[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'input_Packet Length Variance\n",
      "                                                                    [0][0]',                      \n",
      "                                                                     'input_FIN Flag Count[0][0]',\n",
      "                                                                     'input_Fwd IAT Total[0][0]', \n",
      "                                                                     'input_SYN Flag Count[0][0]',\n",
      "                                                                     'input_Bwd Header Length[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'input_Fwd IAT Std[0][0]',   \n",
      "                                                                     'input_Idle Std[0][0]',      \n",
      "                                                                     'input_Bwd Segment Size Avg[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'input_Total TCP Flow Time[0]\n",
      "                                                                    [0]',                         \n",
      "                                                                     'input_Bwd Packet Length Mean\n",
      "                                                                    [0][0]',                      \n",
      "                                                                     'input_Packet Length Mean[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'input_Fwd IAT Min[0][0]',   \n",
      "                                                                     'input_Flow IAT Max[0][0]',  \n",
      "                                                                     'input_Average Packet Size[0]\n",
      "                                                                    [0]',                         \n",
      "                                                                     'input_Bwd Bulk Rate Avg[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'input_ACK Flag Count[0][0]',\n",
      "                                                                     'input_CWR Flag Count[0][0]',\n",
      "                                                                     'input_PSH Flag Count[0][0]',\n",
      "                                                                     'input_Packet Length Max[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'input_Flow Duration[0][0]', \n",
      "                                                                     'input_Down/Up Ratio[0][0]', \n",
      "                                                                     'input_Flow IAT Std[0][0]',  \n",
      "                                                                     'input_Bwd Packets/s[0][0]', \n",
      "                                                                     'input_Flow Packets/s[0][0]',\n",
      "                                                                     'input_Flow IAT Mean[0][0]', \n",
      "                                                                     'input_Src Port[0][0]',      \n",
      "                                                                     'input_Dst Port[0][0]',      \n",
      "                                                                     'input_Protocol[0][0]',      \n",
      "                                                                     'input_Fwd PSH Flags[0][0]', \n",
      "                                                                     'input_Bwd PSH Flags[0][0]', \n",
      "                                                                     'input_Fwd URG Flags[0][0]', \n",
      "                                                                     'input_Bwd URG Flags[0][0]', \n",
      "                                                                     'input_Fwd RST Flags[0][0]', \n",
      "                                                                     'input_Bwd RST Flags[0][0]', \n",
      "                                                                     'input_ICMP Code[0][0]',     \n",
      "                                                                     'input_ICMP Type[0][0]']     \n",
      "                                                                                                  \n",
      " embed (Dense)               (None, 8, 64)                16768     ['feature_concat[0][0]']      \n",
      "                                                                                                  \n",
      " block_0_transformer_encode  (None, 8, 64)                83200     ['embed[0][0]']               \n",
      " r (TransformerEncoderBlock                                                                       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_1_transformer_encode  (None, 8, 64)                83200     ['block_0_transformer_encoder[\n",
      " r (TransformerEncoderBlock                                         0][0]']                       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " slice_last (Lambda)         (None, 64)                   0         ['block_1_transformer_encoder[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " classification_mlp_0_128 (  (None, 128)                  8320      ['slice_last[0][0]']          \n",
      " Dense)                                                                                           \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 128)                  0         ['classification_mlp_0_128[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " binary_classification_out   (None, 1)                    129       ['dropout_8[0][0]']           \n",
      " (Dense)                                                                                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 191617 (748.50 KB)\n",
      "Trainable params: 191617 (748.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Building eval dataset...\n",
      "Splitting dataset to featurewise...\n",
      "Evaluation dataset is built!\n",
      "Positive samples in eval set: 6299\n",
      "Negative samples in eval set: 0\n",
      "Legit indice size print:  0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'a' cannot be empty unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[209], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Get the evaluation results\u001b[39;00m\n\u001b[0;32m     42\u001b[0m eval_results: pd\u001b[38;5;241m.\u001b[39mDataFrame\n\u001b[1;32m---> 43\u001b[0m (train_results, eval_results, final_epoch) \u001b[38;5;241m=\u001b[39m \u001b[43mft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(eval_results)\n",
      "Cell \u001b[1;32mIn[208], line 462\u001b[0m, in \u001b[0;36mFlowTransformer.evaluate\u001b[1;34m(self, m, batch_size, early_stopping_patience, epochs, steps_per_epoch)\u001b[0m\n\u001b[0;32m    460\u001b[0m has_reduced_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps_per_epoch):\n\u001b[1;32m--> 462\u001b[0m     batch_X, batch_y \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_yielder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    464\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    465\u001b[0m     batch_results \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mtrain_on_batch(batch_X, batch_y)\n",
      "Cell \u001b[1;32mIn[208], line 425\u001b[0m, in \u001b[0;36mFlowTransformer.evaluate.<locals>.BatchYielder.get_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    418\u001b[0m malicious_indices_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrs\u001b[38;5;241m.\u001b[39mchoice(malicious_indices_train, size\u001b[38;5;241m=\u001b[39mn_malicious_per_batch,\n\u001b[0;32m    419\u001b[0m                                          replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \\\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom \u001b[38;5;28;01melse\u001b[39;00m \\\n\u001b[0;32m    421\u001b[0m     malicious_indices_train[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcursor_malicious:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcursor_malicious \u001b[38;5;241m+\u001b[39m n_malicious_per_batch]\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLegit indice size print:  \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(legit_indices_train)))\n\u001b[1;32m--> 425\u001b[0m legitimate_indices_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlegit_indices_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_legit_per_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \\\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom \u001b[38;5;28;01melse\u001b[39;00m \\\n\u001b[0;32m    427\u001b[0m     legit_indices_train[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcursor_legit:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcursor_legit \u001b[38;5;241m+\u001b[39m n_legit_per_batch]\n\u001b[0;32m    429\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([malicious_indices_batch, legitimate_indices_batch])\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcursor_malicious \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcursor_malicious \u001b[38;5;241m+\u001b[39m n_malicious_per_batch\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:951\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 'a' cannot be empty unless no samples are taken"
     ]
    }
   ],
   "source": [
    "encodings = [\n",
    "    RecordLevelEmbed(64, project=True)\n",
    "]\n",
    "\n",
    "classification_heads = [\n",
    "    LastTokenClassificationHead()\n",
    "]\n",
    "\n",
    "transformers: List[FunctionalComponent] = [\n",
    "    BasicTransformer(2, 128, n_heads=2),\n",
    "    GPTSmallTransformer()\n",
    "]\n",
    "\n",
    "flow_file_path = r\"dataset/\"\n",
    "\n",
    "#### TODO: WHAT DOES ONE FILE MEAN HERE?\n",
    "datasets = [\n",
    "    (\"CSE_CIC_IDS\", os.path.join(flow_file_path, \"CSECICIDS2018_improved\\Friday-02-03-2018.csv\"), NamedDatasetSpecifications.cse_cic_ids_2018_improved, 0.01, EvaluationDatasetSampling.LastRows)\n",
    "    ]\n",
    "\n",
    "pre_processing = StandardPreProcessing(n_categorical_levels=32)\n",
    "\n",
    "# Define the transformer\n",
    "ft = FlowTransformer(pre_processing=pre_processing,\n",
    "                     input_encoding=encodings[0],\n",
    "                     sequential_model=transformers[0],\n",
    "                     classification_head=classification_heads[0],\n",
    "                     params=FlowTransformerParameters(window_size=8, mlp_layer_sizes=[128], mlp_dropout=0.1))\n",
    "\n",
    "# Load the specific dataset\n",
    "dataset_name, dataset_path, dataset_specification, eval_percent, eval_method = datasets[0]\n",
    "ft.load_dataset(dataset_name, dataset_path, dataset_specification, evaluation_dataset_sampling=eval_method, evaluation_percent=eval_percent)\n",
    "\n",
    "# Build the transformer model\n",
    "m = ft.build_model()\n",
    "m.summary()\n",
    "\n",
    "# Compile the model\n",
    "m.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['binary_accuracy'], jit_compile=True)\n",
    "\n",
    "# Get the evaluation results\n",
    "eval_results: pd.DataFrame\n",
    "(train_results, eval_results, final_epoch) = ft.evaluate(m, batch_size=128, epochs=5, steps_per_epoch=64, early_stopping_patience=5)\n",
    "\n",
    "\n",
    "print(eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
