{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10171705,"sourceType":"datasetVersion","datasetId":6282164}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport gc\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:02:09.431003Z","iopub.execute_input":"2024-12-12T07:02:09.431368Z","iopub.status.idle":"2024-12-12T07:02:09.487795Z","shell.execute_reply.started":"2024-12-12T07:02:09.431330Z","shell.execute_reply":"2024-12-12T07:02:09.486123Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/very-downsampled-csecicids/downsampled_original_ratio.csv\n/kaggle/input/very-downsampled-csecicids/downsampled_custom_ratio.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"#### imports","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom typing import List, Dict, Any, Union, Optional, Tuple","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:02:09.489296Z","iopub.execute_input":"2024-12-12T07:02:09.489728Z","iopub.status.idle":"2024-12-12T07:02:22.681811Z","shell.execute_reply.started":"2024-12-12T07:02:09.489680Z","shell.execute_reply":"2024-12-12T07:02:22.681120Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"#### Preprocess","metadata":{}},{"cell_type":"code","source":"class DataPreprocessor:\n    def __init__(self):\n        self.feature_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n        self.target_encoder = OneHotEncoder(sparse=False)\n        \n    def fit_transform(self, X: pd.DataFrame, y: pd.Series) -> tuple:\n        X_encoded = self.feature_encoder.fit_transform(X)\n        y_encoded = self.target_encoder.fit_transform(y.values.reshape(-1, 1))\n        return X_encoded, y_encoded\n    \n    def transform(self, X: pd.DataFrame, y: Optional[pd.Series] = None) -> Union[np.ndarray, tuple]:\n        X_encoded = self.feature_encoder.transform(X)\n        if y is not None:\n            y_encoded = self.target_encoder.transform(y.values.reshape(-1, 1))\n            return X_encoded, y_encoded\n        return X_encoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:02:22.683029Z","iopub.execute_input":"2024-12-12T07:02:22.683567Z","iopub.status.idle":"2024-12-12T07:02:22.689398Z","shell.execute_reply.started":"2024-12-12T07:02:22.683541Z","shell.execute_reply":"2024-12-12T07:02:22.688510Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"#### multi head attention","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model: int, num_heads: int):\n        super().__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n        \n        assert d_model % num_heads == 0\n        \n        self.depth = d_model // num_heads\n        \n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n        \n        self.dense = tf.keras.layers.Dense(d_model)\n    \n    def split_heads(self, x: tf.Tensor, batch_size: int) -> tf.Tensor:\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n    \n    def call(self, v: tf.Tensor, k: tf.Tensor, q: tf.Tensor, mask: Optional[tf.Tensor] = None) -> tf.Tensor:\n        batch_size = tf.shape(q)[0]\n        \n        q = self.wq(q)\n        k = self.wk(k)\n        v = self.wv(v)\n        \n        q = self.split_heads(q, batch_size)\n        k = self.split_heads(k, batch_size)\n        v = self.split_heads(v, batch_size)\n        \n        scaled_attention = self.scaled_dot_product_attention(q, k, v, mask)\n        \n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n        \n        output = self.dense(concat_attention)\n        return output\n    \n    def scaled_dot_product_attention(self, q: tf.Tensor, k: tf.Tensor, v: tf.Tensor, \n                                   mask: Optional[tf.Tensor] = None) -> tf.Tensor:\n        matmul_qk = tf.matmul(q, k, transpose_b=True)\n        \n        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n        \n        if mask is not None:\n            scaled_attention_logits += (mask * -1e9)\n        \n        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n        output = tf.matmul(attention_weights, v)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:02:22.690682Z","iopub.execute_input":"2024-12-12T07:02:22.690940Z","iopub.status.idle":"2024-12-12T07:02:22.774078Z","shell.execute_reply.started":"2024-12-12T07:02:22.690916Z","shell.execute_reply":"2024-12-12T07:02:22.773278Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"#### Transformer block","metadata":{}},{"cell_type":"code","source":"class TransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, d_model: int, num_heads: int, dff: int, dropout_rate: float = 0.1):\n        super().__init__()\n        \n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = tf.keras.Sequential([\n            tf.keras.layers.Dense(dff, activation='relu'),\n            tf.keras.layers.Dense(d_model)\n        ])\n        \n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        \n        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n    \n    def call(self, x: tf.Tensor, training: bool = False, mask: Optional[tf.Tensor] = None) -> tf.Tensor:\n        attn_output = self.mha(x, x, x, mask)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)\n        \n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:02:22.776072Z","iopub.execute_input":"2024-12-12T07:02:22.776421Z","iopub.status.idle":"2024-12-12T07:02:22.783450Z","shell.execute_reply.started":"2024-12-12T07:02:22.776386Z","shell.execute_reply":"2024-12-12T07:02:22.782557Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"#### Classifier","metadata":{}},{"cell_type":"code","source":"class TransformerClassifier(tf.keras.Model):\n    def __init__(self, \n                 input_dim: int,\n                 num_classes: int,\n                 d_model: int = 128,\n                 num_layers: int = 4,\n                 num_heads: int = 8,\n                 dff: int = 512,\n                 dropout_rate: float = 0.1):\n        super().__init__()\n        \n        self.d_model = d_model\n        self.num_layers = num_layers\n        \n        self.embedding = tf.keras.layers.Dense(d_model)\n        self.pos_encoding = self.positional_encoding(input_dim, d_model)\n        \n        self.transformer_blocks = [\n            TransformerBlock(d_model, num_heads, dff, dropout_rate)\n            for _ in range(num_layers)\n        ]\n        \n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n        self.final_layer = tf.keras.layers.Dense(num_classes, activation='softmax')\n    \n    def positional_encoding(self, position: int, d_model: int) -> tf.Tensor:\n        angle_rads = self.get_angles(\n            np.arange(position)[:, np.newaxis],\n            np.arange(d_model)[np.newaxis, :],\n            d_model\n        )\n        \n        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n        \n        pos_encoding = angle_rads[np.newaxis, ...]\n        return tf.cast(pos_encoding, dtype=tf.float32)\n    \n    def get_angles(self, pos: np.ndarray, i: np.ndarray, d_model: int) -> np.ndarray:\n        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n        return pos * angle_rates\n    \n    def call(self, x: tf.Tensor, training: bool = False) -> tf.Tensor:\n        x = self.embedding(x)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :tf.shape(x)[1], :]\n        \n        x = self.dropout(x, training=training)\n        \n        for transformer_block in self.transformer_blocks:\n            x = transformer_block(x, training=training)\n        \n        # Global average pooling\n        x = tf.reduce_mean(x, axis=1)\n        \n        return self.final_layer(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:02:22.784467Z","iopub.execute_input":"2024-12-12T07:02:22.784718Z","iopub.status.idle":"2024-12-12T07:02:22.800818Z","shell.execute_reply.started":"2024-12-12T07:02:22.784694Z","shell.execute_reply":"2024-12-12T07:02:22.799979Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"#### Metrics","metadata":{}},{"cell_type":"code","source":"class ModelEvaluator:\n    def __init__(self, model: tf.keras.Model, preprocessor: DataPreprocessor):\n        self.model = model\n        self.preprocessor = preprocessor\n        \n    def evaluate(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:\n        \"\"\"\n        Evaluate the model and return various metrics\n        \"\"\"\n        # Transform data\n        X_encoded, y_encoded = self.preprocessor.transform(X, y)\n        \n        # Get predictions\n        y_pred_proba = self.model.predict(X_encoded)\n        y_pred = np.argmax(y_pred_proba, axis=1)\n        y_true = np.argmax(y_encoded, axis=1)\n        \n        # Get class labels\n        class_labels = self.preprocessor.target_encoder.categories_[0]\n        \n        # Calculate metrics\n        metrics = {}\n        \n        # Basic metrics\n        metrics['accuracy'] = np.mean(y_pred == y_true)\n        metrics['f1_micro'] = f1_score(y_true, y_pred, average='micro')\n        metrics['f1_macro'] = f1_score(y_true, y_pred, average='macro')\n        metrics['f1_weighted'] = f1_score(y_true, y_pred, average='weighted')\n        \n        # Confusion matrix\n        metrics['confusion_matrix'] = confusion_matrix(y_true, y_pred)\n        \n        # Classification report\n        metrics['classification_report'] = classification_report(\n            y_true, \n            y_pred, \n            target_names=class_labels,\n            output_dict=True\n        )\n        \n        return metrics\n    \n    def plot_confusion_matrix(self, X: pd.DataFrame, y: pd.Series, \n                            figsize: Tuple[int, int] = (10, 8)) -> None:\n        \"\"\"\n        Plot confusion matrix using seaborn\n        \"\"\"\n        metrics = self.evaluate(X, y)\n        cm = metrics['confusion_matrix']\n        class_labels = self.preprocessor.target_encoder.categories_[0]\n        \n        plt.figure(figsize=figsize)\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                   xticklabels=class_labels,\n                   yticklabels=class_labels)\n        plt.title('Confusion Matrix')\n        plt.ylabel('True Label')\n        plt.xlabel('Predicted Label')\n        plt.tight_layout()\n        plt.show()\n    \n    def print_metrics(self, X: pd.DataFrame, y: pd.Series) -> None:\n        \"\"\"\n        Print all metrics in a formatted way\n        \"\"\"\n        metrics = self.evaluate(X, y)\n        \n        print(\"\\n=== Model Evaluation Metrics ===\\n\")\n        \n        print(\"Overall Metrics:\")\n        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n        print(f\"F1 Score (Micro): {metrics['f1_micro']:.4f}\")\n        print(f\"F1 Score (Macro): {metrics['f1_macro']:.4f}\")\n        print(f\"F1 Score (Weighted): {metrics['f1_weighted']:.4f}\")\n        \n        print(\"\\nDetailed Classification Report:\")\n        report = metrics['classification_report']\n        \n        # Print metrics for each class\n        for class_name in self.preprocessor.target_encoder.categories_[0]:\n            if class_name in report:\n                class_metrics = report[class_name]\n                print(f\"\\nClass: {class_name}\")\n                print(f\"Precision: {class_metrics['precision']:.4f}\")\n                print(f\"Recall: {class_metrics['recall']:.4f}\")\n                print(f\"F1-Score: {class_metrics['f1-score']:.4f}\")\n                print(f\"Support: {class_metrics['support']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:02:22.801900Z","iopub.execute_input":"2024-12-12T07:02:22.802209Z","iopub.status.idle":"2024-12-12T07:02:22.816989Z","shell.execute_reply.started":"2024-12-12T07:02:22.802184Z","shell.execute_reply":"2024-12-12T07:02:22.816321Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"#### Model create","metadata":{}},{"cell_type":"code","source":"def create_and_train_model(\n    X: pd.DataFrame,\n    y: pd.Series,\n    model_params: Dict[str, Any] = None,\n    training_params: Dict[str, Any] = None\n) -> Tuple[TransformerClassifier, DataPreprocessor, ModelEvaluator]:\n    \"\"\"\n    Creates, trains and returns the model, preprocessor, and evaluator\n    \"\"\"\n    # Default parameters\n    default_model_params = {\n        'd_model': 128,\n        'num_layers': 4,\n        'num_heads': 8,\n        'dff': 512,\n        'dropout_rate': 0.1\n    }\n    \n    default_training_params = {\n        'batch_size': 32,\n        'epochs': 10,\n        'validation_split': 0.2,\n        'learning_rate': 0.001\n    }\n    \n    # Update defaults with provided parameters\n    model_params = {**default_model_params, **(model_params or {})}\n    training_params = {**default_training_params, **(training_params or {})}\n    \n    # Preprocess data\n    preprocessor = DataPreprocessor()\n    X_encoded, y_encoded = preprocessor.fit_transform(X, y)\n    \n    # Create model\n    model = TransformerClassifier(\n        input_dim=X_encoded.shape[1],\n        num_classes=y_encoded.shape[1],\n        **model_params\n    )\n    \n    # Compile model\n    optimizer = tf.keras.optimizers.Adam(learning_rate=training_params['learning_rate'])\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    # Train model\n    model.fit(\n        X_encoded,\n        y_encoded,\n        batch_size=training_params['batch_size'],\n        epochs=training_params['epochs'],\n        validation_split=training_params['validation_split']\n    )\n    \n    # Create evaluator\n    evaluator = ModelEvaluator(model, preprocessor)\n    \n    return model, preprocessor, evaluator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:02:22.817898Z","iopub.execute_input":"2024-12-12T07:02:22.818245Z","iopub.status.idle":"2024-12-12T07:02:22.828988Z","shell.execute_reply.started":"2024-12-12T07:02:22.818176Z","shell.execute_reply":"2024-12-12T07:02:22.828295Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"#### Main","metadata":{}},{"cell_type":"code","source":"# 1. Data Preparation\nprint(\"1. Preparing Data...\")\ndf = pd.read_csv(\"/kaggle/input/very-downsampled-csecicids/downsampled_original_ratio.csv\")\nprint(df.shape)\ny = df['Attack']\nX = df.drop(['Attack'], axis=1)\n\ndel df\ngc.collect()\n\n# 2. Train-Test Split\nprint(\"\\n2. Splitting Data...\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\nprint(f\"Training set size: {len(X_train)}\")\nprint(f\"Test set size: {len(X_test)}\")\n\n# 3. Model Configuration\nprint(\"\\n3. Configuring Model...\")\nmodel_params = {\n    'd_model': 128,        # Smaller model for demo\n    'num_layers': 2,      # Fewer layers for faster training\n    'num_heads': 2,\n    'dff': 128,\n    'dropout_rate': 0.3\n}\n\ntraining_params = {\n    'batch_size': 32,\n    'epochs': 10,          # Fewer epochs for demo\n    'validation_split': 0.2,\n    'learning_rate': 0.001\n}\n\nprint(\"Model parameters:\", model_params)\nprint(\"Training parameters:\", training_params)\n\n# 4. Model Training\nprint(\"\\n4. Training Model...\")\nmodel, preprocessor, evaluator = create_and_train_model(\n    X_train,\n    y_train,\n    model_params=model_params,\n    training_params=training_params\n)\n\n# 5. Model Evaluation\nprint(\"\\n5. Evaluating Model...\")\n\n# Print detailed metrics\nprint(\"\\nTraining Set Metrics:\")\nevaluator.print_metrics(X_train, y_train)\n\nprint(\"\\nTest Set Metrics:\")\nevaluator.print_metrics(X_test, y_test)\n\n# Plot confusion matrices\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 2, 1)\nevaluator.plot_confusion_matrix(X_train, y_train, figsize=(7, 5))\nplt.title(\"Training Set Confusion Matrix\")\n\nplt.subplot(1, 2, 2)\nevaluator.plot_confusion_matrix(X_test, y_test, figsize=(7, 5))\nplt.title(\"Test Set Confusion Matrix\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:03:25.910676Z","iopub.execute_input":"2024-12-12T07:03:25.911544Z"}},"outputs":[{"name":"stdout","text":"1. Preparing Data...\n(911627, 45)\n\n2. Splitting Data...\nTraining set size: 729301\nTest set size: 182326\n\n3. Configuring Model...\nModel parameters: {'d_model': 128, 'num_layers': 2, 'num_heads': 2, 'dff': 128, 'dropout_rate': 0.3}\nTraining parameters: {'batch_size': 32, 'epochs': 10, 'validation_split': 0.2, 'learning_rate': 0.001}\n\n4. Training Model...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null}]}