{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import pickle \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.tree import plot_tree\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_classif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (BertTokenizer, BertForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification,\n",
    "                          RobertaTokenizer, RobertaForSequenceClassification, ElectraTokenizer, ElectraForSequenceClassification,\n",
    "                          AutoTokenizer, AutoModelForSequenceClassification,AdamW)\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from tqdm import tqdm  # For the progress bar\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions defined\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "def label_mapper(label):\n",
    "    # BENIGN, web_attack, ddos, botnet, infiltration, dos, ssh_bruteforce, ftp_bruteforce, others\n",
    "    if label == 'Benign':\n",
    "        return 0\n",
    "    elif label.startswith(\"Brute Force\"):\n",
    "        return 1\n",
    "    elif label.startswith(\"DDoS\") or label.startswith(\"DDOS\"):\n",
    "        return 2\n",
    "    elif label.startswith(\"Bot\"):\n",
    "        return 3\n",
    "    elif label.startswith(\"Infilteration\") :\n",
    "        return 4\n",
    "    elif label.startswith(\"DoS\"):\n",
    "        return 5\n",
    "    elif label.startswith(\"SSH\"):\n",
    "        return 6\n",
    "    elif label.startswith(\"FTP\"):\n",
    "        return 7\n",
    "    elif label.startswith(\"SQL\"):\n",
    "        return 8\n",
    "    else:\n",
    "        return 9\n",
    "    \n",
    "\n",
    "def dataset_cleaner(df, label_column):\n",
    "\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Clean the dataset by handling missing values, scaling numerical features, and \n",
    "    applying one-hot encoding to categorical features, while leaving the label class intact.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame, the input DataFrame.\n",
    "    - label_column: str, the name of the target (label) column to be preserved.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with cleaned features and the label column intact.\n",
    "    \"\"\"   \n",
    "    # Separate label column from features\n",
    "    label = df[label_column]\n",
    "    df = df.drop(columns=[label_column])\n",
    "    \n",
    "    # Drop all inf and other NaN values\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "    # Define numerical and categorical columns\n",
    "    numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # 1. Handle numerical columns (impute missing values and scale)\n",
    "    if numerical_cols:\n",
    "        # Imputation for numerical columns\n",
    "        numerical_imputer = SimpleImputer(strategy='mean')\n",
    "        df[numerical_cols] = numerical_imputer.fit_transform(df[numerical_cols])\n",
    "\n",
    "        # Scaling numerical columns\n",
    "        scaler = StandardScaler()\n",
    "        df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "    if categorical_cols:\n",
    "        # 2. Handle categorical columns (impute missing values and apply one-hot encoding)\n",
    "        # Imputation for categorical columns\n",
    "        categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        df[categorical_cols] = categorical_imputer.fit_transform(df[categorical_cols])\n",
    "\n",
    "        df = pd.get_dummies(df)\n",
    "        print(\"hot enconding done\")\n",
    "\n",
    "    # Re-add the label column\n",
    "    df[label_column] = label\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_target_features(df, test_size = 0.30):\n",
    "    X = df.drop(['Label'], axis=1)\n",
    "    y = df['Label']\n",
    "    \n",
    "    return train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "print(\"Functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_fix(df):\n",
    "    df['Label'] = df['Attack'].apply(label_mapper)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "def select_features_correlation(X, y, k=10):\n",
    "    \"\"\"\n",
    "    Select top k features based on correlation with the target.\n",
    "\n",
    "    Parameters:\n",
    "    - X: DataFrame, the feature matrix.\n",
    "    - y: Series, the target variable.\n",
    "    - k: int, number of top features to select.\n",
    "\n",
    "    Returns:\n",
    "    - List of top k features based on correlation.\n",
    "    \"\"\"\n",
    "    # Factorize the target and convert to pandas Series\n",
    "    y_factorized = pd.Series(y.factorize()[0], index=y.index)\n",
    "    \n",
    "    # Convert categorical features to numeric\n",
    "    X_encoded = X.apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    # Compute correlation\n",
    "    correlation = X_encoded.corrwith(y_factorized)\n",
    "    \n",
    "    # Create a DataFrame for correlation values\n",
    "    correlation_df = pd.DataFrame({\n",
    "        'Feature': correlation.index,\n",
    "        'Correlation': correlation.values\n",
    "    }).sort_values(by='Correlation', key=abs, ascending=False)\n",
    "    \n",
    "    # Select top k features\n",
    "    return correlation_df.head(k)['Feature'].tolist()\n",
    "\n",
    "def select_features_information_gain(X, y, k=10):\n",
    "    \"\"\"\n",
    "    Select top k features based on information gain with the target.\n",
    "\n",
    "    Parameters:\n",
    "    - X: DataFrame, the feature matrix.\n",
    "    - y: Series, the target variable.\n",
    "    - k: int, number of top features to select.\n",
    "\n",
    "    Returns:\n",
    "    - List of top k features based on information gain.\n",
    "    \"\"\"\n",
    "    # Convert categorical features to numeric\n",
    "    X_encoded = X.apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    # Calculate information gain\n",
    "    info_gain = mutual_info_classif(X_encoded, y, discrete_features='auto', random_state=42)\n",
    "    \n",
    "    # Create a DataFrame for information gain values\n",
    "    info_gain_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Information Gain': info_gain\n",
    "    }).sort_values(by='Information Gain', ascending=False)\n",
    "    \n",
    "    # Select top k features\n",
    "    return info_gain_df.head(k)['Feature'].tolist()\n",
    "\n",
    "def get_top_features_across_files(file_paths, label_column, k=10):\n",
    "    \"\"\"\n",
    "    Get top k features across multiple files, based on both correlation and information gain.\n",
    "\n",
    "    Parameters:\n",
    "    - file_paths: list of file paths.\n",
    "    - label_column: name of the label column.\n",
    "    - k: number of top features to select.\n",
    "\n",
    "    Returns:\n",
    "    - List of top k features consistent across all files.\n",
    "    \"\"\"\n",
    "    feature_counter = Counter()\n",
    "    idx = 1\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path)\n",
    "        X = df.drop(columns=[label_column])\n",
    "        y = df[label_column]\n",
    "        \n",
    "        # Select top features using both methods\n",
    "        top_corr_features = select_features_correlation(X, y, k)\n",
    "        top_ig_features = select_features_information_gain(X, y, k)\n",
    "        \n",
    "        # Combine and count features\n",
    "        feature_counter.update(top_corr_features + top_ig_features)\n",
    "        print(\"File \"+str(idx)+\" done.\")\n",
    "        idx = idx+1\n",
    "    \n",
    "    # Get most common features across files\n",
    "    most_common_features = feature_counter.most_common(k)\n",
    "    return [feature for feature, _ in most_common_features]\n",
    "\n",
    "def keep_selected_features(df, selected_features, label_column):\n",
    "    \"\"\"\n",
    "    Keep only the selected features in the given DataFrame, while leaving the label column intact.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame, the input DataFrame.\n",
    "    - selected_features: list of feature names to keep.\n",
    "    - label_column: str, the name of the target (label) column to be preserved.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with only the selected features and the label column.\n",
    "    \"\"\"\n",
    "    # Extract the label column\n",
    "    label = df[label_column]\n",
    "    \n",
    "    # Drop the label column from the selected features if it's included\n",
    "    if label_column in selected_features:\n",
    "        selected_features.remove(label_column)\n",
    "    \n",
    "    # Keep only the selected features\n",
    "    updated_df = df[selected_features]\n",
    "    \n",
    "    # Re-add the label column\n",
    "    updated_df[label_column] = label\n",
    "    \n",
    "    return updated_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = ['Attack', 'TCP_WIN_MAX_IN', 'TCP_FLAGS', 'L7_PROTO', 'MAX_IP_PKT_LEN', 'LONGEST_FLOW_PKT', 'MAX_TTL', 'MIN_TTL', 'SERVER_TCP_FLAGS', 'FLOW_DURATION_MILLISECONDS', 'CLIENT_TCP_FLAGS', 'PROTOCOL', 'DNS_TTL_ANSWER', 'DNS_QUERY_ID', 'MIN_IP_PKT_LEN', 'DNS_QUERY_TYPE', 'SHORTEST_FLOW_PKT', 'DURATION_OUT', 'DURATION_IN']\n",
    "top_features.remove('Attack')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature to Strings Convertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_conversion(df, label_column, format_type=\"concatenated\"):\n",
    "        \"\"\"\n",
    "        Converts numerical data into textual representations and optionally concatenates them with labels.\n",
    "\n",
    "        Parameters:\n",
    "        df (DataFrame): The input DataFrame containing features and labels.\n",
    "        label_column (str): The name of the label column.\n",
    "        format_type (str): The format for conversion. Options:\n",
    "                        - \"concatenated\": Concatenate feature values with underscores.\n",
    "                        - \"key_value\": Convert feature-value pairs into text (e.g., feature1: 23.5).\n",
    "\n",
    "        Returns:\n",
    "        DataFrame: A DataFrame with transformed features and labels.\n",
    "        \"\"\"\n",
    "        # Select non-label columns\n",
    "        non_label_columns = df.drop(columns=[label_column]).columns\n",
    "\n",
    "        if format_type == \"concatenated\":\n",
    "            # Convert numerical values to strings and concatenate them with underscores\n",
    "            df['transformed'] = df[non_label_columns].astype(str).apply(lambda row: '_'.join(row), axis=1)\n",
    "\n",
    "        elif format_type == \"key_value\":\n",
    "            # Convert each feature-value pair into key-value text\n",
    "            df['transformed'] = df[non_label_columns].apply(\n",
    "                lambda row: ' '.join([f\"{col}: {row[col]}\" for col in non_label_columns]), axis=1\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid format_type. Choose either 'concatenated' or 'key_value'.\")\n",
    "\n",
    "        # Add the label column to the result\n",
    "        relevant_columns = ['transformed', label_column]\n",
    "        updated_df = df[relevant_columns]\n",
    "\n",
    "        return updated_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Load & Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED FILE , STARTING CLEANUP\n",
      "\n",
      "                                         transformed  Label\n",
      "0  TCP_WIN_MAX_IN: -0.27484193282420133 TCP_FLAGS...      0\n",
      "1  TCP_WIN_MAX_IN: -0.9017693410619491 TCP_FLAGS:...      0\n",
      "2  TCP_WIN_MAX_IN: -0.9017693410619491 TCP_FLAGS:...      0\n",
      "3  TCP_WIN_MAX_IN: -0.5890881034191101 TCP_FLAGS:...      0\n",
      "4  TCP_WIN_MAX_IN: 1.5996423909843704 TCP_FLAGS: ...      2\n",
      "CLEANUP DONE, STARTING SPLIT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df = pd.read_csv('dataset/downsampled/final_downsampled_dataset.csv') \n",
    "df = pd.read_csv('dataset/downsampled/final_downsampled_dataset_5050ratio.csv') \n",
    "print(f\"LOADED FILE , STARTING CLEANUP\\n\")\n",
    "\n",
    "#df =label_fix(df)\n",
    "df.rename(columns={'Attack': 'Label'}, inplace=True)\n",
    "\n",
    "\n",
    "#df = keep_selected_features(df,top_features,\"Label\")\n",
    "    \n",
    "df = dataset_cleaner(df,\"Label\")\n",
    "\n",
    "#df = string_conversion(df,\"Label\",\"concatenated\")  \n",
    "df = string_conversion(df,\"Label\",\"key_value\")\n",
    "print(df.head())  \n",
    " \n",
    "print(f\"CLEANUP DONE, STARTING SPLIT\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label_encoded'] = label_encoder.fit_transform(df['Label'])\n",
    "\n",
    "# Train-test split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['transformed'], df['label_encoded'], test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configuration\n",
    "def load_model_and_tokenizer(model_name, num_classes):\n",
    "    if model_name == \"bert\":\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)\n",
    "    elif model_name == \"distilbert\":\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_classes)\n",
    "    elif model_name == \"roberta\":\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_classes)\n",
    "    elif model_name == \"electra\":\n",
    "        tokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')\n",
    "        model = ElectraForSequenceClassification.from_pretrained('google/electra-base-discriminator', num_labels=num_classes)\n",
    "    elif model_name == \"spanbert\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained('SpanBERT/spanbert-base-cased')\n",
    "        model = AutoModelForSequenceClassification.from_pretrained('SpanBERT/spanbert-base-cased', num_labels=num_classes)\n",
    "    elif model_name == \"codebert\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained('microsoft/codebert-base')\n",
    "        model = AutoModelForSequenceClassification.from_pretrained('microsoft/codebert-base', num_labels=num_classes)\n",
    "    elif model_name == \"gpt2\":\n",
    "        # Load GPT-2 tokenizer and add padding token\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token  # Use the EOS token as the padding token \n",
    "        model = GPT2ForSequenceClassification.from_pretrained(\n",
    "            \"gpt2\", \n",
    "            num_labels=num_classes)\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    elif model_name == \"gpt-neo\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-1.3B')\n",
    "        model = AutoModelForSequenceClassification.from_pretrained('EleutherAI/gpt-neo-1.3B', num_labels=num_classes)\n",
    "    elif model_name == \"gpt-j\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-j-6B')\n",
    "        model = AutoModelForSequenceClassification.from_pretrained('EleutherAI/gpt-j-6B', num_labels=num_classes)\n",
    "    elif model_name == \"opt\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained('facebook/opt-350m')\n",
    "        model = AutoModelForSequenceClassification.from_pretrained('facebook/opt-350m', num_labels=num_classes)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model name. Choose from: bert, distilbert, roberta, electra, spanbert, codebert, gpt2, gpt-neo, gpt-j, opt\")\n",
    "\n",
    "    return tokenizer, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"bert\"    \n",
    "# num_classes = len(label_encoder.classes_)\n",
    "# tokenizer, model = load_model_and_tokenizer(model_name, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create datasets and dataloaders\n",
    "# train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "# val_dataset = TextDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "\n",
    "\n",
    "# # Optimizer\n",
    "# optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "# print(num_classes)\n",
    "# print(label_encoder.classes_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer, device):\n",
    "    model = model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Initialize the tqdm progress bar\n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\", total=len(data_loader))\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the loss and accuracy\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Update progress bar with the loss\n",
    "        progress_bar.set_postfix(loss=total_loss / (total + len(labels)), accuracy=correct / total)\n",
    "\n",
    "    # Return average loss and accuracy\n",
    "    return total_loss / len(data_loader), correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conf_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrix(all_labels, all_preds, save_path=None, title=None):\n",
    "    \"\"\"\n",
    "    Generates and plots the confusion matrix using the classes from all_labels and all_preds.\n",
    "    Optionally, adds a title and saves the plot to the specified file path.\n",
    "\n",
    "    Parameters:\n",
    "    - all_labels: list of true labels\n",
    "    - all_preds: list of predicted labels\n",
    "    - save_path: str (optional) path to save the confusion matrix plot (e.g., 'confusion_matrix.png')\n",
    "    - title: str (optional) title for the plot\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    unique_labels = list(set(all_labels))\n",
    "\n",
    "    # Display confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=unique_labels)\n",
    "    disp.plot(cmap=\"Blues\", xticks_rotation='vertical')\n",
    "\n",
    "    # Add title if provided\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "\n",
    "    # If a save path is provided, save the plot\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Confusion matrix saved to: {save_path}\")\n",
    "    else:\n",
    "        plt.show()  # If no save path, show the plot instead\n",
    "\n",
    "    plt.close()  # Close the plot to avoid memory issues if running multiple times\n",
    "    \n",
    "    return cm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device, label_encoder,epoch,model_name):\n",
    "    model = model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Initialize the tqdm progress bar\n",
    "    progress_bar = tqdm(data_loader, desc=\"Evaluating\", total=len(data_loader))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Append labels and predictions as lists\n",
    "            all_preds.extend(preds.cpu().numpy().tolist())\n",
    "            all_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "            # Update the progress bar with the current loss and accuracy\n",
    "            progress_bar.set_postfix(loss=total_loss / (total + len(labels)), accuracy=correct / total)\n",
    "\n",
    "    # Ensure all_preds and all_labels are iterable\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Plot the confusion matrix using classes from all_labels and all_preds\n",
    "    cm_title = model_name + \"_epoch_\" + str(epoch)\n",
    "    save_path = \"results/\"+cm_title+\".png\"\n",
    "    cm = plot_confusion_matrix(all_labels, all_preds,save_path=save_path,title=cm_title)\n",
    "\n",
    "    # Calculate metrics directly\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average=None, labels=np.unique(all_labels))\n",
    "    recall = recall_score(all_labels, all_preds, average=None, labels=np.unique(all_labels))\n",
    "    f1 = f1_score(all_labels, all_preds, average=None, labels=np.unique(all_labels))\n",
    "\n",
    "    # You can calculate overall or per-class metrics\n",
    "    overall_precision = np.mean(precision)\n",
    "    overall_recall = np.mean(recall)\n",
    "    overall_f1 = np.mean(f1)\n",
    "\n",
    "    # Prepare metrics\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"overall_precision\": overall_precision,\n",
    "        \"overall_recall\": overall_recall,\n",
    "        \"overall_f1\": overall_f1\n",
    "    }\n",
    "\n",
    "    # Return total loss, accuracy, and metrics\n",
    "    return total_loss / len(data_loader), correct / total, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Training loop\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 3\n",
    "\n",
    "# # Variables to store overall metrics across all epochs\n",
    "# total_val_loss = 0\n",
    "# total_val_acc = 0\n",
    "# total_precision = 0\n",
    "# total_recall = 0\n",
    "# total_f1 = 0\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "    \n",
    "#     # Train for each epoch\n",
    "#     train_loss, train_acc = train_epoch(model, train_loader, optimizer, device)\n",
    "    \n",
    "#     # Evaluate on the validation set for each epoch\n",
    "#     val_loss, val_acc, val_metrics = evaluate_model(model, val_loader, device, label_encoder,(epoch+1),model_name)\n",
    "    \n",
    "#     # Print per-epoch results\n",
    "#     print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "#     print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "#     print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "#     print(val_metrics)\n",
    "    \n",
    "#     # Accumulate validation metrics across all epochs for later averaging\n",
    "#     total_val_loss += val_loss\n",
    "#     total_val_acc += val_acc\n",
    "#     total_precision += val_metrics['overall_precision']\n",
    "#     total_recall += val_metrics['overall_recall']\n",
    "#     total_f1 += val_metrics['overall_f1']\n",
    "\n",
    "# # Calculate and print overall mean metrics\n",
    "# mean_val_loss = total_val_loss / epochs\n",
    "# mean_val_acc = total_val_acc / epochs\n",
    "# mean_precision = total_precision / epochs\n",
    "# mean_recall = total_recall / epochs\n",
    "# mean_f1 = total_f1 / epochs\n",
    "\n",
    "# print(\"\\nOverall Validation Results:\")\n",
    "# print(f\"Mean Validation Loss: {mean_val_loss:.4f}\")\n",
    "# print(f\"Mean Validation Accuracy: {mean_val_acc:.4f}\")\n",
    "# print(f\"Mean Precision: {mean_precision:.4f}\")\n",
    "# print(f\"Mean Recall: {mean_recall:.4f}\")\n",
    "# print(f\"Mean F1 Score: {mean_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Running: bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\tf\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training: 100%|██████████| 1579/1579 [05:00<00:00,  5.26it/s, accuracy=0.658, loss=0.0583]\n",
      "Evaluating: 100%|██████████| 677/677 [00:52<00:00, 12.88it/s, accuracy=0.506, loss=0.079] \n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix saved to: results/bert_epoch_1.png\n",
      "Epoch 1/5\n",
      "Train Loss: 0.9324, Train Accuracy: 0.6584\n",
      "Validation Loss: 1.2638, Validation Accuracy: 0.5058\n",
      "{'accuracy': 0.505774738981798, 'overall_precision': 0.06322184237272475, 'overall_recall': 0.125, 'overall_f1': 0.08397251027796528}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1579/1579 [04:59<00:00,  5.28it/s, accuracy=0.501, loss=0.08]  \n",
      "Evaluating: 100%|██████████| 677/677 [00:52<00:00, 12.89it/s, accuracy=0.506, loss=0.0789]\n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix saved to: results/bert_epoch_2.png\n",
      "Epoch 2/5\n",
      "Train Loss: 1.2789, Train Accuracy: 0.5011\n",
      "Validation Loss: 1.2622, Validation Accuracy: 0.5058\n",
      "{'accuracy': 0.505774738981798, 'overall_precision': 0.06322184237272475, 'overall_recall': 0.125, 'overall_f1': 0.08397251027796528}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1579/1579 [05:00<00:00,  5.25it/s, accuracy=0.504, loss=0.0797]\n",
      "Evaluating: 100%|██████████| 677/677 [00:52<00:00, 12.90it/s, accuracy=0.506, loss=0.0794]\n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix saved to: results/bert_epoch_3.png\n",
      "Epoch 3/5\n",
      "Train Loss: 1.2751, Train Accuracy: 0.5039\n",
      "Validation Loss: 1.2704, Validation Accuracy: 0.5058\n",
      "{'accuracy': 0.505774738981798, 'overall_precision': 0.06322184237272475, 'overall_recall': 0.125, 'overall_f1': 0.08397251027796528}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1579/1579 [05:00<00:00,  5.26it/s, accuracy=0.504, loss=0.0797]\n",
      "Evaluating: 100%|██████████| 677/677 [00:52<00:00, 12.91it/s, accuracy=0.506, loss=0.0792]\n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix saved to: results/bert_epoch_4.png\n",
      "Epoch 4/5\n",
      "Train Loss: 1.2753, Train Accuracy: 0.5040\n",
      "Validation Loss: 1.2667, Validation Accuracy: 0.5058\n",
      "{'accuracy': 0.505774738981798, 'overall_precision': 0.06322184237272475, 'overall_recall': 0.125, 'overall_f1': 0.08397251027796528}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1579/1579 [04:59<00:00,  5.27it/s, accuracy=0.506, loss=0.0797]\n",
      "Evaluating: 100%|██████████| 677/677 [00:52<00:00, 12.92it/s, accuracy=0.506, loss=0.0787]\n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix saved to: results/bert_epoch_5.png\n",
      "Epoch 5/5\n",
      "Train Loss: 1.2740, Train Accuracy: 0.5063\n",
      "Validation Loss: 1.2593, Validation Accuracy: 0.5058\n",
      "{'accuracy': 0.505774738981798, 'overall_precision': 0.06322184237272475, 'overall_recall': 0.125, 'overall_f1': 0.08397251027796528}\n",
      "\n",
      "Overall Validation Results:\n",
      "Mean Validation Loss: 1.2645\n",
      "Mean Validation Accuracy: 0.5058\n",
      "Mean Precision: 0.0632\n",
      "Mean Recall: 0.1250\n",
      "Mean F1 Score: 0.0840\n",
      "Model Running: distilbert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\tf\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training: 100%|██████████| 1579/1579 [02:59<00:00,  8.81it/s, accuracy=0.958, loss=0.0106]\n",
      "Evaluating: 100%|██████████| 677/677 [00:38<00:00, 17.65it/s, accuracy=0.975, loss=0.007]  \n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix saved to: results/distilbert_epoch_1.png\n",
      "Epoch 1/5\n",
      "Train Loss: 0.1703, Train Accuracy: 0.9581\n",
      "Validation Loss: 0.1119, Validation Accuracy: 0.9754\n",
      "{'accuracy': 0.9754227108934677, 'overall_precision': 0.8565987323774852, 'overall_recall': 0.7710468036411324, 'overall_f1': 0.7860621707169709}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1579/1579 [03:01<00:00,  8.72it/s, accuracy=0.97, loss=0.00816] \n",
      "Evaluating: 100%|██████████| 677/677 [00:38<00:00, 17.55it/s, accuracy=0.96, loss=0.0161] \n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix saved to: results/distilbert_epoch_2.png\n",
      "Epoch 2/5\n",
      "Train Loss: 0.1305, Train Accuracy: 0.9703\n",
      "Validation Loss: 0.2576, Validation Accuracy: 0.9601\n",
      "{'accuracy': 0.9600850041578121, 'overall_precision': 0.6417915594301717, 'overall_recall': 0.6290469243639634, 'overall_f1': 0.6348144235060285}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1579/1579 [02:58<00:00,  8.84it/s, accuracy=0.969, loss=0.00844]\n",
      "Evaluating: 100%|██████████| 677/677 [00:38<00:00, 17.58it/s, accuracy=0.865, loss=0.0211]\n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix saved to: results/distilbert_epoch_3.png\n",
      "Epoch 3/5\n",
      "Train Loss: 0.1350, Train Accuracy: 0.9690\n",
      "Validation Loss: 0.3379, Validation Accuracy: 0.8650\n",
      "{'accuracy': 0.8650097015614894, 'overall_precision': 0.46561817484952506, 'overall_recall': 0.49709302325581395, 'overall_f1': 0.4795569526869475}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1579/1579 [02:58<00:00,  8.84it/s, accuracy=0.664, loss=0.0536]\n",
      "Evaluating: 100%|██████████| 677/677 [00:38<00:00, 17.50it/s, accuracy=0.506, loss=0.106]\n",
      "c:\\Users\\ahmed\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix saved to: results/distilbert_epoch_4.png\n",
      "Epoch 4/5\n",
      "Train Loss: 0.8576, Train Accuracy: 0.6643\n",
      "Validation Loss: 1.7002, Validation Accuracy: 0.5058\n",
      "{'accuracy': 0.505774738981798, 'overall_precision': 0.06322184237272475, 'overall_recall': 0.125, 'overall_f1': 0.08397251027796528}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1579/1579 [02:58<00:00,  8.84it/s, accuracy=0.8, loss=0.0441]  \n",
      "Evaluating: 100%|██████████| 677/677 [00:38<00:00, 17.55it/s, accuracy=0.81, loss=0.047]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix saved to: results/distilbert_epoch_5.png\n",
      "Epoch 5/5\n",
      "Train Loss: 0.7051, Train Accuracy: 0.7999\n",
      "Validation Loss: 0.7526, Validation Accuracy: 0.8103\n",
      "{'accuracy': 0.8103113739258986, 'overall_precision': 0.21545682267525007, 'overall_recall': 0.24874437800009447, 'overall_f1': 0.22946396589092644}\n",
      "\n",
      "Overall Validation Results:\n",
      "Mean Validation Loss: 0.6321\n",
      "Mean Validation Accuracy: 0.8233\n",
      "Mean Precision: 0.4485\n",
      "Mean Recall: 0.4542\n",
      "Mean F1 Score: 0.4428\n",
      "Model Running: roberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmed\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "model_names = [\"bert\",\"distilbert\",\"roberta\",\"electra\",\"spanbert\",\"codebert\",\"gpt2\"]\n",
    "for selected_model in model_names:\n",
    "    print(\"Model Running: \"+ selected_model)\n",
    "    model_name=selected_model\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    tokenizer, model = load_model_and_tokenizer(model_name, num_classes)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "    val_dataset = TextDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    epochs = 5\n",
    "\n",
    "    # Variables to store overall metrics across all epochs\n",
    "    total_val_loss = 0\n",
    "    total_val_acc = 0\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Train for each epoch\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, device)\n",
    "        \n",
    "        # Evaluate on the validation set for each epoch\n",
    "        val_loss, val_acc, val_metrics = evaluate_model(model, val_loader, device, label_encoder,(epoch+1),model_name)\n",
    "        \n",
    "        # Print per-epoch results\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "        print(val_metrics)\n",
    "        \n",
    "        # Accumulate validation metrics across all epochs for later averaging\n",
    "        total_val_loss += val_loss\n",
    "        total_val_acc += val_acc\n",
    "        total_precision += val_metrics['overall_precision']\n",
    "        total_recall += val_metrics['overall_recall']\n",
    "        total_f1 += val_metrics['overall_f1']\n",
    "\n",
    "    # Calculate and print overall mean metrics\n",
    "    mean_val_loss = total_val_loss / epochs\n",
    "    mean_val_acc = total_val_acc / epochs\n",
    "    mean_precision = total_precision / epochs\n",
    "    mean_recall = total_recall / epochs\n",
    "    mean_f1 = total_f1 / epochs\n",
    "\n",
    "    print(\"\\nOverall Validation Results:\")\n",
    "    print(f\"Mean Validation Loss: {mean_val_loss:.4f}\")\n",
    "    print(f\"Mean Validation Accuracy: {mean_val_acc:.4f}\")\n",
    "    print(f\"Mean Precision: {mean_precision:.4f}\")\n",
    "    print(f\"Mean Recall: {mean_recall:.4f}\")\n",
    "    print(f\"Mean F1 Score: {mean_f1:.4f}\")\n",
    "\n",
    "    with open('results/transformer_result.txt', 'a') as file:\n",
    "        file.write(\"Model Name: \"+ selected_model + \"\\n\")\n",
    "        file.write(\"Overall Validation Results:\\n\")\n",
    "        file.write(f\"Mean Validation Accuracy: {mean_val_acc:.4f}\\n\")\n",
    "        file.write(f\"Mean Precision: {mean_precision:.4f}\\n\")\n",
    "        file.write(f\"Mean Recall: {mean_recall:.4f}\\n\")\n",
    "        file.write(f\"Mean F1 Score: {mean_f1:.4f}\\n\")\n",
    "        file.write(f\"Mean Validation Loss: {mean_val_loss:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
